[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CE313 - Estatística Não-Paramétrica",
    "section": "",
    "text": "Apresentação\nCE313 - Estatística não-paramétrica Departamento de Estatística - UFPR Professor: Fernando Lucambio Pérez Primeiro Semestre 2024 (15/Fevereiro/2024 - 29/Junho/2024).",
    "crumbs": [
      "Apresentação"
    ]
  },
  {
    "objectID": "index.html#objetivos",
    "href": "index.html#objetivos",
    "title": "CE313 - Estatística Não-Paramétrica",
    "section": "Objetivos",
    "text": "Objetivos\nOs métodos de inferência estatística não paramétricos ou de distribuição livre, são procedimentos matemáticos para testes de hipóteses e modelos de regressão que, diferentemente da estatística paramétrica, não fazem suposições sobre a distribuição de probabilidade das variáveis a serem consideradas. Objetivo geral: Espera-se que, ao final da disciplina, o aluno deva saber identificar o uso de testes não-paramétricos e lidar de forma apropriada com problemas práticos.\nObjetivos específicos: Identificar situações nas quais procedimentos não-paramétricos podem ser aplicados, selecionar testes não-paramétrico adequados para um problema em estudo, construir as hipóteses correspondentes e aplicar os procedimentos escolhidos utilizando funções R para esta finalidade.",
    "crumbs": [
      "Apresentação"
    ]
  },
  {
    "objectID": "index.html#ementa",
    "href": "index.html#ementa",
    "title": "CE313 - Estatística Não-Paramétrica",
    "section": "Ementa",
    "text": "Ementa\nEstatística paramétrica e não paramétrica. Testes não paramétricos para uma, duas ou mais amostras. Estimação não paramétrica de densidades. Introdução aos modelos não paramétricos de regressão.\nLocal: Laboratório B. Horário: quarta-feira 19:00h, sexta-feira 20:45h. Nota: a nota final será a soma das notas obtidas nos quatro trabalhos assíncronos programados.",
    "crumbs": [
      "Apresentação"
    ]
  },
  {
    "objectID": "index.html#referências-bibliográficas-básicas",
    "href": "index.html#referências-bibliográficas-básicas",
    "title": "CE313 - Estatística Não-Paramétrica",
    "section": "Referências bibliográficas básicas",
    "text": "Referências bibliográficas básicas\n\nConover, W.J. (1999). Practical nonparametric statistics. 3rd. ed. New York: Chichester: John Wiley & Sons (Asia).\nGibbons, J.D. (1993). Nonparametric Statistics: An Introduction, Newbury Park: Sage Publications.\nSiegel, S. and Castellan, N.H. (2006). Estatística não-paramétrica para ciências do comportamento. RS: Artmed.\nLucambio, F. (2023). Estatística não paramétrica",
    "crumbs": [
      "Apresentação"
    ]
  },
  {
    "objectID": "index.html#referências-bibliográficas-complementares",
    "href": "index.html#referências-bibliográficas-complementares",
    "title": "CE313 - Estatística Não-Paramétrica",
    "section": "Referências bibliográficas complementares",
    "text": "Referências bibliográficas complementares\n\nHastie, T.; Tibshirani, R. and Friedman, J. (2013). The elements of statistical learning: Data mining, inference, and prediction. Springer New York.\nHollander, M. and Wolfe, D.A. (1999). Nonparametric statistical methods. 2nd. ed. New York: John Wiley & Sons.\nKloke, J. and McKean, J.W. (2015). Nonparametric statistical methods using R. Boca Raton: CRC Press.\nSilverman, B.W. (1994). Nonparametric Regression and Generalized Linear Models: A Roughness Penalty Approach, London: Editora Chapman & Hall.\nWasserman, L. (2006). All of nonparametric statistics: New York: Springer.",
    "crumbs": [
      "Apresentação"
    ]
  },
  {
    "objectID": "index.html#conteúdo-programático",
    "href": "index.html#conteúdo-programático",
    "title": "CE313 - Estatística Não-Paramétrica",
    "section": "Conteúdo Programático",
    "text": "Conteúdo Programático\nMÓDULO I: Estimação não paramétrica\n\nAula 01 - Estatística não paramétrica\nAula 02 - Estatísticas de ordem\nAula 03 -\nAula 04 -\nAula 05 -\nAula 06 -\n\n\nMÓDULO II: Problemas de amostra única\n\nAula 07 -\nAula 08 -\nAula 09 -\nAula 10 -\nAula 11 -\nAula 12 -\nAula 13 -\nAula 14 -\n\n\nMÓDULO III: Procedimentos em k amostras\n\nAula 15 -\nAula 16 -\nAula 17 -\nAula 18 -\nAula 19 -\nAula 20 -\nAula 21 -\nAula 22 -\n\n\nMÓDULO IV: Regressão não paramétrica\n\nAula 23 -\nAula 24 -\nAula 25 -\nAula 26 -\nAula 27 -\nAula 28 -\nAula 29 -\nAula 30 -\n\n\nMÓDULO V: Redes neurais\n\nAula 31 -\nAula 32 -\nAula 33 -\nAula 34 -\nAula 35 -\nAula 36 -",
    "crumbs": [
      "Apresentação"
    ]
  },
  {
    "objectID": "Modulo I/modulo-01.html",
    "href": "Modulo I/modulo-01.html",
    "title": "Módulo I",
    "section": "",
    "text": "Estimação de densidades\nDe certa forma, problemas de estimação não-paramétrica são extensões de problemas de estimação paramétrica, mas a natureza do primeiro é bem diferente do último. Considere, por exemplo, a situação de observações independentes identicamente distribuídas, digamos \\(X_1,X_2,\\cdots,X_n\\). Em um problema paramétrico, assumimos que a distribuição de \\(X_i\\) é \\(F(\\cdot;\\theta)\\), a qual é totalmente especificada até o vetor de parâmetros \\(\\theta\\); então o problema é essencialmente a estimaçã de \\(\\theta\\). Em um problema não-paramétrico, a distribuição é totalmente desconhecida com, talvez, algumas restriçõs em propriedades gerais e, portanto, é denotada por \\(F\\).\nAqui consideramos estimadores de \\(F\\) em termos de função de densidade \\(f\\). A função de densidade tem a vantagem de fornecer uma representação visualmente mais informativa da distribuição subjacente. Por exemplo, o histograma geralmente dá uma ideia aproximada da forma da distribuição. Este último ficou como o único estimador de densidade não paramétrico até 1950. Por essa razão, nossa discussão começará com os histogramas.\nEmbora o histograma seja usado extensivamente, não é tão frequente que seja necessária uma definição matemática. Uma maneira de defini-lo é através da função de densidade empírica.\nO parâmetro \\(h\\) é chamado de largura de banda. Podemos escrever \\(\\widehat{f}\\), definido acima como, \\[\n    \\widehat{f}(x)=\\frac{1}{2nh}\\sum_{i=1}^n \\pmb{1}_{(x-h;x+h)}(X_i)\\cdot\n\\] Podemos excrever a função de densidade como $f(x)=_{h} ( F(x+h)-F(x-h) ) $, mas não se pode definir daqui o histograma porque, então, esse limite é zero ou infinito e assim em algum momento é preciso parar, em outras palavras, não se pode chegar muito perto de zero.\nDemonstração. Exercício.\nDeste teorema segue que \\(\\widehat{f}(x)\\) é um estimador consistente pontual de \\(f(x)\\) quando \\(h\\to 0\\) e \\(nh\\to \\infty\\). A seguir, o processo de limite é entendido como \\(h=h_n\\), de maneira que \\(h_n\\to 0\\) e \\(nh_n\\to \\infty\\). Estas condições podem ser interpretadas como se fosse necessário \\(h_n\\) ir a zero, mas não muito rápido. Isso é exatamente o que temos especulado, exceto que agora temos a taxa exata de convergência, que pode ser escrita como \\(h_n=o(n)\\).\nExemplo. Utilaremos dados simulados da distribuíão \\(N(0,1)\\), com isso mostramos o histograma destes 50 dados utilizando duas formas diferentes de encontrarmos uma expressão para \\(h_n\\), a chamada largura de banda.\nCode\nset.seed(1340)\nx = rnorm(50)\npar(mar=c(4,2,1,1))\nhist(x, main = NULL, freq = FALSE, breaks = \"Sturges\")\nbox()\n\n\n\n\n\n\n\n\n\nCode\npar(mar=c(4,2,1,1))\nhist(x, main = NULL, freq = FALSE, breaks = \"Scott\")\nbox()\nNeste exemplo utilizamos duas formas de escolher a largura de banda \\(h_n\\) dentre três diferentes possibilidades programadas na função hist. Por padrão escolhe-se breaks = “Sturges”, porposto por Sturges (1929), o qual sugere que \\[\n    h_n=\\frac{\\max(X_1,\\cdots,X_n)-\\min(X_1,\\cdots,X_n)}{1+3.322\\ln(n)}\\cdot\n\\] A segunda situação indica que o qual significa que se os dados provêm da distribuição Normal temos que \\(h_n=3.49 s n^{-1/3}\\) sendo \\(s\\) o desvio padrão estimado. Esta proposta deve-se à Scott (1979).\nEmbora o histograma é um estimador consistente quando \\(h_n\\to 0\\) e \\(nh_n\\to \\infty\\), verifica-se que se pode fazer melhor. A melhoria também é motivada por uma preocupação prática: o histograma não é uma função suave, uma propriedade que se pode esperar que qualquer função de densidade real tenha.\nÉ tipicamente assumido que \\(K\\) seja não-negativa, simétrica em torno de zero e satisfaz \\(\\int K(u)\\mbox{d}u = 1\\). Claro que o histograma é um caso especial do estimador do kernel se \\(K\\) for escolhido como a função de densidade da distribuição $ Uniforme(-1,1)$. O último não é uma função suave e é por isso que o histograma não é suave; mas escolhendo \\(K\\) como uma função suave, tem-se um estimador de \\(f\\) que seja suave.\nPor exemplo, escolhendo a função de densidade \\(N(0,1)\\), temos por resultado o conhecido como kernel Gaussiano e assim também utilizando a densidade de densidade \\(Beta\\) simétrica, dada por \\[\n    K(u)=\\frac{\\Gamma(\\nu+3/2)}{\\Gamma(1/2)\\Gamma(\\nu+1)}(1-u^2)^\\nu, \\qquad -1&lt; u &lt; 1,\n\\] e \\(K(u)=0\\) caso contrário. Os casos especiais \\(\\nu=0,1,2,3\\) correspondem às funções kernel uniforme, Epanechnikov, biweight e triweight, respectivamente.\nCode\nkernels = eval(formals(density.default)$kernel)\nplot (density(0, bw = 1), xlab = \"\", main = \"Diferentes kernel em R\")\nfor(i in 2:length(kernels)) lines(density(0, bw = 1, kernel =  kernels[i]), col = i)\nlegend(1.5,.4, legend = kernels, col = seq(kernels), lty = 1, cex = .8, y.intersp = 1)\n\n\n\n\n\n\n\n\n\nCode\nh.f = sapply(kernels, function(k) density(kernel = k, give.Rkern = TRUE))\nh.f = (h.f[\"gaussian\"] / h.f)^ .2\nh.f\n\n\n    gaussian epanechnikov  rectangular   triangular     biweight       cosine \n   1.0000000    1.0100567    0.9953989    1.0071923    1.0088217    1.0079575 \n   optcosine \n   1.0099458 \n\n\nCode\nbw = bw.SJ(x) ## escolha automática\nplot(density(x, bw = bw), main = \"Larguras de banda equivalentes\")\nfor(i in 2:length(kernels)) lines(density(x, bw = bw, adjust = h.f[i], \n                                                           kernel = kernels[i]), col = i)\nlegend(55, 0.035, legend = kernels, col = seq(kernels), lty = 1)\nUm problema prático importante na estimação de densidades via kernel é como escolher a largura de banda \\(h_n\\). Note que dadas condições como \\(h_n\\to 0\\) e \\(nh_n\\to\\infty\\), ainda existem muitas opções para \\(h_n\\). Então, de certo modo, a ordem de convergência ou divergência não resolve o problema. Uma solução para esse problema é conhecida como compensação de viés-variância. Antes de entrarmos nos detalhes, vamos primeiro apressentar um resultado em relação ao viés assintótico do estimador kernel. Aqui, o viés é definido como \\[\n    \\mbox{viés}\\big(\\widehat{f}(x)\\big)=\\mbox{E}\\big(\\widehat{f}(x)\\big)-f(x),\n\\] para um dado \\(x\\).\nDemonstração. Observemos que \\[\n\\begin{array}\n    \\mbox{E}\\big(\\widehat{f}(x)\\big) & = & \\frac{1}{n}\\sum_{i=1}^n \\frac{1}{h_n}\\int K\\Big( \\frac{x-y}{h_n}\\Big)f(y)\\mbox{d}y \\\\\n    & = & \\int K(u)f(x-h_n u)\\mbox{d}u \\, = \\, f(x)+\\int K(u)\\big( f(x-h_n u)-f(x)\\big)\\mbox{d}u\\cdot\n\\end{array}\n\\] Utilizando então o teorema da convergência dominada completa-se a demonstração.&#9609\nDemonstração. A demonstração é baseada na expansão de Taylor, \\[\n    f(x-h_nu) = f(x)-h_nuf'(x)+\\frac{h_n^2u^2}{2}f''(x)-\\frac{h_n^3u^3}{6}f'''(\\epsilon),\n\\] sendo \\(\\epsilon\\) fica entre \\(x-h_nu\\) e \\(x\\). Os detalhes são deixados como um exercício.\nUma medida de precisão do estimador é o erro quadrático médio (EQM), dado por \\[\n    EQM\\big(\\widehat{f}(x)\\big) = \\mbox{E}\\big(\\widehat{f}(x)-f(x)\\big)^2\\cdot\n\\] é fácil mostrar que o \\(EQM\\) combina o viés e a variância de tal maneira que \\[\n    EQM\\big(\\widehat{f}(x)\\big) = \\mbox{viés}\\big(\\widehat{f}(x)\\big)^2 + \\mbox{Var}\\big(\\widehat{f}(x)\\big)\\cdot\n\\] Vemos que, sob as condições \\(h_n\\to 0\\) e \\(nh_n\\to \\infty\\) e se ignorarmos os termos de baixa ordem, temos \\[\n    EQM\\big(\\widehat{f}(x)\\big) \\approx \\frac{h_n^4}{4}\\big(f''(x)\\big)^2\\tau^4 + \\frac{f(x)}{nh_n}\\gamma^2,\n\\] onde \\(\\tau^2=\\int u^2K(u)\\mbox{d}u\\) e \\(\\gamma^2=\\int K^2(u)\\mbox{d}u\\). O termo à direita da expressão acima é minimizada quando \\[\n    h_n = \\left( \\frac{\\gamma^2f(x)}{\\tau^4\\big(f''(x)\\big)^2} \\right)^{\\frac{1}{5}}n^{-\\frac{1}{5}}\\cdot\n\\] Note ainda que a expressão acima não é a solução ideal, isso porque \\(f\\) é desconhecida na prática. No entanto, dá-nos pelo menos alguma ideia sobre a taxa ideal de convergência a zero, sendo esta \\(h_n=O(n^{-\\frac{1}{5}})\\).\nQuando \\(f\\) é desconhecida, uma abordagem natural seria substituí-lo por um estimador e, assim, obter uma largura de banda ideal estimada. Uma complicação é que a largura de banda ideal depende de \\(x\\) mas, idealmente, gostaríamos de usar uma largura de banda que funcionasse para diferentes \\(x\\) dentro de um certo intervalo, se não todos os \\(x\\). Para obter uma largura de banda ideal que não depende de \\(x\\), integramos os dois lados da expressão de \\(EQM\\) em relação a \\(x\\). Isto nos leva a \\[\n    \\int EQM\\big(\\widehat{f}(x)\\big)\\mbox{d}x \\approx \\frac{\\tau^4h_n^4}{4}\\int \\big(f''(x)\\big)^2 \\mbox{d}x +\n    \\frac{\\gamma^2}{nh_n}\\int f(x)\\mbox{d}x \\, = \\, \\frac{\\tau^4\\theta^2h_n^4}{4} + \\frac{\\gamma^2}{nh_n},\n\\] com \\(\\theta^2=\\int \\big(f''(x)\\big)^2 \\mbox{d}x\\). Pelo mesmo argumento, o lado direito acima é minimizado quando\n\\[\n    h_n = \\left( \\frac{\\gamma^2}{\\tau^4\\theta^2} \\right)^{\\frac{1}{5}}n^{-\\frac{1}{5}}\\cdot\n\\] Desta vez, o \\(h_n\\) ideal não depende de \\(x\\). Além disso, a integral do \\(EQM\\) ou o \\(IEQM\\) mínimo é dado por \\[\n    IEQM \\, = \\, \\int EQM\\big(\\widehat{f}(x)\\big)\\mbox{d}x \\, = \\, \\frac{5}{4}\\big( \\tau\\gamma^2 \\big)^\\frac{4}{5}\\theta^{\\frac{2}{5}}\n    n^{-\\frac{4}{5}}\\cdot\n\\]\nUma implicação é a seguinte. Note que o \\(IEQM\\) depende do kernel \\(K\\) através de \\(c_K=\\big( \\tau\\gamma^2 \\big)^\\frac{4}{5}\\). Mostrou-se que para os kernels comumente usados, tais como aqueles listados, o desempenho dos estimadores de kernel correspondentes é quase o mesmo em termos dos valores de \\(c_K\\). Voltando ao problema sobre a estimação da largura de banda ideal, vemos que tudo o que precisamos é encontrar um estimador consistente de \\(\\theta^2\\). Se \\(f\\) é a função de densidade da distribuição normal com desvio padrão \\(\\sigma\\), então pode ser mostrado que \\(\\theta^2=3/8\\sqrt{\\pi}\\sigma^5\\). Naturalmente, se alguém souber que \\(f\\) é normal, então a estimação da densidade não-paramétrica não seria necessária, porque um método paramétrico provavelmente seria melhor. Em geral, pode-se expandir \\(f\\) em torno da densidade gaussiana usando a expansão de Edgeworth.\nUtilizando a abordagem acima, Hjort and Jones (1996) obteveram o seguinte estimador ótimo para a largura de banda \\[\n    \\widehat{h}_n = \\widehat{h}_0\\left(1+\\frac{35}{48}\\widehat{\\gamma}_4+\\frac{35}{32}\\widehat{\\gamma}_3^2+\n        \\frac{385}{1024}\\widehat{\\gamma}_4^2\\right)^{-\\frac{1}{5}},\n\\] onde \\(\\widehat{h}_0\\) é a estimativa ideal da largura de banda assumindo que \\(f\\) é normal, isto é, com \\(\\theta^2\\) substituído por \\(3/8\\sqrt{\\pi}\\sigma^5\\) ou mais explicitamente \\[\n    \\widehat{h}_0=1.06\\left(\\frac{\\widehat{\\sigma}}{n^{\\frac{1}{5}}}\\right),\n\\] chamamos \\(\\widehat{h}_0\\) a largura de banda da linha de base e \\(\\widehat{\\sigma}^2\\) é a variância amostral dada por \\[\n    \\widehat{\\sigma}^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\overline{X})^2\\cdot\n\\] Além disso, \\(\\widehat{\\gamma}_3\\) e \\(\\widehat{\\gamma}_4\\) são os estimadores dos coefcientes de assimetria de amostra e curtose, dado por \\[\n    \\widehat{\\gamma}_3=\\frac{1}{(n-1)\\widehat{\\sigma}^3}\\sum_{i=1}^n (X_i-\\overline{X})^3\n\\] e \\[\n    \\widehat{\\gamma}_4=\\frac{1}{(n-1)\\widehat{\\sigma}^4}\\sum_{i=1}^n (X_i-\\overline{X})^4-3,\n\\] respectivamente. Houve outras abordagens para a seleção da largura de banda ótima, incluindo o método de validação cruzada. Ambos procedimentos foram programados na função density.\nExemplo. Utilaremos os dados simulados da distribuíão \\(N(0,1)\\) no exemplo anterior para com isso mostrarmos o histograma e o estimador Kernel da função de densidade.\nCode\nset.seed(1340)\nx = rnorm(50)\npar(mar=c(4,2,1,1))\nhist(x, main = NULL, freq = FALSE, breaks = \"Sturges\")\nbox()\nlines(density(x, bw = \"nrd0\"), col = \"red\")\n\n\n\n\n\n\n\n\n\nCode\npar(mar=c(4,2,1,1))\nhist(x, main = NULL, freq = FALSE, breaks = \"Scott\")\nbox()\nlines(density(x, bw = \"bcv\"), col = \"red\")\n\n\nWarning in bw.bcv(x): mínimo ocorreu em uma das extremidades do intervalo",
    "crumbs": [
      "Módulo I"
    ]
  },
  {
    "objectID": "Modulo I/Aula 01/aula01.html",
    "href": "Modulo I/Aula 01/aula01.html",
    "title": "1  Estatística Não Paramétrica",
    "section": "",
    "text": "1.1 Introdução\nEm todos os problemas de inferência estatística considerados, assumimos que a distribuição da variável aleatória amostrada seja conhecida a menos, talvez, para alguns parâmetros. Na prática, entretanto, a forma funcional da distribuição é raramente ou nunca conhecida. Por conseguinte, é desejável conceber alguns procedimentos que estejam livres desta hipótese relativa à distribuição.\nPara entender a ideia de estatística não-paramétrica, primeiro requeremos uma compreensão de conceitos da estatística básica paramétrica. Conceitos elementares introduzem o conceito de teste de significância estatística com base na distribuição amostral de uma estatística particular. Em resumo, se tivermos um conhecimento básico da distribuição subjacente de uma variável, poderemos fazer previsões sobre como, em amostras repetidas de tamanho igual, essa estatística específica se comportará, isto é, como será distribuída.\nEstudamos aqui alguns procedimentos que são comumente referidos como métodos sem distribuição ou não paramétricos. O termo livre de distribuição refere-se ao fato de que nenhuma suposição é feita sobre a distribuição subjacente, exceto que a função de distribuição sendo amostrada seja absolutamente contínua ou puramente discreta. O termo não paramétrico refere-se ao fato de não haver parâmetros envolvidos no sentido tradicional do termo parâmetro utilizado até o momento.\nGrosseiramente falando, um procedimento não-paramétrico é um procedimento estatístico que possui certas propriedades desejáveis que mantêm suposições relativamente leves em relação às populações subjacentes das quais os dados são obtidos.\nNos dois exemplos seguintes mostramos distribuições conhecidas que são livres de parâmetros.\nCode\n# Definindo os valores para o eixo x1 e x2\nx1 &lt;- seq(-6, 6, length.out = 1000)\nx2 &lt;- seq(0, 10, length.out = 1000)\n\n# Graus de liberdade para a distribuição t-Student e qui-quadrado\ndf_t &lt;- 3\ndf_chi &lt;- 3\n\n# Calculando os valores de densidade das distribuições\ndensidade_t &lt;- dt(x1, df = df_t)\ndensidade_chi &lt;- dchisq(x2, df = df_chi)\n\n# Configurando o layout da plotagem\npar(mfrow = c(1, 2))\n\n# Plotando a distribuição t-Student\nplot(x1, densidade_t, type = \"l\", lwd = 2, col = \"blue\", \n     main = \"Distribuição t-Student (3)\",\n     xlab = \"x\", ylab = \"Densidade\")\n\n# Plotando a distribuição qui-quadrado\nplot(x2, densidade_chi, type = \"l\", lwd = 2, col = \"green\", \n     main = \"Distribuição Qui-Quadrado (3)\",\n     xlab = \"x\", ylab = \"Densidade\")\nO desenvolvimento repetido e contínuo de procedimentos estatísticos não paramétricos nas últimas décadas deve-se às seguintes vantagens de técnicas não paramétricas:\nMas têm desvantagens:",
    "crumbs": [
      "Módulo I",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Estatística Não Paramétrica</span>"
    ]
  },
  {
    "objectID": "Modulo I/Aula 01/aula01.html#introdução",
    "href": "Modulo I/Aula 01/aula01.html#introdução",
    "title": "1  Estatística Não Paramétrica",
    "section": "",
    "text": "Métodos não-paramétricos exigem poucas suposições sobre as populações subjacentes das quais os dados são obtidos. Em particular, os procedimentos não paramétricos abandonam a suposição tradicional de que as populações subjacentes sejam normais.\nOs procedimentos não paramétricos permitem que o usuário obtenha p-valores exatos para testes, probabilidades de cobertura exatas para intervalos de confiança, taxas exatas de erros experimentais para procedimentos de comparação múltipla e probabilidades exatas de cobertura para faixas de confiança sem confiar nas suposições de que as populações subjacentes sejam normais.\nAs técnicas não paramétricas são frequentemente, embora nem sempre, mais fáceis de aplicar do que as suas contrapartes teóricas normais.\nOs procedimentos não paramétricos são geralmente muito fáceis de entender.\nEmbora, à primeira vista, a maioria dos procedimentos não- paramétricos pareça sacrificar muito as informações básicas nas amostras, investigações de eficiência teórica mostraram que esse não é o caso. Normalmente, os procedimentos não-paramétricos são apenas ligeiramente menos eficientes do que os seus concorrentes de teoria normal quando as populações subjacentes são normais e podem ser moderadamente ou muito mais eficientes que os concorrentes quando as populações subjacentes não são normais.\n\n\n\nMétodos não paramétricos são relativamente insensíveis a observações distantes.\nOs procedimentos não paramétricos são aplicáveis em muitas situações em que os procedimentos teóricos normais não podem ser utilizados. Muitos procedimentos não-paramétricos exigem apenas as classificações das observações em vez da magnitude real das observações, enquanto os procedimentos paramétricos exigem as magnitudes.\nNem todos os procedimentos desenvolvidos na estatística paramétrica podem ser aplicados à estatística não-paramétrica.",
    "crumbs": [
      "Módulo I",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Estatística Não Paramétrica</span>"
    ]
  },
  {
    "objectID": "Modulo I/Aula 02/aula02.html",
    "href": "Modulo I/Aula 02/aula02.html",
    "title": "2  Estatísticas de ordem",
    "section": "",
    "text": "2.1 Amostras aleatórias\nConsidere-se um experimento estatístico que culmina em desfechos \\(x\\), que são os valores assumidos por uma variável aleatória \\(X\\). Seja \\(F\\) a função de distribuição de \\(X\\). Na prática, \\(F\\) não será completamente conhecida, isto é, um ou mais parâmetros associados com \\(F\\) serão desconhecidos. O trabalho de um estatístico é estimar esses parâmetros desconhecidos ou testar a validade de certas afirmações sobre eles. Ele pode, por exemplo, obter \\(n\\) observações independentes de \\(X\\). Isso significa que ele observa \\(n\\) valores \\(x1, x2, \\cdots , x_n\\) assumidos da variável aleatória \\(X\\). Cada \\(x_i\\) pode ser considerado como o valor assumido pela variável aleatória \\(X_i, i = 1, 2, \\cdots , n\\) onde \\(X_1, X_2, \\cdots, X_n\\) são variáveis aleatórias independentes com distribuição comum \\(F\\) . Os valores observados \\((x_1, x_2, \\cdots, x_n)\\) são então valores assumidos por \\((X_1, X_2, \\cdots, X_n)\\). O conjunto \\((X_1, X_2, \\cdots , X_n)\\) é, então, uma amostra de tamanho \\(n\\) da distribuição da população \\(F\\) . O conjunto de \\(n\\) valores \\(x_1, x_2, \\cdots , x_n\\) é chamado de uma realização ou estimativa da amostra. Note-se que os possíveis valores do vector aleatório \\((X_1, X_2, \\cdots , X_n)\\) podem ser olhados como pontos em \\(\\mathbb{R}^n\\), os quais podem ser chamados de elementos do espaço amostral. Na prática podemos não observar \\(x_1, x_2, \\cdots, x_n\\) mas alguma função \\(g(x_1, x_2, \\cdots , x_n)\\). Então \\(g(x_1, x_2, \\cdots , x_n)\\) serão considerados os valores assumidos pela variável aleatória \\(g(X)\\).\nVamos agora formalizar esses conceitos.\nSe \\(X_1, X_2, \\cdots , X_n\\) é uma amostra aleatória de \\(F (\\cdot; \\theta)\\), a função de distribuição conjunta é dada por:\n\\[F(x_1, \\cdots, x_n; \\theta) = \\prod_{i=1}^{n} F(x_i; \\theta)\\]\nSegundo esta definição cada uma das variáveis na amostra isoladamente é uma estatística assim como funções destas que, eventualmente, podem não fornecer informações úteis. Duas das estatísticas mais comumente utilizadas são mostradas no exemplo a seguir.\nDeve-se lembrar que as estatísticas amostrais apresentadas neste exemplo \\(\\overline{X}_n, S_{n}^{2}\\) e outras que irão definir-se posteriormente são variáveis aleatórias, com todas as consequências que isso implica, enquanto os parâmetros populacionais \\(\\mu, \\sigma^2\\) e assim por diante são constantes fixas, que podem ser desconhecidas.\nExemplo. Seja \\(X \\sim Bernoulli(p)\\), onde \\(p\\) é desconhecido. A função de distribuição de \\(X\\), mostrada na Figura 2.1, é dada por\n\\[F(x) = p\\delta (x − 1) + (1 − p)\\delta(x),    x \\in \\mathbb{R},\\] onde a função \\(\\delta(\\cdot)\\) foi definida em (1.2) como \\(\\delta(x) = \\begin{cases} 1, \\ x \\ \\geq \\ 0, \\\\ 0, \\ x \\ &lt; 0 \\end{cases}\\), chamada de função delta.\nSuponha que cinco observações independentes de \\(X\\) sejam 0, 1, 1, 1, 0. Então 0, 1, 1, 1, 0 é uma realização da amostra \\(X_1, X_2, \\cdots , X_5\\). A estimativa da média amostral é\n\\[\\overline{x}_5 = \\displaystyle \\frac{0 + 1 + 1 + 1 + 0}{5} = 0, 6\\] o qual é o valor assumido pela variável aleatória \\(\\overline{X}_n\\). A estimativa da variância amostral é \\[S_{5}^{2} = \\sum_{i=1}{5} \\displaystyle \\frac{(x_i - \\overline{x}_5)}{5 - 1}= \\displaystyle \\frac{2 \\ \\times (0,6)^2 \\ + \\ 3 \\ \\times (0,4)^2}{4}= 0,3\\]\nsendo este o valor assumido pela variável aleatória \\(S_{5}^{2}\\).\nExemplo. Seja \\(X \\sim N (\\mu, \\sigma^2)\\), \\(\\mu\\) conhecida, \\(\\sigma^2\\) desconhecido e \\(X_1, \\cdots , X_n\\) uma amostra aleatória dessa distribuição. De acordo com nossa definição, a função \\(\\sum_{i=1}^{n} X_{i} / \\sigma^2\\) não é uma estatística. Suponha que cinco observações de \\(X\\) -0,864; 0,561; 2.355; 0,582 e -0,774. Então, a estimativa da média amostral é 0.372 e a estimativa da variância amostral é 1.648.",
    "crumbs": [
      "Módulo I",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estatísticas de ordem</span>"
    ]
  },
  {
    "objectID": "Modulo I/Aula 02/aula02.html#momentos-amostrais",
    "href": "Modulo I/Aula 02/aula02.html#momentos-amostrais",
    "title": "2  Estatísticas de ordem",
    "section": "2.3 Momentos amostrais",
    "text": "2.3 Momentos amostrais\nNesta seção vamos estudar algumas estatísticas amostrais comumente utilizadas e suas distribuições.\nObservemos que nFn(x) é o número de Xk (1 ≤ k ≤ n) menores ou iguais a x. Se X(1), X(2), , X(n) são as estatísticas de ordem de \\(X_1, X_2, \\cdots , X_n\\) então claramente\nFn(x) =  \nk , se X n\n\n\n\n≤ x &lt; X\n(k+1)\n, (k = 1, 2, , n − 1)\n· (2.11)\n 1, se x ≥ X(n)\ncom esperança e variância\nVar[Fb\nE[Fn(x)] = \\(F\\) (x) (2.13)\nF (x)[1 − \\(F\\) (x)]\nDemonstração : Dado que δ(x − Xi), i = 1, 2, , n são variáveis aleatórias independentes igualmente distribuídas cada uma com função de probabilidade P [δ(x − Xi) = 1] = P (x − Xi ≥ 0) = \\(F\\) (x) e P [δ(x − Xi) = 0] = 1 − \\(F\\) (x), sua soma nF ∗(x) é uma variável aleatória com distribuição Binomial(n, p), onde p = \\(F\\) (x). As relações (2.12), (2.13) e (2.14) seguem-se imediatamente.\nDemonstração :\nE´ uma consequência da Lei dos Grandes Números .\nCorolário 2.12\nonde Z ∼ N (0, 1).\n√n[F (x) F (x)] √F (x)[1 − \\(F\\) (x)] −→ Z quando n → ∞,\nDemonstração :\nE´ consequência do Teorema do Limite Central.\nExemplo 2.15\nVamos apresentar o conceito de função distribuição empírica no caso de termos uma amostra aleatória da distribuição N (0, 1). A lista de comandos na linguagem de programação R está disponível abaixo. O primeiro comando destina-se a fixar o gerador de amostras e, assim, em qualquer momento podemos obter a mesma amostra aleatória. Na Figura 2.2 mostramos a forma da distribuição empírica, de três formas diferentes, para uma amostra de tamanho 12. A representação da função de distribuição empírica é realizada permitindo escolher qual utilizar segundo o agrado.\nlwd = 2\n−1.5 −1.0 −0.5 0.0 0.5\nx\n−1.5 −1.0 −0.5 0.0 0.5\nx\n−1.5 −1.0 −0.5 0.0 0.5\nx\nFigura 2.2: Representação da função de distribuição amostral ou empírica, de três formas diferentes, para uma amostra normal padrão de tamanho 12.\nA linhas de comando a seguir permitiram-nos gerar os gráficos na Figura 2.2: construímos : set.seed(5739); x=rnorm(12); Fn=ecdf(x) par(mar=c(5,4,3,1), cex=0.9) plot(Fn, main=““) plot(Fn, verticals = TRUE, do.points = FALSE, main=”“) plot(Fn , lwd = 2, main=”“); mtext(”lwd = 2”, adj = 1) xx=unique(sort(c(seq(-3, 2, length = 201), knots(Fn12)))) lines(xx, Fn(xx), col = “blue”) abline(v = knots(Fn), lty = 2, col = “gray70”) Observemos que a convergência da distribuição empírica, segundo o Teorema 2.10, é para cada valor de x. E´ possível fazer uma demonstração da convergência em probabilidade simultaneamente para todos os x, ou seja, da convergência uniforme.\nDemonstração : Seja ϵ &gt; 0. Escolhemos um inteiro k &gt; 1/ϵ e números −∞ = x0 &lt; x1 ≤ x2 ≤ ≤ xk−1 &lt; xk = ∞, tais que \\(F\\) (x−) ≤ j/k ≤ \\(F\\) (xj), para j = 1, , k − 1. Observe que se xj−1 &lt; xj, então\nPela Lei dos Grandes Números\nF (x−) − \\(F\\) (xj−1) ≤ ϵ·\nq.c. Fn(xj) −→ \\(F\\) (xj) e — q.c. −\npara j = 1, , k − 1. Consequentemente,\nFbn(xj ) −→ \\(F\\) (xj ),\n∗ − q.c. ∆n = max{|Fbn(xj) − \\(F\\) (xj)|, |Fn (xj ) − \\(F\\) (xj)|, j = 1, , k − 1} −→ 0·\nSeja x arbitrário e encontremos j tal que xj−1 &lt; x ≤ xj. Então, Fbn(x) − \\(F\\) (x) ≤ Fbn(x−) − \\(F\\) (xj−1) ≤ Fbn(x−) − \\(F\\) (x−) + ϵ,\ne\nIsto implica que\nFbn(x) − \\(F\\) (x) ≥ Fbn(xj−1) − \\(F\\) (x−) ≥ Fbn(xj−1) − \\(F\\) (xj−1) − ϵ·\nq.c. sup |Fn(x) F (x)| ∆n + ϵ ϵ· x Como isso vale para todo ϵ &gt; 0, o teorema segue.\nAgora, dado que \\(F\\) ∗(x) tem pontos de salto em Xi, i = 1, 2, , n é claro que existem todos os momentos de \\(F\\) ∗(x). Vamos considerar alguns valores típicos da função de distribuição \\(F\\) , chamados de estatísticas amostrais. Escrevamos a = 1 ∑ Xk, (2.15)\npara os momentos de ordem k ao redor do 0 (zero). Aqui ak, serão chamados de momentos amostrais de ordem k. Com esta notação\nO momento amostral central é definido por\nn a1 = Xi n i=1\n= X·\nb = 1 ∑(X − a )k = 1 ∑(X\n— X) · (2.16)\nLogicamente,\nk n i 1 i=1\nn i i=1\nb = 0 e b\n= (n − 1 ) S2·\nComo mencionado anteriormente, não chamamos b2 a variaˆncia amostral. S2\nserá chamada como a variaˆncia\namostral por razões que se tornarão claras posteriormente. Temos que b2 = a2 − a2· Para a função geradora de momentos de Fn podemos afirmar que n\nMFbn\n\n= 1 etXi · n\n\ni=1\nDefinições similares são realizadas para momentos amostrais de distribuições multivariadas. Por exemplo, se (X1, Y2), (X2, Y2), , (Xn, Yn) é uma amostra de uma distribuição bivariada, podemos escrever n n X = 1 ∑ X , Y = 1 ∑ Y\npara as duas médias amostrais e para os momentos de segunda ordem centrais escrevemos\nb20\nn = (Xi n i=1\n— X) , b02\nn = (Yi n i=1\n— Y ) ,\nMais uma vez, escrevemos\nb11\nn\nn = (Xi n i=1\n— X)(Yi\n— Y )·\nn\nS2 = 1 ∑(X\n— X)2, S2 = 1 ∑(Y\n— Y ) , (2.17)\npara as duas variaˆncias amostrais e para a covariância amostral utilizamos n\nS11\n= 1 (X n − 1 i=1\n— X)(Yi\n— Y )· (2.18)\nEm particular, o coeficiente de correlação amostral é definido por b11 S11\nR = 20\nb02\n= · S1S2\nPode ser demonstrado que |R| ≤ 1 e que os valores extremos ±1 ocorrem somente quando todos os pontos amostrais (X1, Y2), (X2, Y2), , (Xn, Yn) estão alinhados. Correspondendo a uma amostra \\(X_1, X_2, \\cdots , X_n\\) de observações em \\(F\\) , p-ésimo quantil amostral é definido como o p-ésimo quantil da função de distribuição amostral, ou seja, como \\(F\\) −1. Os quatis amostrais são definidos de maneira similar. Então, se 0 &lt; p &lt; 1, o quantil amostral de ordem p,\nr = [np] se n é um número par [np] + 1 se n é um número ímpar\n· (2.19)\nComo usual, [x] denota o maior inteiro x. Observe que, se [n] for par, podemos escolher qualquer valor entre X([np]) e X([np]+1) como o p-ésimo quantil amostral. Então, se p = 1 e n par podemos escolher qualquer valor entre X(n/2) e X(n/2+1), os dois valores do meio, como a mediana amostral. Habitualmente é escolhido o ponto médio. Assim, a mediana amostral é definida como\nξb1/2 = \nX((n+1)/2) se n é ímpar X(n/2) + X((n/2)+1) se n é par\n· (2.20)\nObserve que\n2 [n + 1] = (n + 1 )\n2 2 se n é ímpar. Consideraremos agora os momentos de características amostrais. Nos seguintes desenvolvimentos denotaremos E(Xk) = mk e E[(X µ)k] = µk como os momentos populacionais e os momentos populacionais centrais de k-ésima ordem, respectivamente. Nas situações onde utilizamos mk ou µk assumiremos que estes existem. Também, σ2 representará a variância populacional.\nDemonstração : Para provar (2.23) observemos que\n(∑n\nXi = ∑\nX3 + 3 ∑ ∑\nX2Xk + ∑ ∑ ∑ XiXjXk,\ni=1\ni=1\ni=1 j=1 j̸=k\ni=1 j=1 k=1 i̸=j, i̸=k j̸=k\ndesta expressão obtemos o resultado em (2.23). Similarmente\n(∑n )4\n( n )  n n n\n\n∑ ∑ ∑\nXi =\n∑ Xi ∑ X3 + 3 ∑ ∑ X2Xj +\ni j k\ni=1\ni=1\nn\n i=1\ni i=1 j=1 i̸=j n n\ni=1 j=1 k=1 i̸=j, i̸=k j̸=k\n= ∑ X4 + 4 ∑ ∑ XiX3 + 3 ∑ ∑ X2X2\ni i=1\ni=1 j=1 i̸=j\nj i j i=1 j=1 i̸=j\nn n n n n n n = +6 ∑ ∑ ∑ X2XjXk + ∑ ∑ ∑ ∑ XiXjXkXl·\ni=1 j=1 k=1 i̸=j, i̸=k j̸=k\ni=1 j=1 k=1 l=1 i̸=j, i̸=k, i̸=l j̸=k, j̸=l k̸=l\nUm detalhe importante é que os momentos centrais podem ser calculados a partir dos momentos, por exemplo, µ2 = E[(X − µ)2] = m2 − µ2, µ3 = E[(X − µ)3] = m3 − 3µm2 + 2µ3\ne assim por diante. Sabemos agora como calcular os momentos, até quarta ordem, de X. Vejamos a seguir como calcular os momentos centrais.\nDemonstração : Temos que µ (X) = E(X − µ)3 = E {∑n\n(X − µ)3} = ∑n\nE(X\n3 µ3 — µ) = ·\n3\nNo caso do quarto momento central\nn3 i=1 i\n1\nn3\n{∑n\ni=1 i n2\n}\nda qual obtemos que\nµ (X) = E(X µ)4 = E n4\ni=1\n(Xi − µ)4 ,\n1 µ4(X) =\nE(Xi − µ)4 +\n\n1 + ∑ ∑\n\nE{(Xi − µ)2(Xj − µ)2}·\nn4 i=1\n2 n4\ni=1 j=1 i̸=j\nDesenvolvendo adequadamente chegamos ao resultado em (2.26).\nExemplo 2.16\n, Xn uma amostra aleatória da distribuição Gamma(α, β). Sabemos da Seção 1.2 que E(X) = αβ, Var(X) = αβ2\nmk = βk(α + k − 1)(α + k − 2) α, k ≥ 1· αβ2 E(X) = αβ, Var(X) = n 1 1 µ (X) = µ = (6α3β3 + 3α2β3 + 2αβ3)· 3 n2 3 n\nAté o momento estudamos como calcular os momentos da média amostral. Mais complexo é obter expressões para os momentos da variância amostral S2. O teorema a seguir dedica-se ao objetivo de encontrarmos expressões, até segunda ordem, dos momentos amostrais centrais. Como consequência deste resultado obtemos os momentos da variaˆncia amostral.\nDemonstração : Temos que\nn E(b2) = E n i=1\nX2 n2\nn 2 Xi i=1\n= m2 −\n1  n E \nX2 + ∑\n∑ X2Xj\nAgora\n= m2\n1 — n2 [nm2\n\nn(n − 1)µ2] = ( n − 1 ) (m\n\n— µ )·\nn2b2 =\nn\ni=1\n2\n(Xi − µ)2 − n(X − µ)2 ·\nEscrevendo Yi = Xi − µ, vemos que E(Yi) = 0, Var(Yi) = σ2 e E(Y 4) = µ4. Temos então que\nn2 E(b2) = E\nn i=1 n\n2\nY 2 − nY 2\nn n\n n n n \n= E ∑ Y 4 + ∑ ∑ Y 2Y 2 − 2 ∑ ∑ Y 2Y 2 + ∑ Y 4\n\n1 3 ∑ ∑\n\nY 2Y 2 + ∑\nY 4 ·\nSegue então que\nn i j i=1 j=1 i̸=j\ni=1\ni \n2 1 n2 E(b2) = nµ + n(n − 1)σ2 − [n(n − 1)σ4 + nµ ] + [3n(n − 1)σ4 + nµ ]\n= (n − 2 + 1 ) µ + (n − 2 + 3 ) (n − 1)µ2 · (µ\n= σ2)\nPortanto\nn 4 n 2 2\nVar(b2) = E(b2) − [ E(b2)]2\n= (n − 2 +\n1 ) µ4\n\n(n − 1) (n − 2 + 3 µ 2 —\n\n( n − 1 )2\n= (n − 2 +\n1 ) µ4\nµ2 + (n − 1)(3 − n) ,\ncomo afirmado. As relações (2.29) e (2.30) podem ser provadas de forma semelhante.\nEste é justamente o motivo pelo qual chamamos S2 e não b2 de variância amostral.\nExemplo 2.17 (Continuação do Exemplo 2.16)\ninteir Nesta situação, σ2 = αβ2, µ2 = σ2 e µ4 = m4 − 4m3µ + 6m2µ2 − 3µ4. Obtemos que E(S2) = αβ2 e Var)(S2) = µ4 + 3 − n α2β4· n n(n − 1)\nO seguinte resultado fornece uma justificativa para a nossa definição de covariaˆncia amostral.\nDemonstração : Do Corolário 2.17 sabemos que E(S2) = σ2 e E(S2) = σ2. Para provar que E(S11) = ρσ1σ2 1 1 2 2 observemos que Xi é independente de Xj, (i ̸= j) e de Yj, (i ̸= j). Temos que\nAgora\n(n − 1) E(S11) = E\nE{(Xi − X)(Yi − Y )} =\nn i=1\n(Xi − X)(Yi − Y )] ·\n( ∑n Yj\n∑n Yj\n∑n Xj ∑n\nYj )\ne segue que\n1 = E(XY ) − n [ E(XY ) + (n − 1) E(X) E(Y )] 1 − n [ E(XY ) + (n − 1) E(X) E(Y )] 1 − n2 [n E(XY ) + n(n − 1) E(X) E(Y )] = n − 1 [ E(XY ) E(X) E(Y )] n\n(n − 1) E(S11) = n ( ) [ E(XY ) − E(X) E(Y )], n − 1\nisto é\nE(S11) = E(XY ) − E(X) E(Y ) = Cov(X, Y ) = ρσ1σ2·\nA seguir, voltamos nossa atenção para as distribuições das características da amostra. Existem várias possi- bilidades. Se for necessária a distribuição exata o método de transformação de variáveis pode ser utilizado. As vezes, a técnica da função geradora de momentos pode ser aplicada. Assim, se \\(X_1, X_2, \\cdots , X_n\\) é uma amostra aleatória de uma população com distribuição para a qual existe a função geradora de momentos, a função geradora de momentos da média amostral X é dada por n M (t) = E(etXi/n) = [MX(t/n)]n , (2.32) i=1 onde MX é a função geradora de momentos da distribuição populacional. Se MX (t) tiver alguma forma conhecida seria possível escrever a função de probabilidade ou de densidade de X. Embora este método tem a desvantagem óbvia que se aplica apenas à distribuições para as quais existem todos os momentos, veremos sua efetividade na situação importante de amostras da distribuição normal.\nExemplo 2.18\nSeja \\(X_1, X_2, \\cdots , X_n\\) uma amostra aleatória de tamanho n da distribuição Gama(α, 1). Nesta situação podemos encontrar a função de densidade de X. Temos que\nMX (t) = [MX\n(t/n)]n = 1 , t (1 − t/n)αn n\n&lt; 1,\nda qual obtemos que X ∼ Gama(nα, 1/n).\nExemplo 2.19\nSeja \\(X_1, X_2, \\cdots , X_n\\) uma amostra aleatória da distribuição Uniforme no intervalo (0, 1). Considere a média geométrica\nYn =\nn\ni=1\n1/n Xi ·\nSabemos que log(Yn) = (1/n) ∑n log(Xi) e, desta forma, log(Yn) é a média amostral de log(X1), , log(Xn). A função de densidade comum de log(X1), , log(Xn) é\nex, se x &lt; 0 f (x) = , 0, caso contrário\nque é a distribuição exponencial negativa com parâmetro β = 1. Vemos que a função geradora de momentos de log(Yn) é dada por\nMlog(Yn)\nn (t) = E(et log(Xi)/n) = , (1 t/n)n i=1\ne a função de densidade de log(Yn) é dada por\nflog(Yn)\n\n= \n\nnn Γ(n)[−y]\nn−1\neny\n, se − ∞ &lt; y &lt; 0 ·\n 0, caso contrário\nSegue então que Yn tem por função de densidade\nfYn\n\n= \n\nnn y Γ(n)\nn−1\n[− log(y)]\nn−1\n, se 0 &lt; y &lt; 1 ·\n\nVoltemos ao quantil amostral de ordem p,\n0, caso contrário\nξbp, o qual sabemos é ou X([np]) ou X([np]+1) dependendo se [np] é\num número par ou ímpar, como definido em (2.19). Simplificando, vamos discutir as propriedades de X([np]), onde p ∈ (0, 1) e n é grande. Isso, por sua vez, nos informará sobre as propriedades de ξp. Primeiro observemos que, se U1, U2, , Un é uma amostra aleatória da distribuição U (0, 1) então, pelo Teorema 2.3, temos que\ndo qual obtemos que\nU([np]) ∼ Beta([np], n − [np] + 1),\n[np]\nE(U([np])) =\nn + 1\nn−→→∞ p,\nCov(U , U\n) = n np1\n−→ p (1 − p )·\nUtilizando este resultado e a desigualdade de Chebychev, demonstramos que U −P→ p· (2.33)\nIsso gera a questão\nξbp −→ ξp?\nqualquer seja a distribuição da amostra aleatória X1, , Xn. Para respondermos a pergunta acima vamos utilizar o Lema de Hoeffding, ou seja, para respondermos se o quantil amostral de ordem p converge em probabilidade para o quantil teórico correspondente, utilizaremos o seguinte resultado devido a Hoeffding (1963).\nDemonstração : Dado que as variáveis aleatórias são limitadas ao intervalo (0, 1), sabemos que ehX ≤ (1 − X) + Xeh, isto deve-se a que a função exponencial ehX é convexa e, portanto, seu gráfico é limitado por cima no intervalo 0 ≤ X ≤ 1 pela linha que conecta as ordenadas X = 0 e X = 1. Então E(ehX ) ≤ (1 − E(X)) + E(X)eh· (2.35)\nSeja Sn = ∑n\nXi. Sabemos que\nP (Sn − E(Sn) ≥ nt) = E(1[Sn− E(Sn)−nt≥0]),\ntambém sabemos que\n1[Sn− E(Sn)−nt≥0] ≤ exp (h(Sn − E(Sn) − nt)),\nqualquer seja h uma constante positiva arbitrária. Então P Sn − E(Sn) ≥ nt ≤ E eh(Sn− E(Sn)−nt) (2.36) e como estamos assumindo que as variáveis são independentes, podemos escrever\n( ( ))\n∏ ( ( ))\nEscrevendo µi = E(Xi) temos, pela expressão em (2.35) que E(eh(Xi−µi)) ≤ e−hµi ((1 − µi) + µieh) = ef(h), (2.38) onde \\(F\\) (h) = −hµi + ln(1 − µi + µieh). As primeiras duas derivadas são:\n′ µi\n′′ µie−h(1 − µi)\nf (h) = −µi + e−h(1 − µ ) + µ\ne f (h) = [µi\n\ne−h(1 − µ )]2 ·\nµi\nNa segunda derivada, escolhendo u = µ + e−h(1 − µ ) 0 &lt; u &lt; 1. Portanto, \\(F\\) ′′(h) ≤ 1 . Pela série de Taylor\n\nvemos que este quociente é da forma u(1 − u), sendo\nEntão, pela expressão em (2.38)\nf (h) ≤\nf (0) + \\(F\\) ′(0)h +\n1 h2 = 8\n1 h2· 8\nSubstituindo em (2.36) temos que\nE(eh(Xi−µi)) ≤ e 1 h2 ·\nP (Sn\n— E(Sn\n) ≥ nt) ≤ e−nht+ 1 nh2 ,\ne o mínimo no expoente é atingido quando h = 4t. Então, o mínimo do limite superior da probabilidade é exp(−2nt2).\nDevemos lembrar que esta não é a única maneira de termos uma taxa de convergência para Teorema do Limite Central. Por exemplo, se Y1, Y2, , Yn forem variáveis aleatórias independentes e identicamente distribuídas, utilizando o Teorema de Berry-Esseen2, temos que\n( ∑n ∑\n) ( √ Var(Y1))\nC E|Y1\n— E(Y1)|\nP i=1\nXi −\ni=1\nE(Xi) ≥ nt ≤ Φ t n\n\n√n\n\nVar3/2(Y ) ·\n2\nDemonstração : Berry (1941); Esseen (1942).\nPode-se consultar o livro de Feller (1971) para uma demonstração moderna.\nExemplo 2.20\nCaso a amostra aleatória seja Bernoulli(µ), temos que n Xk ∼ Binomial(n, µ)· i=1 Então, segundo a desigualdade de Hoeffding\nP (X − µ ≥ t) ≤ exp(−2nt2)· Uma vantagem da desigualdade no Lema de Hoeffding é que não assume-se conhecimento da variância e, em geral, o limite da probabilidade é mais acurado do que outras desigualdades. Caso as variáveis aleatórias sejam limitadas como a ≤ Xi ≤ b, com a &lt; b, o limite superior da desigualdade (2.34) seria exp − 2nt2/(b − a)2 .\nExemplo 2.21\nSejam X1, , Xn variáveis aleatórias com distribuição U ( 1, 1). Nesta situação E(X) = 0, a = 1 e b = 1. A desigualdade de Hoeffding assume a forma P (X ≥ t) ≤ exp ( − nt2/2)·\nDemonstração : Para ϵ &gt; 0 qualquer, podemos escrever P (|ξp − ξp| &gt; ϵ) = P (ξp &gt; ξp + ϵ) + P (ξp &lt; ξp − ϵ)· Pelo Teorema 2.9, podemos escrever P (ξbp &gt; ξp + ϵ) = P (p &gt; Fbn(ξp + ϵ))\nn = P i=1\n1[Xi&gt;ξp+ϵ] &gt; n(1 − p))\nn = P i=1\nVi −\n∑i=1\nE(Vi) &gt; nδ1),\nonde Vi = 1[Xi&gt;ξp+ϵ] e δ1 = \\(F\\) (ξp + ϵ) − p. Da mesma forma, P (ξbp &lt; ξp − ϵ) = P (p &gt; Fbn(ξp − ϵ))\nn = P i=1\nWi −\n∑i=1\nE(Wi) &gt; nδ2),\nonde Wi = 1[Xi&lt;ξp−ϵ] e δ2 = p − \\(F\\) (ξp − ϵ) − p. Portanto, utilizando o Lema de Hoeffding (Lema 2.20), temos P (ξbp &gt; ξp + ϵ) ≤ exp(−2nδ2) P (ξbp &lt; ξp − ϵ) ≤ exp(−2nδ2)· Colocando δϵ = min{δ1, δ2}, a prova está completa.\nDemonstramos que\nlim P (|ξbp − ξp| &gt; ϵ) ≤ lim 2 exp(−2nδ2) = 0,\no qual significa que ξp −→ ξp. Em outras palavras, sempre que ξp seja solução única da desigualdade \\(F\\) (ξp ) ≤ p ≤ \\(F\\) (ξp), 0 &lt; p &lt; 1, o quantil amostral converge em probabilidade para o quantil populacional e isto sempre acontece nas distribuições contínuas. Um detalhe importante é que para demonstrarmos a convergência em probabilidade de ξp utilizamos o Lema de Hoeffding e ele depende da existência da esperança. O seguinte resultado fornece a distribuição assintótica da r-ésima estatística de ordem amostral de uma po- pulação com uma função de distribuição \\(F\\) , absolutamente contínua, e função de densidade \\(F\\) .\nDemonstração : Vamos demonstrar somente para o caso p = 1/2. Observemos que ξ1/2 é mediana única dado que f (ξ1/2) &gt; 0. Primeiro, consideremos que n seja ímpar, por exemplo, n = 2m − 1, logo P [√n(X(m) − \\(F\\) −1(1/2)) ≤ t] = P (X(m) ≤ t/√n + \\(F\\) −1(1/2))·\nSeja Sn o número de X que excedem t/ n + F (1/2). Então\nPercebemos que\nt X(m) ≤ √n + F\n(1/2) se, e somente se, Sn ≤ m − 1 =\nn − 1 · 2\nSn ∼ Binomial(n, 1 − \\(F\\) (F −1(1/2) + t/√n))· Fazendo pn = 1 − \\(F\\) (F −1(1/2) + t/√n), temos que\nP [√n(X\n\n\n\n— F −1(1/2)) ≤ t] = P (Sn\n≤ n − 1 )\n( Sn − npn 1 (n − 1) − npn )\n= P\nUtilizando o Teorema de Berry-Esseen, temos que\n√npn(1 − pn) ≤ √npn(1 − p ) · n\n{ ( n − 1 ) ( 1 (n − 1) − npn )}\nlim P n→∞\nSn ≤ 2\n— Φ √np\n(1 − pn)\n= 0·\nEscrevendo\n1 (n − 1) − npn npn(1 − pn)\n=\n√n( 1 − pn) 1/2 √n( − 1 + \\(F\\) (t/√n + \\(F\\) −1(1/2)))\n= 2t\n1/2 F (t/√n + \\(F\\) −1(1/2)) − \\(F\\) (F −1(1/2))\n−→ 2tf\n(F −1(1/2))·\nEntão\n( 1 (n − 1) − npn )\n( ( −1 ))\nΦ npn ou\n(1 − pn)\n≈ Φ 2tf F\n(1/2)\n√n(X\n\n− F\n\n( 1 )) −D→ N (0,\n4f 2\n1 (F −1(1/2)\n)) ·\nQuando n é par, digamos n = 2m, ambos P (√n X(m) F −1(1/2) t) quanto P (√n X(m+1) F −1(1/2) t) convergem a Φ(2tf (F −1(1/2))).\nObserve que o quantil amostral de ordem p, assintótica\nξbp, como consequência do Teorema 2.23, tem por distribuição\nN (ξ , 1 p(1 − p)) , onde ξp é o correspondente quantil populacional e \\(F\\) é a função de densidade populacional. Por exemplo, suponha temos uma amostra aleatória da di√stribuição N (µ, σ2) de tamanho n. Seja ξb1/2 a mediana amostral obtida dessa b ( πσ2 )\nTambém devemos ter em consideração que para demonstrarmos o Teorema 2.23 utilizamos a Teorema de Berry- Esseen, o qual depende da existência dos primeiros dois momentos da variável aleatória. Com isso, caso X Cauchy(µ, σ), o Teorema 2.23 não se aplica.",
    "crumbs": [
      "Módulo I",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estatísticas de ordem</span>"
    ]
  },
  {
    "objectID": "Modulo I/Aula 02/aula02.html#amostras-aleatórias",
    "href": "Modulo I/Aula 02/aula02.html#amostras-aleatórias",
    "title": "2  Estatísticas de ordem",
    "section": "",
    "text": "Definição. Seja \\(X\\) uma variável aleatória com função de distribuição \\(F\\) e \\(X_1, \\cdots, X_n\\) variáveis aleatórias independentes com distribuição comum \\(F\\). Chamaremos a coleção \\(X_1, \\cdots, X_n\\) de uma amostra aletória de tamanho \\(n\\) de \\(F\\) ou simplesmente como \\(n\\) observações independentes de \\(X\\).\n\n\n\n\nDefinição. Sejam \\(X_1, X_2, \\cdots, X_n\\) \\(n\\) observações independentes da variável aleatória \\(X\\) e seja \\(g: \\mathbb{R}^n \\to \\mathbb{R}\\) uma função real derivável. Então a variável aleatória \\(g(X_1, X_2, \\cdots, X_n)\\) é chamada de estatística, desde que não dependa de parâmetros desconhecidos.\n\n\n\nDefinição. Seja \\(X_1, X_2, \\cdots, X_n\\) uma amostra aleatória da função de distribuição \\(F\\). A estatística \\[\\overline{X}_n = \\sum_{i=1}^{n} \\displaystyle \\frac{Xi}{n},\\] é chamada de média amostral e a estatística \\[S_{n}^{2} = \\sum_{i=1}^{n} \\displaystyle \\frac{(X_i - \\overline{X}_n)^2}{n-1}\\] é chama de variância amostral.\n\n\n\n\n\n\n\n\n\n\nFigura 2.1: Representação da função de distribuição Bernoulli para três valores do parâmetro \\(p\\) = 0.3, 0.5 e 0.8. Observe que nesta curva a reta no intervalo (0, 1) depende de 1 - \\(p\\), isso porque a função \\(\\delta\\) é sempre zero para \\(x\\) - 1 nesse intervalo.",
    "crumbs": [
      "Módulo I",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estatísticas de ordem</span>"
    ]
  },
  {
    "objectID": "Modulo I/Aula 02/aula02.html#estatísticas-de-ordem",
    "href": "Modulo I/Aula 02/aula02.html#estatísticas-de-ordem",
    "title": "2  Estatísticas de ordem",
    "section": "2.2 Estatísticas de ordem",
    "text": "2.2 Estatísticas de ordem\nSeja (\\(X_1, X_2, \\cdots , X_n\\)) um vetor aleatório n-dimensional e (\\(x_1, x_2, \\cdots , x_n\\)) uma \\(n\\)-tupla assumida por (\\(X_1, X_2, \\cdots , X_n\\)). Vamos organizar \\(x_1, x_2, \\cdots , x_n\\) em ordem crescente de magnitude, para que \\[x_{(1)}\\le x_{(2)} \\le \\cdots ≤ x_{(n)},\\] onde \\(x_{(1)}\\) = min(\\(x_1, x_2, \\cdots , x_n\\)), \\(x_{(2)}\\) é o segundo menor valor em \\(x_1, \\cdots , x_n\\) e assim por diante, \\(x_{(n)}\\) = max(\\(x_1, \\cdots , x_n\\)). Se quaisquer dois \\(x_i\\), \\(x_j\\) forem iguais, a ordem não importa.\n\nDefinição. A função \\(X_{(k)}\\) de (\\(X_1, \\cdots ,X_n\\)) que assume o valor \\(x_{(k)}\\) em cada possível sequência (\\(x_1, x_2, \\cdots , x_n\\)) de valores assumidos por (\\(X_1,X_2, \\cdots , X_n\\)) é conhecida como a \\(k\\)-ésima estatística de ordem ou a estatística de ordem \\(k\\). O conjunto (\\(X_{(1)},X_{(2)}, \\cdots , X_{(n)}\\)) é chamado de estatísticas de ordem para (\\(X_1, X_2, \\cdots , X_n\\)).\n\nExemplo. Consideremos \\(X_1\\), \\(X_2\\), \\(X_3\\) três variáveis aleatórias discretas de maneira que, \\(X_1\\) e \\(X_3\\) sejam tais que assumam somente valores 0, 1 e que \\(X_2\\) assuma valores 1, 2, 3. O vetor aleatório (\\(X_1\\), \\(X_2\\), \\(X_3\\)) assume os valores: (0, 1, 0), (0, 2, 0), (0, 3, 0), (0, 1, 1), (0, 2, 1), (0, 3, 1), (1, 1, 0), (1, 2, 0), (1, 3, 0), (1, 1, 1), (1, 2, 1) e (1, 3, 1). Então \\(X_{(1)}\\) assume somente valores 0 ou 1; \\(X_{(2)}\\) assume somente valores 0 ou 1 e \\(X_{(3)}\\) assume somente valores 1, 2 ou 3.\n\n\n\n\n\n\nTeorema\n\n\n\nSeja (\\(X_{(1)}, X_{(2)}, \\cdots , X_{(n)}\\)) um vetor aleatório de dimensão \\(n\\) e seja \\(X_{(k)}, 1 \\le k \\le n\\), a \\(k\\)-ésima estatística de ordem. Então \\(X_{(k)}\\) é também uma variável aleatória.\n\n\nExercício. Na apresentação dos resultados a seguir assumiremos que \\(X_1, X_2, \\cdots , X_n\\) são variáveis aleatórias independentes e igualmente distribuídas contínuas com função de densidade \\(f\\) . Seja \\(\\{ X_{(1)}, X_{(2)}, \\cdots , X_{(n)} \\}\\) o conjunto das estatística de ordem para \\(X_1, X_2, \\cdots , X_n\\). Dado que todas as \\(X_i\\) são contínuas segue que, com probabilidade 1 \\[X_{(1)} \\le X_{(2)} \\le \\cdots \\le X_{(n)}·\\]\n\n2.2.1 2.2.1 Propriedades das estatísticas de ordem\nComeçaremos o estudo das propriedades encontrando a função de densidade conjunta de (X(1), X(2), , X(n)).\nDemonstração : A transformação de (X1, , Xn) a (X(1), , X(n)) não é biunívoca. De fato, existem um total de n! possíveis arranjos de x1, , xn em ordem crescente de magnitude. Assim, existem n! inversas para a transformação. Por exemplo, uma das n! permutações pode ser x4 &lt; x1 &lt; xn−1 &lt; x3 &lt; &lt; xn &lt; x2·\nA inversa correspondente é x4 = x(1), x1 = x(2), xn−1 = x(3), x3 = x(4) xn = x(n−1), x2 = x(n)· O determinante Jacobiano desta transformação é a matriz n×n identidade com as colunas reorganizadas, isto devido a que cada x(i) é igual a uma, e somente uma, das \\(X_1, X_2, \\cdots , X_n\\). Portanto J = ±1 e\nn f (x(2), x(n), x(4), x(1), , x(3), x(n−1))|J| = f (x(i)), i=1 quando x(1) &lt; x(2) &lt; &lt; x(n). A mesma expressão é válida para cada um dos n! arranjos. Segue então que\nf (x(1), , x(n)) = Todas as n! permutações\nn f (x(i)) i=1\n=  \nn! i=1\nf (x(i)), caso x(1) &lt; x(2) &lt; &lt; x(n) · 0, caso contrário\nExemplo 2.4 Sejam X1, , Xn variáveis aleatórias independentes com função de densidade comum f (x) = 1, se 0 &lt; x &lt; 1 · 0, caso contrário Então a função de densidade conjunta de X(1), X(2), , X(n) é\nf (x\n\n\n\n, , x\n\n\n\n) = n!, 0 &lt; x(1) &lt; x(2) &lt; &lt; x(n) 0, caso contrário\n· (2.5)\nEstamos confiados que como resultado do Teorema 2.2 temos funções de densidade. Vejamos neste exemplo se isso é realmente acontece. Consideremos, para simplificar, o caso n = 3 e verifiquemos se a integral da função de densidade em (2.5) é 1. Então\n∫∫∫\nf (x(1), x(2), x(3)) dx(1) dx(2) dx(3) = 6\n∫ 1 [∫ 1\n(∫ 1\ndx(3))\ndx(2)]\ndx(1)\n0 1 = 6 0\nx(1) [ x(1)\nx(2) (1 − x(2))\ndx(2)]\ndx(1)\n1 1 = 6 − x(1)\nx2 + (1) 2\ndx(1)\n= 1·\n0 Um detalhe interessante é que esta e outras propriedades demonstradas aqui somente são válidas quando as variáveis aleatórias são contínuas. Isso não significa que estatísticas de ordem não possam ser definidas no caso discreto. O que estamos dizendo é que estas propriedades somente podem ser demonstradas no caso contínuo.\nExemplo 2.5 Consideremos a situação em que temos somente três variáveis aleatórias independentes X1, X2 e X3 com\ndistribuição geométrica de parâmetro p, isto é, P (X = x; p) = (1 − p)px, x = 0, 1, 2, Encontremos P (X(1) &lt; X(2) &lt; X(3)). Nesta situação a probabilidade requerida pode ser escrita como: P (X(1) &lt; X(2) &lt; X(3)) = 1 − P (X1 = X2 ̸= X3) − P (X1 = X3 ̸= X2) −P (X2 = X3 ̸= X1) − P (X1 = X2 = X3) a qual pode ser escrita como P (X(1) &lt; X(2) &lt; X(3)) = 1 − 3P (X1 = X2 ̸= X3) − P (X1 = X2 = X3) = 1 − 3 [P (X1 = X2) − P (X1 = X2 = X3)] − P (X1 = X2 = X3) = 1 − 3P (X1 = X2) + 2P (X1 = X2 = X3)·\nNão é difícil perceber que\ne que\nP (X1 = X2) =\n(1 p)2 1 − p2 , (1 − p)3\ndo qual obtemos que\nP (X1 = X2 = X3) =\n1 − p3 ,\n6p3\nP (X(1) &lt; X(2) &lt; X(3)) = (1 − p)(1 + p + p2)· As propriedades das estatísticas de ordem que serão demonstradas valerão somente caso as variáveis sejam contínuas. Isto deve-se a que, caso as variáveis sejam discretas, a probabilidade\nP (X(1) = X(2) = = X(n)) ̸= 0,\ncomo vai ser mostrado no seguinte exemplo. Acontece que o fato da probabilidade das estatística de ordem poderem coincidir, com probabilidade diferente de zero, altera a estrutura da demonstração e não nos permite obtermos estes resultados para o caso discreto.\nExemplo 2.6 Sejam \\(X_1, X_2, \\cdots , X_n\\) variáveis aleatórias independentes assumindo somente 0 e 1 com probabilidade 1/2. Observemos que\nn n P (X(1) = X(2) = = X(n)) = ∏ P (X(k) = 0) + ∏ P (X(k) = 1)\nk=1 n = P (Xk k=1\nk=1 n = 0) + P (Xk k=1\n1 = 1) = 2n−1 ·\nEstudemos agora o comportamento marginal, ou seja, nos interessa agora encontrar a função de distribuição marginal de cada estatística de ordem.\nDemonstração : Partimos da expressão da função de densidade conjunta das estatísticas de ordem obtida no Teorema 2.2. Então,\nfr(x(r)) = n!f (x(r))\n∫ x(r) ∫ x(r−1)\n∫ x(2) ∫ +∞ ∫ +∞\n∫ +∞ ∏\nf (ti) dtn dtr+1 dt1 dtr−1\n−∞ −∞\n−∞ x(r)\nx(r+1)\nx(n−1) i̸=r\n= n!f (x(r))\n[1 − \\(F\\) (x(r))]n−r x(r)\n(n − r)!\n∫ x(2) r∏−1\nf (ti) dti\n[1 − \\(F\\) (x(r))]n−r [F (x(r))]r−1\n= n!f (x(r))\n(n − r)!\n· (r − 1)!\nComo utilidade deste teorema podemos mencionar o fato de agora podermos encontrar os momentos das es- tatística de ordem. Faremos isso como consequência do seguinte exemplo.\nExemplo 2.7 Sejam \\(X_1, X_2, \\cdots , X_n\\) variáveis aleatórias independentes U (0, 1). Então\nfr(x(r)) =  \nn!\n(r − 1)!(n − r)!\nxr−1(1 − x(r))n−r, se 0 &lt; x(r) &lt; 1 (1 ≤ r ≤ n) 0, caso contrário\nObservemos que, na situação do exemplo acima, X(r) ∼ Beta(r, n − r + 1), logo, valem os resultados da distribuição Beta e, por exemplo, E(X(r)) = r/(n + 1)· Para uma densidade qualquer e somente quatro variáveis aleatórias a forma da densidade marginal, de uma qualquer estatística de ordem, é mostrada no seguinte exemplo.\nExemplo 2.8 Sejam X1, X2, X3, X4 variáveis aleatórias independentes com densidade comum f. A função de densidade conjunta das estatísticas de ordem X(1), X(2), X(3), X(4) é\nf (x\n\n\n\n, x(2)\n, x(3)\n, x(4)\n) = 4!f (x(1))f (x(2))f (x(3))f (x(4)), se x(1) &lt; x(2) &lt; x(3) &lt; x(4) · 0, caso contrário\nVamos calcular a função de densidade marginal de X(2). Temos que, se x(1) &lt; x(2) &lt; x(3) &lt; x(4) f2(x(2)) = 4! ∫ ∫ ∫ ∫ \\(F\\) (t1)f (x(2))f (t3)f (t4) dt1 dt3 dt4\n= 4!f (x(2))\n∫ x(2) ∫ +∞ [∫ +∞\nf (t4) dt4]\nf (t3)f (t1) dt3 dt1\n−∞ x(2) t3\n= 4!f (x(2))\n∫ x(2) {∫ +∞\n[1 − \\(F\\) (t3)]f (t3) dt3}\nf (t1) dt1\n−∞ x(2)\n= 4!f (x(2))\nx(2) [1 F (x )]2 2 f (t1) dt1 = 4!f (x(2))\n[1 F (x )]2 2! F (x(2))·\nEvidentemente, a expressão acima coincide com o resultado apresentado no Teorema 2.3.\nDemonstração :\nfrs(x(r), x(s)) =\n∫ x(r)\n∫ t2\n∫ x(s)\n∫ x(s) ∫ +∞\n∫ +∞\n−∞ −∞\nx(r)\nts−2\nx(s)\ntn−1\nn!f (t1) \\(F\\) (tn) dtn dts+1 dts−1 dtr+1 dt1 dtr−1\n= n!\n∫ x(r)\n∫ t2\n∫ x(s)\n∫ x(s) [1 − \\(F\\) (x(s))]n−s\n−∞ −∞\nx(r)\nts−2\n(n − s)!\n×f (t1)f (t2) \\(F\\) (x(s)) dts−1 dtr+1 dt1 dtr−1\n= n!\n[1 − \\(F\\) (x(s))]n−s (n − s)! f (x(s))\nx(r)\n−∞\nt2 f (t1) \\(F\\) (x(r))× −∞\n[F (x(s)) − \\(F\\) (x(r))]s−r−1 × (s − r − 1)! dt1 dtr−1\nn!  \n= (n − s)!(s − r − 1)![1 − \\(F\\) (x\n\n\n\n)]n−s×\n[F (x(r))]r−1\ncaso x(r) &lt; x(s).\n×[F (x(s)) − \\(F\\) (x(r))]s−r−1f (x(s))f (x(r))\n, (r − 1)!\nDe modo semelhante, podemos mostrar que a função de densidade conjunta de X(k1), , X(km) se 1 ≤ k1 &lt;\nk2 &lt; &lt; km ≤ n, 1 ≤ m ≤ n, é dada por fk1k2···km (x(k1), x(k2), , x(km)) =\n(k1\n— 1)!(k2\n— k1\nn! × — 1)! × (n − km)!\n×Fk1−1(x(k ))f (x(k ))[F (x(k )) − \\(F\\) (x(k ))]k2−k1−1f (x(k )) × × ×[F (x(k )) − \\(F\\) (x(k ))]km−1−km−2−1f (x(k ))[1 − \\(F\\) (x(k ))]n−km \\(F\\) (x(k )), caso x(k1) &lt; x(k2) &lt; &lt; x(km) e zero noutras situações.\nExemplo 2.9 (Continuação do Exemplo 2.7) Sabemos que as variáveis aleatórias \\(X_1, X_2, \\cdots , X_n\\) são independentes e tem como função de densidade comum f (x) = 1, se 0 &lt; x &lt; 1 · 0, caso contrário Então, a função de densidade conjunta de X(r) e X(s) é dada por\nfrs\n(x(r)\n, x(s)) =\n n!\nxr−1(x(r) − x(s))s−r−1(1 − x(s))n−s (r − 1)!(s − r − 1)!(n − s)!\n, se x\n\n\n\n&lt; x(s) ·\n\nonde 1 ≤ r &lt; s ≤ n.\n0, caso contrário\nUma situação mais complexa é trabalharmos com funções de estatísticas de ordem. Não temos um resultado simples para o caso de qualquer funções destas estatísticas. Mas, no exemplo a seguir, podemos encontrar um resultado interessante para o comportamento da diferença de estatísticas de ordem.\nExemplo 2.10 Sejam X(1), X(2), X(3) as estatísticas de ordem das variáveis aleatórias independentes e igualmente distribuídas X1, X2, X3 com função de densidade comum { βe−xβ, se x ≥ 0 sendo β &gt; 0. Sejam Y1 = X(3) − X(2) e Y2 = X(2). Mostraremos que Y1 e Y2 são independentes. Para isso primeiro observemos que a função de densidade conjunta de X(2) e X(3) é dada por\nf23(x, y) =\n1!0!0!\n· 0, caso contrário\nA função de densidade conjunta de (Y1, Y2) é então f (y1, y2) = 3!β2(1 − e−y2β )e−y2βe−(y1+y2)β = [3!βe−2y2β (1 e−y2β )][βe−y1β ], se 0 &lt; y &lt; + , 0 &lt; y &lt; + = · 0, caso contrário Do qual segue que Y1 e Y2 são independentes. Duas estatísticas de ordem importantes são o máximo e mínimo. Nesses casos é possível encontrar, de maneira\nanalítica, expressões para a função de distribuição. Vejamos no teorema a seguir as expressões da função de distribuição das estatísticas de ordem X(1) e X(n).\nDemonstração : Exercício.\nAcerca da função de distribuição de qualquer estatística de ordem temos o resultado a seguir.\nDemonstração : O evento {X(k) ≤ x} ocorre se, e somente se, pelo menos k dos \\(X_1, X_2, \\cdots , X_n\\) são menores ou iguais a x, por isso o somatório começa em k.\nNos dois teoremas seguintes relacionamos a distribuição condicional de estatísticas de ordem, condicionadas em outra estatística de ordem, com a distribuição de estatísticas de ordem de uma população cuja distribuição é uma forma truncada da função de distribuição da população original \\(F\\) .\nDemonstração : A densidade condicional de X(j) dado que X(i) = xi calcula-se dividindo a densidade conjunta de X(i) e X(j), dada em (2.7), pela densidade marginal de X(i), esta obtida no Teorema 2.4. Temos então que, quando\ni &lt; j ≤ n e xi ≤ xj &lt; ∞,\nf (xj|X(i)\n= x ) = fij(xi, xj) i fi(xi) (n − i)! [ \\(F\\) (xj) − \\(F\\) (xi)]j−i−1\n[ 1 − \\(F\\) (xj)]n−j \\(F\\) (xj)\n(j − i − 1)!(n − j)! 1 − \\(F\\) (xi)\n1 − \\(F\\) (xi) 1 − \\(F\\) (xi)\nO resultado segue observando que \\(F\\) (xj ) − \\(F\\) (xi) e \\(F\\) (xj ) são, respectivamente, as funções de distribuição e de 1 − \\(F\\) (xi) 1 − \\(F\\) (xi) densidade truncando à esquerda em xi a distribuição \\(F\\) .\nNa demonstração do teorema anterior utiliza-se o conceito de distribuição truncada, o que é isso? define-se a seguir este conceito e incluem-se exemplos explicativos.\nCaso a variável aleatória X seja discreta com função de probabilidade P , a distribuição truncada de X é dada por\nP (X = x|X ∈ A) =\nP (X = x, X ∈ A) P (X ∈ A)\n=  \nP (X = x) P (X = a), se x ∈ A a∈A 0, se x ∈/ A\nNa situação X do tipo contínua, com função de densidade \\(F\\) , temos que\nP (X ≤ x|X ∈ A) =\nP (X ≤ x, X ∈ A) = P (X ∈ A)\n∫(−∞,x]∩A\nf (y) dy\n· (2.8)\nConcluindo então que, a função de densidade da distribuição truncada é dada por\nh(x) =\n ∫\nf (x) f (y) dy\n, caso x ∈ A,\n· (2.9)\n 0 A\ncaso x ∈/ A\nExemplo 2.11 Suponhamos X uma variável aleatória com distribuição normal padrão e A = ( , 0]. Então, P (X A) = 1/2, dado que X é simétrica e contínua. Para a densidade truncada temos que { 2f (x), caso − ∞ &lt; x ≤ 0,\nO truncamento é especialmente importante nos casos em que a distribuição \\(F\\) em questão não tem média finita. Se X é uma variável aleatória, truncamos X em algum c &gt; 0, onde c é finito, substituindo X por Xc = X caso |X| c e zero caso |X| &gt; c. Então Xc é X truncada em c e todos os momentos de Xc existem e são finitos. Na verdade, sempre podemos selecionar c suficientemente grande para que P (X ̸= Xc) = P (|X| &gt; c),\nseja arbitrariamente pequena. A distribuição de Xc é então dada por\nP (Xc ≤ x) = P (X ≤ x| |X| ≤ c) = no caso contínuo com função de densidade \\(F\\) e é dada por\nf (y) dy (−∞,x]∩[−c,+c] , P (|X| ≤ c)\nP (Xc = x) =\n \nP (X = x)\nP (X = a) a∈[−c,+c]\n, se x ∈ [−c, +c] ,\n0, se x ∈/ [−c, +c] no caso discreto. Observemos que, para algum α &gt; 0, E(|Xc|)α ≤ cα·\nExemplo 2.12 Caso X Cauchy(0, 1), sabemos que E(X) não existe. Seja c &gt; 0 um número finito, truncando X em c definimos\nEntão\nXc =\nX, caso |X| c, · 0, caso |X| &gt; c\n1 ∫ +c 1\n2 −1\nSendo que a função de densidade truncada é dada por\n 1 1\n1 , caso x ∈ [−c, +c],\nh(x) =\nDesta expressão obtemos que\n2 1 + x2 tan−1(c) ·  0, caso x ∈/ [−c, +c]\n1    ∫ +c   x   \ne também que\nE(Xc) =\n2 tan−1(c)\n−c 1 + x2\ndx = 0,\nE(Xc)2 =\n1    ∫ +c\nx2 dx =\nc   \n— 1·\n2 tan−1(c)\n−c 1 + x2\ntan−1(c)\nPor último, temos o seguinte resultado estabelecendo novamente relação entre estatísticas de ordem e distri- buições truncadas.\nDemonstração : A densidade condicional de X(i) dado que X(j) = xj calcula-se dividindo a densidade conjunta de X(i) e X(j), dada em (2.7), pela densidade marginal de X(j), esta obtida no Teorema 2.4. Temos então que, quando i &lt; j ≤ n e xi ≤ xj &lt; ∞, (j − i)! [ \\(F\\) (xi) ]i−1 [ \\(F\\) (xj) − \\(F\\) (xi)]j−i−1 \\(F\\) (xi) O resultado segue observando que \\(F\\) (xi)/F (xj) e \\(F\\) (xi)/F (xj) são, respectivamente, as funções de distribuição e de densidade truncando à direita em xj a distribuição \\(F\\) .\n\n\n2.2.2 2.2.2 Quantis\nLembremos que a função de distribuição \\(F\\) é contínua à direita e que o número de descontinuidades é, no máximo, enumerável. Estas são propriedades importantes que farão toda diferença na definição dos quantis amostrais, por isso, demonstraremos as propriedades mencionadas da função de distribuição. A prova de que \\(F\\) é contínua à direita advém do seguinte fato F (x + hn) − \\(F\\) (x) = P (x &lt; X ≤ x + hn), onde {hn} é uma sequência de números reais estritamente positivos tais que limn→∞ hn = 0. Segue, da propriedade de continuidade da função de probabilidade,1 que lim [F (x + hn) F (x)] = 0, n→∞ e, portanto, \\(F\\) é contínua à direita. Definamos por D o conjunto dos pontos de descontinuidade de \\(F\\) e seja D = {x ∈ D : P (X = x) ≥ 1 } , onde n é um inteiro positivo. Dado que \\(F\\) ( ) F ( ) = 1, o número de elementos em Dn não pode exceder n. Logicamente ∞ D = Dn n=1 e, então, o conjunto D é enumerável. Demonstrando-se assim a segunda propriedade importante mencionada da função de distribuição. Definimos a seguir o conceito de quantil teórico e depois mostramos a forma de cálculo.\n1A função de distribuição é contínua, devido a que P lim n→∞\nAn = lim n→∞\nP (An),\nse o limite limn→∞ An existir.\nA função \\(F\\) −1(t), 0 &lt; t &lt; 1 foi definida em (1.29) e é chamada de função inversa de \\(F\\) . O seguinte teorema fornece-nos propriedades úteis. Fica claro que as propriedades apresentadas no seguinte teorema nos permitirão o cálculo dos quantis e é por isso que dedicamos atenção a este conceito.\nDemonstração : Exercício.\nExemplo 2.13\nSeja X Exponencial(). Sabemos que a função de distribuição neste caso é \\(F\\) (x) = 1 e−x/. Resulta que a expressão de qualquer um dos quantis é possível de ser encontrada de maneira exata via\nobtendo-se que\nF (ξp) = p 1 − e−ξp/= p 1 − p = e−ξp/,\nξp = −ln(1 − p)\né a expressão teórica do p-ésimo quantil. Devemos mencionar que a expressão dos quantis está bem definida, no sentido de que o resultado é sempre positivo. Isto é importante porque devemos lembrar que a distribuição exponencial está definida somente para valores positivos, então o quantil teórico deve ser positivo, já que é um dos possíveis valores da variável. Observemos que caso \\(F\\) seja contínua e estritamente crescente, \\(F\\) −1 é definida como F −1(y) = x quando y = \\(F\\) (x)· Ainda podemos observar que, se x0 é um ponto de descontinuidade de \\(F\\) e supondo que F (x−) &lt; y &lt; \\(F\\) (x0) = \\(F\\) (x+) 0 0\nvemos que, embora não exista x tal que y = \\(F\\) (x), \\(F\\) −1(y) é definido como igual a x0. A situação na qual \\(F\\) não é estritamente crescente, por exemplo, caso da variável aleatória ser discreta, podemos escrever\nF (x) =\n= y, caso a ≤ x ≤ b ·  &gt; y, caso x &gt; b\nEntão, qualquer valor a x b poderia ser escolhido como x = \\(F\\) −1(y). A convenção é que, neste caso, definimos F −1(y) = a. Em particular ξ1/2 = \\(F\\) −1(1/2), (2.10) é chamada de mediana de \\(F\\) . Observemos que ξp satisfaz a desigualdade F (ξp− ) ≤ p ≤ \\(F\\) (ξp)· Exemplo 2.14 (Continuação do Exemplo 2.13) Caso p = 1/2, a mediana amostral será ξ1/2 = −ln(1/2) = 0.6931472.",
    "crumbs": [
      "Módulo I",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estatísticas de ordem</span>"
    ]
  },
  {
    "objectID": "Modulo I/Aula 02/aula02.html#gráficos-descritivos",
    "href": "Modulo I/Aula 02/aula02.html#gráficos-descritivos",
    "title": "2  Estatísticas de ordem",
    "section": "2.4 Gráficos descritivos",
    "text": "2.4 Gráficos descritivos\nVejamos alguns conjuntos de dados disponíveis na linguagem de programação R (R Core Team, 2014), especifi- camente na libraria datasets, que nos permitiram mostrar a utilidade dos momentos amostrais para resumir as informações contidas nos dados. Para consultar estes conjuntos de dados basta digitar library(help = “datasets”) Alguns dos diversos exemplos disponíveis serão apresentados aqui.\nExemplo 2.22 (Puromicina)\nOs dados sobre a velocidade de uma reação enzimática são obtidos por Treloar (1974) e disponíveis no arquivo de dados Puromycin. O número de contagens por minuto de produto radioativo a partir da reação foi medida como uma função da concentração do substrato em partes por milhão (ppm) e a partir destas contagens a taxa\ninicial (ou velocidade) da reação foi calculada (contagens/min/min). O experimento foi realizado uma vez com a enzima tratada com puromicina e depois com a enzima não tratada. A estrutura destes dados tem 23 linhas e 3 colunas, cada coluna contendo as informações das variáveis: conc: um vector numérico de concentrações de substrato (ppm); rate: um vector numérico de taxas de reação instantânea (contagens/min/min); state: um fator com níveis treated (tratada) ou untreated (não tratada). Para a leitura e observação dos nomes das variáveis utilizamos os comandos a seguir: data(Puromycin) names(Puromycin) Uma maneira de obtermos estatísticas descritivas é utilizando as linhas de comando a seguir: summary(rate[state==’treated’]) Min. 1st Qu. Median Mean 3rd Qu. Max. 47.0 104.5 145.5 141.6 193.2 207.0 e summary(rate[state==’untreated’]) Min. 1st Qu. Median Mean 3rd Qu. Max. 51.0 85.0 115.0 110.7 137.5 160.0 para o caso da variável rate, as concentrações, obtidas as estatísticas descritivas segundo os níveis do fator state, se as concentrações foram ou não tratadas com puromicina. No caso das estatísticas descritivas acerca das concentrações de substrato, variável conc, temos: summary(conc[state==’treated’]) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.020 0.060 0.165 0.345 0.560 1.100 e summary(conc[state==’untreated’]) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.0200 0.0600 0.1100 0.2764 0.3900 1.1000 Os valores mínimos é máximos foram registrados sempre com os nomes de Min. e Max., respectivamente. O primeiro e terceiro quantis ou quantis de 25% e 75% respectivos são registrados com os nomes 1st Qu. e 3rd Qu. e, finalizando, o resumo de informações de estatísticas de posição temos os valores de medianas (Median) e médios (Mean).\nExemplo 2.23 (Rock )\nMedições em 48 amostras de rochas de um reservatório de petróleo estão disponíveis no arquivo de dados rock. Este conjunto de dados contem 48 linhas e 4 colunas numéricas, descritas a seguir: area área do espaço de poros, em pixels de 256 por 256; peri perímetro em pixels; shape perímetro/sqrt(area) perm permeabilidade em mili-Darcies. Doze amostras do núcleo de reservatórios de petróleo foram amostrados por 4 seções transversais. Cada amostra foi medida no núcleo para a permeabilidade e cada seção transversal tem uma área total de poros, perímetro total de poros e forma. A fonte destes dados é a BP Research e a análise das imagens foi de Ronit Katz, Oxford University. Na geologia, a permeabilidade é a medida da capacidade de um material (tipicamente uma rocha) para transmitir fluídos. E´ de grande importância na determinação das características de fluxo dos hidrocarbonetos em reservatórios de petróleo e gás e da água nos aquíferos. A unidade de permeabilidade é o Darcy ou, mais habitualmente, o mili- Darcy ou mD.\n\n2.4.1 2.4.1 Gráfico de Boxplot\nEm 1977, John Tukey (Tukey, 1977) publicou uma proposta que posteriormente foi reconhecida como sendo um eficiente método para mostrar cinco número que sumarizam qualquer conjunto de dados. O gráfico proposto é chamado de boxplot (também conhecido como box and whisker plot) e resume as seguintes medidas de posição estatísticas: mediana, quantis inferior e superior e os valores mínimos e máximos. Os quantis inferior e superior entendem-se serem os quantis de 25% e 75%, respectivamente. No caso do exemplo 2.22, deixamos a disposição os dados digitando attach(Puromycin) e com isso podemos mudar o nome dos níveis do fator da forma state=factor(state,labels=c(’Tratada’,’N~ao tratada’)) Então, com os comandos a seguir geramos o gráfico de boxplot, tanto para a variável rate quanto para a variável conc, estas segundo os níveis do fator state. par(mar=c(5,4,3,1)) boxplot(rate ~ state, col = grey(c(0.4,1)), main=’Taxas de reaç~ao instant^anea’)\npara o caso do rate. Observemos que a primeira linha par(mar=c(5,4,3,1)) serve somente para dimensionar a janela gráfica. Para o caso da variável conc utilizamos comandos semelhantes. par(mar=c(5,4,3,1)) boxplot(conc ~ state, col = grey(c(0.4,1)), main=’Concentraç~oes de substrato’) O resultado deste trabalho pode ser observado na Figura 2.3. Interpretemos o gráfico de boxplot. A caixa (box) propriamente contém a metade 50% dos data. O limite superior da caixa indica o percentil 75% dos dados e o limite inferior da caixa indica o percentil 25%. A distancia entre esses dois quantis é conhecida como inter-quantil. A linha na caixa indica o valor de mediana dos dados. Se a linha mediana dentro da caixa não é equidistante dos extremos, diz-se então que os dados são assimétricos. O boxplot da variável rate (esquerda na Figura 2.3) é um exemplo de dados simétricos já a situação da variável conc (direita na Figura 2.3) é um caso clássico de assimetria dos dados. Os extremos do gráfico indicam os valores mínimo e máximo, a menos que valores outliers3 estejam presentes, nesse caso o gráfico de estende ao máximo de 1.5 vezes da distância inter-quantil. Os pontos fora do gráfico são então outliers ou suspeitos de serem outliers. Mais elegante seria utilizar a biblioteca de funções ggplot2, para isso, digitamos: library(ggplot2) Para gerar os gráficos de boxplot respectivos, fazemos: par(mar=c(5,4,3,1)) qplot(state, rate, geom=c(“boxplot”, “jitter”), main=“Taxas de reaç~ao instant^anea”, xlab=““, ylab=” “) e par(mar=c(5,4,3,1)) qplot(state, conc, geom=c(”boxplot”, “jitter”), main=“Concentraç~oes de substrato”, xlab=““, ylab=” “)\n3Em estatística, outlier, valor aberrante ou valor atípico, é uma observação que apresenta um grande afastamento das demais observações em uma amostra. A existência de outliers implica, tipicamente, em prejuízos a interpretação dos resultados dos testes estatísticos aplicados as amostras.\nTaxas de reação instantânea Concentrações de substrato\nTratada Não tratada Tratada Não tratada\nFigura 2.3: Gráfico de boxplot da variável rate à esquerda e da variável conc à direita, segundo os níveis do fator state, se a enzima foi tratada ou não com puromicida. Gráfico gerado utilizando a função R boxplot,\nTaxas de reação instantânea Concentrações de substrato\n200\n0.9\n150\n0.6\n100 0.3\n50\nTratada Não tratada\n0.0\nTratada Não tratada\nFigura 2.4: Gráfico de boxplot da variável rate à esquerda e da variável conc à direita, segundo os níveis do fator state, se a enzima foi tratada ou não com puromicida. Gráfico gerado utilizando a função R qplot, opção geom=c(”boxplot”, ”jitter”).\nobtendo-se assim os gráficos na Figuras 2.4. Além de melhor qualidade gráfica acrescentamos os pontos observados no boxplot, isso permite termos uma ideia também da dispersão dos dados. Vejamos as vantagens do boxplots. Mostra graficamente a posição central dos dados (mediana) e a tendência. Fornece algum indicativo de simetria ou assimetria dos dados. Ao contrário de muitas outras formas de mostrar os dados, o boxplots mostra os outliers. Utilizando o boxplot para cada variável categórica no mesmo gráfico, pode-se facilmente comparar os dados. Esta é a situação no exemplo na Figura 2.3, podemos observar o comportamento das variáveis rate e conc segundo os níveis do fator state. Um detalhe do boxplot é que ele tende a enfatizar as caudas da distribuição, que são os pontos ao extremo nos dados. Também fornece detalhes da distribuição dos dados. Mostrar o histograma (Seção 2.4.2) em conjunto com o boxplot ajuda a entender a distribuição dos dados, constituindo estes dos gráficos ferramentas importantes na análise exploratória. Logicamente, o comportamento dos dados dentro da caixa (box), como podemos perceber nas figuras 2.3 e 2.4, permanece um mistério. Isso porque caso estejam os dados bem espalhados ou não, o gráfico boxplot continua mostrando uma caixa. Somente perceberemos algum comportamento diferente se o valor da mediana estiver mais próximo de um dos extremos desta caixa. Para tentar diminuir essa limitação foi sugerido uma melhoria, obtendo-se o chamada boxplot entalhado (notched boxplot). Com as linhas de comando a seguir se obtém os gráficos na Figura 2.5.\npar(mar=c(5,4,3,1)) boxplot(rate ~ state, col = grey(c(0.4,1)), notch=TRUE, main=’Taxas de reaç~ao instant^anea’)\ne par(mar=c(5,4,3,1)) boxplot(conc ~ state, col = grey(c(0.4,1)), notch=TRUE, main=’Concentraç~oes de substrato’)\nObserva-se que a única diferença é a inclusão da opção notch=TRUE, permanecendo todas as outras instruções iguais. Mais elaborado é o chamado violin plot, mistura de boxplot com estimação de densidade, tema este tratado na Seção 4.3. Este gráfico, introduzido no artigo Hintze & Nelson (1998), sinergicamente combina o gráfico de boxplot e a estimação da densidade, também chamado de histograma suavizado, em uma única tela que revela a estrutura encontrada nos dados. Com as linhas de comando a seguir se obtém os gráficos na Figura 2.6.\npar(mar=c(5,4,3,1)) qplot(state, rate, geom = c(“violin”, “jitter”), notch=TRUE, main=“Taxas de reaç~ao instant^anea”, xlab=““, ylab=” “)\ne par(mar=c(5,4,3,1)) qplot(state, conc, geom=c(“violin”, “jitter”), notch=TRUE, main=“Concentraç~oes de substrato”, xlab=““, ylab=” “)\nEste gráfico é similar ao boxplot excepto que mostra também a densidade de probabilidade dos dados. Pode incluir também um marcador para a média dos dados e uma caixa que indica a distância interquartil, como nos gráficos boxplot. O objetivo do gráfico violin plot é o mesmo do que o boxplot original porém, considera de alguma maneira o comportamento dos dados dentro da caixa (box). Assim, percebemos melhor a distribuição dos dados dentro do intervalo interquartil.\nTaxas de reação instantânea Concentrações de substrato\nTratada Não tratada Tratada Não tratada\nFigura 2.5: Gráfico de boxplot entalhado da variável rate à esquerda e da variável conc à direita, segundo os níveis do fator state, se a enzima foi tratada ou não com puromicida. Gráfico gerado utilizando a função R qplot, opção geom=c(”boxplot”, ”jitter”), notch=TRUE.\nTaxas de reação instantânea Concentrações de substrato\n200\n0.9\n150\n0.6\n100 0.3\n50\nTratada Não tratada\n0.0\nTratada Não tratada\nFigura 2.6: Gráfico de violin plot da variável rate à esquerda e da variável conc à direita, segundo os níveis do fator state, se a enzima foi tratada ou não com puromicida. Gráfico gerado utilizando a função R qplot, opção geom=c(”vioplot”, ”jitter”), notch=TRUE.\n\n\n2.4.2 2.4.2 Histograma\nUm histograma é uma representação gráfica da função de probabilidades ou da função de densidade de um conjunto de dados independentes e foi introduzido pela primeira vez por Karl Pearson4. A representação mais comum do histograma é um gráfico de barras verticais. A palavra histograma é de origem grega, derivada de duas: histos que pode significar testemunha no sentido de aquilo que se vê, como as barras verticais do histograma, e da também palavra grega gramma que significa desenhar, registrar ou escrever. Histograma Histograma com a curva norma\n−2 −1 0 1 2 Dados simulados\n−2 −1 0 1 2 Dados simulados\nFigura 2.7: Gráfico de histograma para dados simulados.\nPara construir um exemplo controlado do gráfico de histograma, simulamos uma amostra de tamanho 150 da distribuição normal padrão, com o comando x=rnorm(150) e, depois, construímos um gráfico colorido com as linhas de comando par(mar=c(5,4,2,1)) hist(x, breaks=12, col=“red”, xlab=“Dados simulados”, ylab=’Frequ^encia’, main=“Histograma”) box() Posteriormente, acrescentamos a este gráfico uma linha com a densidade normal par(mar=c(5,4,2,1)) h=hist(x, breaks=10, col=“red”, xlab=“Dados simulados”, ylab=’Frequ^encia’, main=“Histograma com a curva normal”) xfit=seq(min(x),max(x),length=40) yfit=dnorm(xfit,mean=mean(x),sd=sd(x)) yfit=yfitdiff(h$mids[1:2])length(x) lines(xfit, yfit, col=“blue”, lwd=2) box()\n4Pearson, K. (1895). Contributions to the Mathematical Theory of Evolution. II. Skew Variation in Homogeneous Material. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 186: 343-414.\nDesta forma geramos os gráficos na Figura 2.7. A ideia é mostrar que o histograma assemelha-se ao gráfico da densidade normal, a densidade dos dados.\nHistograma c2(6) Histograma c2(6)\n2 4 6 8 10 12 14 14 intervalos\n2 4 6 8 10 12 14 26 intervalos\nFigura 2.8: Histogramas da distribuição χ2 com 6 graus de liberdade. Número de intervalos 14 e 26, respectivamente.\nO histograma é um gráfico composto por retângulos justapostos em que a base de cada um deles corresponde ao intervalo de classe e a sua altura à respectiva frequência. A construção de histogramas tem caráter preliminar em qualquer estudo e é um importante indicador da distribuição de dados. Pode indicar se uma distribuição aproxima-se de uma densidade normal como pode indicar mistura de densidades, quando os dados apresentam várias modas. Os histogramas podem ser um mau método para determinar a forma de uma distribuição porque são fortemente influenciados pelo número de intervalos utilizados. Por exemplo, decidimos gerar 50 amostras da densidade χ2(6), da forma set.seed(5678) z=rchisq(50, df=6) Os gráficos de histogramas correspondentes com 14 e 26 intervalos são apresentados na Figura 2.8 e foram gerados com as linhas de comando\npar(mar=c(5,4,2,1)) hist(z, breaks=14, col=“blue”, main=expression(paste(’Histograma ’, chi^2,’(6)’)), ylab=’Frequ^encia’, xlab=’14 intervalos’) box()\ne\npar(mar=c(5,4,2,1)) hist(z, breaks=26, col=“blue”, main=expression(paste(’Histograma ’, chi^2,’(6)’)), ylab=’Frequ^encia’, xlab=’26 intervalos’) box()\nNa Figura 2.9 podemos observar os gráficos de histograma obtidos das variáveis descritas no Exemplo 2.23. A situação em (a) representa o caso em a distribuição dos dados de assemelha à distribuição normal, já a situação descrita no gráfico em (b) mostra-se uma mistura de densidades, percebemos a existência de duas modas. (a) Área do espaço de poros (b) Perímetro em pixels\n0 4000 8000 12000 pixels de 256 x 256\n0 1000 3000 5000\nFigura 2.9: Histogramas das variáveis no Exemplo 2.23.\nOutras situações no mesmo exemplo, mas diferentes variáveis, são descritas nos gráficos na Figura 2.10. Nessa figura apresentamos dois gráficos, chamados de (c) e (d), nesta figura. Correspondem, como podemos observar, à distribuições assimétricas e descrevem os dados coletados nas variáveis shape e perm do arquivo de dados Rock, Exemplo 2.23. Os histogramas foram pensados somente para o caso de variáveis contínuas, porém é uma descrição discreta delas. Logicamente, também podemos utiliza-los em situações de variáveis aleatórias discretas, nada impede isso. Estas figuras foram geradas utilizando a configuração padrão do comando hist, isto é, utilizamos uma maneira automática de determinar o número de intervalos, mais adiante dedicamos maior atenção a diferentes formas de calcular este número. Como pode ter sido observado, além de não ficar claro como determinar o número de intervalos nem como delimitar os intervalos, também não ficou claro o que queremos realmente observar com o gráfico desta função. Vejamos agora uma definição mais clara do histograma, esta definição nos permitirá obter propriedades impor- tantes.\n\nPerímetro/sqrt(Área) (d) Permeabilidade\n\n0.1 0.2 0.3 0.4 0.5\n0 200 600 1000 1400 mili−Darcies\nFigura 2.10: Histogramas das variáveis no Exemplo 2.23.\nFoi provado por Robertson (1967) que, dados os intervalos I1, I2, , Ik, o histograma \\(F\\) é um estimador de máxima verossimilhança5 dentre os estimadores expressados como funções simples e semicontínuas superiormente, isto se o fecho de cada intervalos contiver duas ou mais observac¸ões. Os gráficos apresentados nas figuras 2.7, 2.9 e 2.10 são histogramas também segundo a proposta de Robertson (1967). Pode-se observar que este estimador tem duas limitações importantes: a dependência do comprimento do intervalo e o fato de o histograma não constituir uma função contínua. A primeira destas limitações foi amplamente estudada por Wegman (1975). Ele provou que os pontos extremos de cada intervalo Ik devem ser coincidentes com observações e que, se o número mínimo de observações em cada intervalo aumente, conforme aumenta o tamanho da amostra, o estimador \\(F\\) é consistente6. A segunda limitação importante do histograma, isto é, o fato de ele não constituir uma função contínua, incentivou diversos estudos na procura de estimadores contínuos da função de densidade. No Capítulo 3, a Seção 4.3 dedica-se a mostrar estimadores contínuos da função de densidade.\n5Os estimadores de máxima verossimilhanc¸a serão estudados na Seção 4.2 6Estimadores consistentes serão estudados na Seção 3.1.1\nCálculo automático do número de intervalos num histograma Uma questão importante é determinar de maneira automatizada o número de intervalos disjuntos que serão utili- zados para a construção do gráfico. Uma primeira forma de escolher o número de intervalos foi dada por Sturges (1926) e que constitui a forma padrão no R. Conhecida como fórmula de Sturges é dada por k = [log2(n) + 1], (2.41) isto significa que o número de intervalos é a parte inteira do logaritmo base 2 do número de observações mais 1. Outras expressões comumente utilizadas são a fórmula de Scott (Scott, 1979) h = 3.5s/√3 n, onde s é o desvio padrão e a fórmula de Freedman Diacconi (Freedman & Diaconis, 1981) h = 2IQR(x)/√3 n, onde IRQ é a diferença entre o terceiro e o primeiro quantil.\nExemplo 2.24\nNa libraria de funções R robustbase temos disponíveis dados do teor de cálcio e do pH em amostras de colo coletadas em diferentes comunidades da região de Condroz, na Bélgica. Podemos ler estes dados digitando as linhas de comando abaixo, primeiro para escolher a libraria de funções e depois para selecionar os dados. library(robustbase) data(condroz) Temos registadas duas variáveis: Ca que registra o tero de cálcio na amostra de solo e o pH, o pH corres- pondente. Construímos histogramas da variável Ca segundo a três formas de escolha do número de intervalos e os apresentamos na Figura 2.11. Os dados deste exemplo foram publicados em: Hubert, M. and Vandervieren, E. (2006). An Adjusted Boxplot for Skewed Distributions, Technical Report TR-06-11, KULeuven, Section of Statistics, Leuven.\nSturges\nScott\nFreedman−Diaconis\n0 1000 2000 3000 4000 Ca\n0 1000 2000 3000 4000 Ca\n0 1000 2000 3000 4000 Ca\nFigura 2.11: Diferentes histogramas da variável Ca no Exemplo 2.24.\n\n\n2.4.3 2.4.3 Gráficos para verificar normalidade\nUm primeiro gráfico chamado de qq-norm permite a comparação de duas distribuições de probabilidades traçando seus quantis uns contra os outros. Depois exploramos um gráfico mais recente, conhecido como worm plot (gráfico de minhoca), consistindo numa determinada coleção de de qq-norm.\nQQ-norm O gráfico quantil-quantil ou qq-plot, proposto por Wilk & Gnanadesikan (1968), é um dispositivo gráfico explo- ratório utilizado para verificar a validade de um pressuposto de distribuição para um conjunto de dados. Em geral, a ideia básica é a de calcular o valor teoricamente esperado para cada ponto de dados com base na distribuição em questão. Se os dados de fato seguirem a distribuição assumida os pontos deste gráfico formarão aproximadamente uma linha reta. Percebemos que podemos verificar com este gráfico qualquer densidade contínua, eventualmente pode ser uti- lizado também para funções de probabilidade. O qq-plot vai apresentar-se como uma linha reta se a densidade assumida estiver correta. Vejamos o caso particular de verificarmos se a densidade é normal, nesta situação o gráfico qq-plot será chamado de qq-norm. Primeiro consideraremos a situação da densidade normal padrão. Seja z1, z2, , zn uma amostra aleatória de uma distribuição normal com média µ = 0 e desvio padrão σ = 1. As estatísticas de ordem amostrais são z(1) ≤ z(2) ≤ ≤ z(n)· Estes valores desempenharão o papel dos quantis da amostra. Agora, quais devemos tomar como os quantis teóricas correspondentes? Se a função de distribuição cumulada da densidade normal padrão fosse denotada por Φ, usando a notação quantil, se ξq é o q-ésimo quantil de uma distribuição normal, então Φ(ξq) = q, ou seja, a probabilidade de uma amostra normal ser inferior a ξq é, de fato, apenas q. Considere o primeiro valor ordenado z(1). O que podemos esperar que o valor Φ(z(1)) seja? Intuitivamente, esperamos que essa seja a probabilidade de assumir um valor no intervalo (0, 1/n). Do mesmo modo, espera-se que Φ(z(2)) seja a probabilidade de assumir um valor no intervalo (1/n, 2/n). Continuando, esperamos que Φ(z(n)) seja a probabilidade de assumir um valor no intervalo (n 1)/n, 1). Assim, o quantil teórico desejamos seja definido pelo inverso da função de distribuição acumulada normal padrão. Em particular, o quantil teórico correspondente ao quantil empírico z(i) deve ser\npara i = 1, 2, , n.\nξ = q i − 0, 5 , q n\nQQ−plot nomal\nQQ−plot nomal\nQQ−plot nomal\n−3 −2 −1 0 1 2 3 Quantis teóricos\n−3 −2 −1 0 1 2 3 Quantis teóricos\n−3 −2 −1 0 1 2 3 Quantis teóricos\nFigura 2.12: Diferentes qqplot para dados normais.\nNa Figura 2.12, a esquerda acima exibimos o qq-norm de uma pequena amostra normal de tamanho 5. Os restantes quadros na Figura 2.12 exibem as plotagens de qq-norm para amostras normais de tamanhos n = 100 e\nn = 1000, respectivamente. Como o tamanho da amostra aumenta, os pontos encontram-se mais perto da linha y = x. Estes gráficos (Figura 2.12) foram gerados utilizando as linhas de comando: set.seed(1278) x=rnorm(5) qqnorm(x, xlim=c(-3,3), ylim=c(-3,3), cex=0.6, pch=19, ylab=’Quantis amostrais’, xlab=’Quantis teóricos’, main=’QQ-plot nomal’) qqline(x,col=“red”) text(-1,2,’n=5’) para a situação de amostra de tamanho 5. A primeira linha de comando serve para fixar o gerador de números laetórios e, dessa forma, podermos simular sempre a mesma amostra e reproduzir o gráfico idêntico. Nas outras situações somente muda-se o tamanho da amostra que se quer gerar.\nQQ−plot nomal QQ−plot nomal\n−3 −2 −1 0 1 2 3 Quantis teóricos\n−3 −2 −1 0 1 2 3 Quantis teóricos\nFigura 2.13: Diferentes qqplot para dados não normais. Assim, os comandos para gerar o segundo e terceiro gráficos são: x=rnorm(100) qqnorm(x, xlim=c(-3,3), ylim=c(-3,3), cex=0.6, pch=19, ylab=’Quantis amostrais’, xlab=’Quantis teóricos’, main=’QQ-plot nomal’) qqline(x,col=“red”) text(-1,2,’n=100’)\nx=rnorm(1000) qqnorm(x, xlim=c(-3,3), ylim=c(-3,3), cex=0.6, pch=19, ylab=’Quantis amostrais’, xlab=’Quantis teóricos’, main=’QQ-plot nomal’) qqline(x,col=“red”) text(-1,2,’n=1000’) Caso os dados não forem padronizados bastar aplicar a transformação (X − µ)/σ, onde X representa os dados originais e µb e σb representam os estimadores dos parâmetros µ e σ, respectivamente.\nEstes gráficos podem indicar afastamentos da normalidade por isso apresentamos duas situações de dados não simétricos e com cuadas pesadas. Na Figura 2.13, mostramos o que acontece se os dados forem da distribuição t-Student(8) e da distribuição χ2(5), sempre de tamanho n = 1000. Observe, em particular, que os dados a partir da distribuição t-Student seguem a curva normal bem de perto até os últimos pontos em cada extremo. Na outra situação o afastamento da distribuição normal é evidente. Foi mencionado que o qq-norm é uma situação particular do qq-plot devido a este último permitir comparar os quantis amostrais com os quantis distribucionais. Com isto queremos dizer que o qq-plot serve para verificar se os dados forem t-Student ou χ2(5), por exemplo. Na Figura 2.14 apresentamos a aparência dos gráficos qq-plot caso queira-se verificar se as amostras seguem distribuição t-Student(8) ou χ2(5), respectivamente.\nQQ plot para t−Student(8)\n−4 −2 0 2 4 t−Student(8)\nQQ plot para c2(5)\n0 5 10 15 20 c2(5)\nFigura 2.14: Diferentes qqplot para dados não normais. Os gráficos na Figura 2.14 foram gerados pelas linhas de comandos qqplot(qt(ppoints(1000), df = 8), x, cex=0.6, pch=19, main = “QQ plot para t-Student(8)”, xlab=“t-Student(8)”) qqline(x, distribution = function(p) qt(p, df = 8), prob = c(0.1, 0.6), col = 2) no caso t-Student(8) e qqplot(qchisq(ppoints(1000), df = 5), x, cex=0.6, pch=19, main = expression(“QQ plot para” ~~ {chi^2}(5)), xlab=expression({chi^2}(5))) qqline(x, distribution = function(p) qchisq(p, df = 5), prob = c(0.1, 0.6), col = 2) para o caso χ2(5).\nWorn plot O worm-plot é uma série de parcelas de gráficos qq-plot retificados. Constitui uma ferramenta de diagnóstico para visualização de quão bem um modelo estatístico se ajusta aos dados, para encontrar locais em que o ajuste pode ser melhorado e para comparar o ajuste de diferentes modelos. Na Figura 2.15 mostramos este gráfico para duas situações: a esquerda os dados são normais e a direita os dados são t-Student com 8 graus de liberdade. Nesta situação aparece bem a qualidade da observação com esta\n−4 −2 0 2 4 Unit normal quantile\n−4 −2 0 2 4 Unit normal quantile\nFigura 2.15: Diferentes worm-plot para dados normais.\nfigura. Se os dados forem normais o curva worm-plot ou gráfico de minhoca deve aparentar um verme achatado, os pontos próximos a curva vermelha e com poucas oscilações. Quando aplicamos este gráfico ao caso t-Student percebemos uma oscilação grande no verme e com pontos fugindo da banda de confiança. Isso comprova que os dados não seguem como referência a distribuição normal.\n−4 −2 0 2 4 Unit normal quantile\n−4 −2 0 2 4 Unit normal quantile\nFigura 2.16: Diferentes worm-plot para dados não normais. As linhas a seguir mostram os comandos necessários para gerar os gráficos na Figura 2.15. Utilizamos a libraria de comandos R gamlss (Rigby & Stasinopoulos, 2005).\nlibrary(gamlss) x=rnorm(1000) wp(gamlss(x~1), cex=0.6) x=rt(1000, df=8) wp(gamlss(x~1), cex=0.6)\nNa Figura 2.16, a esquerda temos o caso de dados com distribuição χ2(5) e a direita dados com distribuição Cauchy padrão. Nestas situações fica claro que os dados não são normais. Oa gráficos na figura foram gerados pelas linhas de comando a seguir. x=rchisq(1000, df=5) wp(gamlss(x~1), cex=0.6) x=rcauchy(1000) wp(gamlss(x~1), cex=0.6)",
    "crumbs": [
      "Módulo I",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estatísticas de ordem</span>"
    ]
  },
  {
    "objectID": "Modulo I/Aula 02/aula02.html#exercícios",
    "href": "Modulo I/Aula 02/aula02.html#exercícios",
    "title": "2  Estatísticas de ordem",
    "section": "2.5 Exercícios",
    "text": "2.5 Exercícios\nExercícios da Seção 2.1 1. Seja X ∼ Bernoulli( 1 ) e considere todas as possíveis amostras aleatórias de tamanho n = 3. Calcule Xn e S2 cada uma das\n2 n oito amostras. Encontre a função de probabilidade de Xn e S2. 2. Um dado é lançado. Seja X o valor da face superior que aparece e X1, X2 duas observações independentes de X. Encontre a função de probabilidade de Xn. 3. Seja X1, , Xn uma amostra aleatória de alguma população. Mostre que (n − 1)Sn\nmax |Xi Xn| &lt; 1≤i≤n\nonde Sn é a raiz quadrada positiva da variância amostral S2.\n√n ,\nExercícios da Seção 2.2 1. Seja (X(1), X(2), , X(n)) o conjunto das estatísticas de ordem de n variáveis aleatórias independentes \\(X_1, X_2, \\cdots , X_n\\) com função de densidade comum\nf (x) =\nβe−xβ, se x 0 · 0, caso contrário\n\nMostre que X(s) e X(r) − X(s) são independentes para quaisquer r &gt; s.\nEncontre a função de densidade de X(r+1) − X(r).\nSeja Z1 = nX(1), Z2 = (n − 1)(X(2) − X(1)), Z3 = (n − 2)(X(3) − X(2)), …, Zn = ((X(n) − X(n−1))). Prove que (Z1, Z2, , Zn) e (\\(X_1, X_2, \\cdots , X_n\\)) são identicamente distribuídas.\n\n\nProvar o Teorema 2.1\nSejam \\(X_1, X_2, \\cdots , X_n\\) variáveis aleatórias com distribuição geométrica de parâmetros p1, p2, , pn, respectivamente. Prove que Nn = min(\\(X_1, X_2, \\cdots , X_n\\)) têm também distribuição geométrica de parâmetro n p = 1 − (1 − pi)· i=1\nAs X1, , Xn variáveis aleatórias independentes e identicamente distribuídas tem por função de probabilidade BN (1; p) se, e somente se, Nn = min(X1, , Xn) tem distribuição geométrica de parâmetro 1 − (1 − p)n.\nSejam \\(X_1, X_2, \\cdots , X_n\\) variáveis aleatórias independentes e igualmente distribuídas com função de densidade comum\n\nf (x) =\nσ 0, se x ≤ \nMostre que X(1), X(2) − X(1), X(3) − X(2), , X(n) − X(n−1) são independentes. 6. Sejam \\(X_1, X_2, \\cdots , X_n\\) variáveis aleatórias independentes e igualmente distribuídas com função de distribuição acumulada comum\nF (t) =\ntα, se 0 &lt; t &lt; 1  1, se t ≥ 1\npara α &gt; 0. Mostre que X(i)/X(n), i = 1, 2, , n − 1 e X(n) são independentes. 7. Sejam X1 e X2 duas variáveis aleatórias discretas independentes com função de probabilidade comum P (X = x) = (1 − )x−1, x = 1, 2, ; 0 &lt; &lt; 1· Mostre que X(1) e X(2) − X(1) são independentes. 8. Sejam X1, , Xn duas variáveis aleatórias independentes com função de densidade comum \\(F\\) . Encontre a função de densidade de X(1) e de X(n).\n\nSejam X(1), X(2), , X(n) as estatísticas de ordem de n variáveis aleatórias independentes e igualmente distribuídas \\(X_1, X_2, \\cdots , X_n\\) com função de densidade comum f (x) = 1 se 0 &lt; x &lt; 1 · 0, caso contrário Prove que Y1 = X(1)/X(2), Y2 = X(2)/X(3), , Yn−1 = X(n−1)/X(n) e Yn = X(n) são independentes. Encontre a função de densidade conjunta de Y1, Y2, , Yn.\nSejam X1.X2, , Xn variáveis aleatórias independentes identicamente distribuídas não negativas contínuas. Prove que se E|X| &lt; ∞, então E|X(r)| &lt; ∞. Definamos Mn = X(n) = max(\\(X_1, X_2, \\cdots , X_n\\)). Mostre que ∫ ∞\n\nEncontre E(Mn) em cada uma das seguintes situações: a) Xk tem como função de distribuição comum \\(F\\) (x) = 1 − e−xβ, se x ≥ 0. b) Xk tem como função de distribuição comum \\(F\\) (x) = x, se 0 &lt; x &lt; 1.\n\nProvar que, qualquer seja a amostra aleatória X1.X2, , Xn sempre cumpre-se que X(1) ≤ X ≤ X(n).\nDemonstrar o Teorema 2.5.\nDemonstrar o Teorema 2.9.\n\nExercícios da Seção 2.3 1. Demonstre o Corolário 2.17. 2. Demonstre o Corolário 2.18.\n\nSeja X1, , Xn uma amostra aleatória Poisson(). Encontre Var(S2) e compare-a com Var(X). Observe que E(X) = = E(S2).\nSeja \\(X_1, X_2, \\cdots , X_n\\) uma amostra aleatória da função de distribuição \\(F\\) e seja \\(F\\) ∗(x) a função de distribuição amostral. Encontre Cov[F ∗(x), \\(F\\) ∗(y)] para números reais fixos x, y. n n\nSeja \\(F\\) ∗ a função de distribuição empírica de uma amostra aleatória com função de distribuição teórica \\(F\\) . Prove que { ∗ ϵ } 1\nSejam \\(X_1, X_2, \\cdots , X_n\\) n observacões independentes da variável aleatória X. Encontre a distribuição amostral de X, a média amostral, se:\n\n\nX ∼ P ();\nX ∼ Cauchy(1, 0);\nX ∼ χ2(m).\n\n\nSeja X1, , Xn uma amostra aleatória Poisson(). Encontre Var(S2) e compare-a com Var(X). Observe que E(X) = = E(S2).\nDemonstre o Teorema 2.23. [Dica: para quaisquer reais µ e σ &gt; 0, encontre a função de densidade de (U(r) − µ)/σ e mostre que as variáveis padronizadas de U(r), (U(r) − µ)/σ, são assintoticamente N (0, 1) sob as condições do teorema.]\nProvar que o momentos amostral central b1 é sempre zero.",
    "crumbs": [
      "Módulo I",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estatísticas de ordem</span>"
    ]
  },
  {
    "objectID": "Modulo I/modulo-01.html#estimação-de-densidades",
    "href": "Modulo I/modulo-01.html#estimação-de-densidades",
    "title": "Módulo I",
    "section": "",
    "text": "Definição. Seja \\(f\\) a derivada de \\(F\\); por isso pode-se expressar como \\[\nf(x)=\\lim_{h\\to 0}\\frac{F(x+h)-F(x-h)}{2h}\\cdot\n\\] Então, dizemos que \\(\\widehat{f}\\), definido por \\[\n\\widehat{f}(x)=\\frac{\\widehat{F}(x+h)-\\widehat{F}(x-h)}{2h},\n\\] é o histograma, sendo que \\(\\widehat{F}\\) é a função de distribuição empírica.\n\n\n\n\n\n\n\n\nTeorema\n\n\n\nSeja \\(f\\) a função de densidade da função de distribuição \\(F\\). Então, com probabilidade 1,\n\\[\n    \\widehat{f}(x)\\sim Binomial(n,p),\n\\] com \\(p=F(x+h)-F(x-h)\\). Assim, o comportamento assintótico do histograma pode ser derivado da distribuição binomial como \\[\n    \\mbox{E}\\big(\\widehat{f}(x)\\big)=\\frac{F(x+h)-F(x-h)}{2h}\n\\] e \\[\n    \\mbox{Var}\\big(\\widehat{f}(x)\\big)=\\frac{p(1-p)}{4nh^2}\\cdot\n\\]\n\n\n\n\n\n\n\n\n\nDefinição. O estimador kernel da função de densidade é dado por \\[\n\\widehat{f}(x)=\\frac{1}{nh_n}\\sum_{i=1}^n K\\Big( \\frac{x-X_i}{h_n}\\Big),\n\\] onde \\(K(\\cdot)\\) é uma função conhecida como kernel.\n\n\n\n\n\n\n\n\n\n\n\nTeorema\n\n\n\nSuponhamos que \\(f\\) seja contínua e limitada. Então o viés do estimador kernel de densidade converge a zero quando \\(h_n\\to 0\\), para todo \\(x\\).\n\n\n\n\n\n\n\n\n\nTeorema\n\n\n\nSuponhamos que \\(f\\) seja contínua três vezes diferenciável, com terceira derivada limitada na vizinhança de \\(x\\) e \\(K\\) satisfazendo \\[\n    \\int K^2(u)\\mbox{d}u&lt;\\infty \\qquad \\mbox{e} \\qquad \\int |u|^3K(u)\\mbox{d}u&lt;\\infty\\cdot\n\\]\n\n\nSe \\(h_n\\to 0\\) quando \\(n\\to\\infty\\), temos que \\[\n        \\mbox{viés}\\big(\\widehat{f}(x)\\big) = \\frac{h_n^2}{2}f''(x)\\int u^2K(u)\\mbox{d}u + o(h_n^2)\\cdot\n    \\]\n\n\nSe, além disso \\(nh_n\\to\\infty\\) quando \\(n\\to\\infty\\), então temos \\[\n        \\mbox{Var}\\big(\\widehat{f}(x)\\big) = \\frac{f(x)}{nh_n}\\int K^2(u)\\mbox{d}u + o\\big((nh_n)^{-1}\\big)\\cdot\n    \\]",
    "crumbs": [
      "Módulo I"
    ]
  },
  {
    "objectID": "Modulo I/modulo-01.html#teorema-6",
    "href": "Modulo I/modulo-01.html#teorema-6",
    "title": "Módulo I",
    "section": "Teorema",
    "text": "Teorema\nSuponhamos que \\(f\\) seja contínua três vezes diferenciável, com terceira derivada limitada na vizinhança de \\(x\\) e \\(K\\) satisfazendo \\[\n    \\int K^2(u)\\mbox{d}u&lt;\\infty \\qquad \\mbox{e} \\qquad \\int |u|^3K(u)\\mbox{d}u&lt;\\infty\\cdot\n\\]\n\n\n\nSe \\(h_n\\to 0\\) quando \\(n\\to\\infty\\), temos que \\[\n      \\mbox{viés}\\big(\\widehat{f}(x)\\big) = \\frac{h_n^2}{2}f''(x)\\int u^2K(u)\\mbox{d}u + o(h_n^2)\\cdot\n  \\]\n\n\n\nSe, além disso \\(nh_n\\to\\infty\\) quando \\(n\\to\\infty\\), então temos \\[\n    \\mbox{Var}\\big(\\widehat{f}(x)\\big) = \\frac{f(x)}{nh_n}\\int K^2(u)\\mbox{d}u + o\\big((nh_n)^{-1}\\big)\\cdot\n\\]\n\n\n\n\nDemonstração. A demonstração é baseada na expansão de Taylor, \\[\n    f(x-h_nu) = f(x)-h_nuf'(x)+\\frac{h_n^2u^2}{2}f''(x)-\\frac{h_n^3u^3}{6}f'''(\\epsilon),\n\\] sendo \\(\\epsilon\\) fica entre \\(x-h_nu\\) e \\(x\\). Os detalhes são deixados como um exercício.\nUma medida de precisão do estimador é o erro quadrático médio (EQM), dado por \\[\n    EQM\\big(\\widehat{f}(x)\\big) = \\mbox{E}\\big(\\widehat{f}(x)-f(x)\\big)^2\\cdot\n\\] é fácil mostrar que o \\(EQM\\) combina o viés e a variância de tal maneira que \\[\n    EQM\\big(\\widehat{f}(x)\\big) = \\mbox{viés}\\big(\\widehat{f}(x)\\big)^2 + \\mbox{Var}\\big(\\widehat{f}(x)\\big)\\cdot\n\\] Vemos que, sob as condiçotilde;es \\(h_n\\to 0\\) e \\(nh_n\\to \\infty\\) e se ignorarmos os termos de baixa ordem, temos \\[\n    EQM\\big(\\widehat{f}(x)\\big) \\approx \\frac{h_n^4}{4}\\big(f''(x)\\big)^2\\tau^4 + \\frac{f(x)}{nh_n}\\gamma^2,\n\\] onde \\(\\tau^2=\\int u^2K(u)\\mbox{d}u\\) e \\(\\gamma^2=\\int K^2(u)\\mbox{d}u\\). O termo à direita da expressão acima é minimizada quando \\[\n    h_n = \\left( \\frac{\\gamma^2f(x)}{\\tau^4\\big(f''(x)\\big)^2} \\right)^{\\frac{1}{5}}n^{-\\frac{1}{5}}\\cdot\n\\] Note ainda que a expressão acima não é a solução ideal, isso porque \\(f\\) é desconhecida na prática. No entanto, dá-nos pelo menos alguma ideia sobre a taxa ideal de convergência a zero, sendo esta \\(h_n=O(n^{-\\frac{1}{5}})\\).\nQuando \\(f\\) é desconhecida, uma abordagem natural seria substituí-lo por um estimador e, assim, obter uma largura de banda ideal estimada. Uma complicação é que a largura de banda ideal depende de \\(x\\) mas, idealmente, gostaríamos de usar uma largura de banda que funcionasse para diferentes \\(x\\) dentro de um certo intervalo, se não todos os \\(x\\). Para obter uma largura de banda ideal que não depende de \\(x\\), integramos os dois lados da expressão de \\(EQM\\) em relação a \\(x\\). Isto nos leva a \\[\n    \\int EQM\\big(\\widehat{f}(x)\\big)\\mbox{d}x \\approx \\frac{\\tau^4h_n^4}{4}\\int \\big(f''(x)\\big)^2 \\mbox{d}x +\n    \\frac{\\gamma^2}{nh_n}\\int f(x)\\mbox{d}x \\, = \\, \\frac{\\tau^4\\theta^2h_n^4}{4} + \\frac{\\gamma^2}{nh_n},\n\\] com $2=(f’’(x))2 x \\(. Pelo mesmo argumento, o lado direito acima é minimizado quando\\)$ h_n = ( ){}n{-} \\[\nDesta vez, o $h_n$ ideal não depende de $x$. Além disso, a integral do $EQM$ ou o $IEQM$ mínimo é dado por\n\\] IEQM , = , EQM((x))x , = , ( ^2 ){} n^{-} $$\nUma implicação é a seguinte. Note que o \\(IEQM\\) depende do kernel \\(K\\) através de \\(c_K=\\big( \\tau\\gamma^2 \\big)^\\frac{4}{5}\\). Mostrou-se que para os kernels comumente usados, tais como aqueles listados, o desempenho dos estimadores de kernel correspondentes é quase o mesmo em termos dos valores de \\(c_K\\). Voltando ao problema sobre a estimação da largura de banda ideal, vemos que tudo o que precisamos é encontrar um estimador consistente de \\(\\theta^2\\). Se \\(f\\) é a função de densidade da distribuição normal com desvio padrão \\(\\sigma\\), então pode ser mostrado que \\(\\theta^2=3/8\\sqrt{\\pi}\\sigma^5\\). Naturalmente, se alguém souber que \\(f\\) é normal, então a estimação da densidade não-paramétrica não seria necessária, porque um método paramétrico provavelmente seria melhor. Em geral, pode-se expandir \\(f\\) em torno da densidade gaussiana usando a expansão de Edgeworth.\nUtilizando a abordagem acima, Hjort and Jones (1996) obteveram o seguinte estimador ótimo para a largura de banda \\[\n    \\widehat{h}_n = \\widehat{h}_0\\left(1+\\frac{35}{48}\\widehat{\\gamma}_4+\\frac{35}{32}\\widehat{\\gamma}_3^2+\n        \\frac{385}{1024}\\widehat{\\gamma}_4^2\\right)^{-\\frac{1}{5}},\n\\] onde \\(\\widehat{h}_0\\) é a estimativa ideal da largura de banda assumindo que \\(f\\) é normal, isto é, com \\(\\theta^2\\) substituído por \\(3/8\\sqrt{\\pi}\\sigma^5\\) ou mais explicitamente \\[\n    \\widehat{h}_0=1.06\\left(\\frac{\\widehat{\\sigma}}{n^{\\frac{1}{5}}}\\right),\n\\] chamamos \\(\\widehat{h}_0\\) a largura de banda da linha de base e \\(\\widehat{\\sigma}^2\\) é a variância amostral dada por \\[\n    \\widehat{\\sigma}^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\overline{X})^2\\cdot\n\\] Além disso, \\(\\widehat{\\gamma}_3\\) e \\(\\widehat{\\gamma}_4\\) são os estimadores dos coefcientes de assimetria de amostra e curtose, dado por \\[\n    \\widehat{\\gamma}_3=\\frac{1}{(n-1)\\widehat{\\sigma}^3}\\sum_{i=1}^n (X_i-\\overline{X})^3\n\\] e \\[\n    \\widehat{\\gamma}_4=\\frac{1}{(n-1)\\widehat{\\sigma}^4}\\sum_{i=1}^n (X_i-\\overline{X})^4-3,\n\\] respectivamente. Houve outras abordagens para a seleção da largura de banda ótima, incluindo o método de validação cruzada. Ambos procedimentos foram programados na função density.\nExemplo. Utilaremos os dados simulados da distribuíão \\(N(0,1)\\) no exemplo anterior para com isso mostrarmos o histograma e o estimador Kernel da função de densidade.\n\n\nCode\nset.seed(1340)\nx = rnorm(50)\npar(mar=c(4,2,1,1))\nhist(x, main = NULL, freq = FALSE, breaks = \"Sturges\")\nbox()\nlines(density(x, bw = \"nrd0\"), col = \"red\")\n\n\n\n\n\n\n\n\n\nCode\npar(mar=c(4,2,1,1))\nhist(x, main = NULL, freq = FALSE, breaks = \"Scott\")\nbox()\nlines(density(x, bw = \"bcv\"), col = \"red\")\n\n\nWarning in bw.bcv(x): mínimo ocorreu em uma das extremidades do intervalo",
    "crumbs": [
      "Módulo I"
    ]
  },
  {
    "objectID": "Modulo I/modulo-01.html#exercícios",
    "href": "Modulo I/modulo-01.html#exercícios",
    "title": "Módulo I",
    "section": "Exercícios",
    "text": "Exercícios\n\nSeja \\(T(X_1,\\cdots,X_n)\\) uma estatística simétrica nas observaçóes. Mostre que \\(T\\) pode ser escrita como função das estatísticas de ordem. Por outro lado, se \\(T(X_1,\\cdots,X_n)\\) pode ser escrita como função das estatísticas de ordem, \\(T\\) é simétrica nas observações.\nSejam \\(X_1,X_2,\\cdots,X_m\\) e \\(Y_1,Y_2,\\cdots,Y_n\\) amostras independentes de duas distribuições absolutamente contínuas. Encontre o estimador não viciado de mínima variância de:\n\n\n\\(\\mbox{E}(XY)\\)\n\\(\\mbox{Var}(X+Y)\\)\n\n\nSeja \\((X_1,Y_1), (X_2,Y_2), \\cdots, (X_n,Y_n)\\) uma amostra aleatória com distribuição absolutamente contínua bivariada. Encontre o estimador não viciado de mínima variância de:\n\n\n\\(\\mbox{E}(XY)\\)\n\\(\\mbox{Var}(X+Y)\\)\n\n\nConsidere \\((\\mathcal{R},\\mathcal{B},P_\\theta)\\) um espaço de probabilidade e \\(\\mathcal{P}=\\{P_\\theta \\, : \\theta\\in\\Theta\\}\\). Seja \\(A\\) um elemento da \\(\\sigma\\)-álgebra de Borel e considere \\(d(\\theta)=P_\\theta(A)\\).\n\n\nA função \\(d\\) é estimável? Se sim, qual é o grau?\nEncontre o estimador não viciado de mínima variância de \\(d\\), baseado em uma amostra de tamanho \\(n\\) e assumindo que \\(\\mathcal{P}\\) seja a classe de todas as distribuições contínuas.",
    "crumbs": [
      "Módulo I"
    ]
  },
  {
    "objectID": "ModuloI/index.html",
    "href": "ModuloI/index.html",
    "title": "Módulo I",
    "section": "",
    "text": "Estimação de densidades\nDe certa forma, problemas de estimação não-paramétrica são extensões de problemas de estimação paramétrica, mas a natureza do primeiro é bem diferente do último. Considere, por exemplo, a situação de observações independentes identicamente distribuídas, digamos \\(X_1,X_2,\\cdots,X_n\\). Em um problema paramétrico, assumimos que a distribuição de \\(X_i\\) é \\(F(\\cdot;\\theta)\\), a qual é totalmente especificada até o vetor de parâmetros \\(\\theta\\); então o problema é essencialmente a estimaçã de \\(\\theta\\). Em um problema não-paramétrico, a distribuição é totalmente desconhecida com, talvez, algumas restriçõs em propriedades gerais e, portanto, é denotada por \\(F\\).\nAqui consideramos estimadores de \\(F\\) em termos de função de densidade \\(f\\). A função de densidade tem a vantagem de fornecer uma representação visualmente mais informativa da distribuição subjacente. Por exemplo, o histograma geralmente dá uma ideia aproximada da forma da distribuição. Este último ficou como o único estimador de densidade não paramétrico até 1950. Por essa razão, nossa discussão começará com os histogramas.\nEmbora o histograma seja usado extensivamente, não é tão frequente que seja necessária uma definição matemática. Uma maneira de defini-lo é através da função de densidade empírica.\nO parâmetro \\(h\\) é chamado de largura de banda. Podemos escrever \\(\\widehat{f}\\), definido acima como, \\[\n    \\widehat{f}(x)=\\frac{1}{2nh}\\sum_{i=1}^n \\pmb{1}_{(x-h;x+h)}(X_i)\\cdot\n\\] Podemos excrever a função de densidade como $f(x)=_{h} ( F(x+h)-F(x-h) ) $, mas não se pode definir daqui o histograma porque, então, esse limite é zero ou infinito e assim em algum momento é preciso parar, em outras palavras, não se pode chegar muito perto de zero.\nDemonstração. Exercício.\nDeste teorema segue que \\(\\widehat{f}(x)\\) é um estimador consistente pontual de \\(f(x)\\) quando \\(h\\to 0\\) e \\(nh\\to \\infty\\). A seguir, o processo de limite é entendido como \\(h=h_n\\), de maneira que \\(h_n\\to 0\\) e \\(nh_n\\to \\infty\\). Estas condições podem ser interpretadas como se fosse necessário \\(h_n\\) ir a zero, mas não muito rápido. Isso é exatamente o que temos especulado, exceto que agora temos a taxa exata de convergência, que pode ser escrita como \\(h_n=o(n)\\).\nExemplo. Utilaremos dados simulados da distribuíão \\(N(0,1)\\), com isso mostramos o histograma destes 50 dados utilizando duas formas diferentes de encontrarmos uma expressão para \\(h_n\\), a chamada largura de banda.\nCode\nset.seed(1340)\nx = rnorm(50)\npar(mar=c(4,2,1,1))\nhist(x, main = NULL, freq = FALSE, breaks = \"Sturges\")\nbox()\n\n\n\n\n\n\n\n\n\nCode\npar(mar=c(4,2,1,1))\nhist(x, main = NULL, freq = FALSE, breaks = \"Scott\")\nbox()\nNeste exemplo utilizamos duas formas de escolher a largura de banda \\(h_n\\) dentre três diferentes possibilidades programadas na função hist. Por padrão escolhe-se breaks = “Sturges”, porposto por Sturges (1929), o qual sugere que \\[\n    h_n=\\frac{\\max(X_1,\\cdots,X_n)-\\min(X_1,\\cdots,X_n)}{1+3.322\\ln(n)}\\cdot\n\\] A segunda situação indica que o qual significa que se os dados provêm da distribuição Normal temos que \\(h_n=3.49 s n^{-1/3}\\) sendo \\(s\\) o desvio padrão estimado. Esta proposta deve-se à Scott (1979).\nEmbora o histograma é um estimador consistente quando \\(h_n\\to 0\\) e \\(nh_n\\to \\infty\\), verifica-se que se pode fazer melhor. A melhoria também é motivada por uma preocupação prática: o histograma não é uma função suave, uma propriedade que se pode esperar que qualquer função de densidade real tenha.\nÉ tipicamente assumido que \\(K\\) seja não-negativa, simétrica em torno de zero e satisfaz \\(\\int K(u)\\mbox{d}u = 1\\). Claro que o histograma é um caso especial do estimador do kernel se \\(K\\) for escolhido como a função de densidade da distribuição $ Uniforme(-1,1)$. O último não é uma função suave e é por isso que o histograma não é suave; mas escolhendo \\(K\\) como uma função suave, tem-se um estimador de \\(f\\) que seja suave.\nPor exemplo, escolhendo a função de densidade \\(N(0,1)\\), temos por resultado o conhecido como kernel Gaussiano e assim também utilizando a densidade de densidade \\(Beta\\) simétrica, dada por \\[\n    K(u)=\\frac{\\Gamma(\\nu+3/2)}{\\Gamma(1/2)\\Gamma(\\nu+1)}(1-u^2)^\\nu, \\qquad -1&lt; u &lt; 1,\n\\] e \\(K(u)=0\\) caso contrário. Os casos especiais \\(\\nu=0,1,2,3\\) correspondem às funções kernel uniforme, Epanechnikov, biweight e triweight, respectivamente.\nCode\nkernels = eval(formals(density.default)$kernel)\nplot (density(0, bw = 1), xlab = \"\", main = \"Diferentes kernel em R\")\nfor(i in 2:length(kernels)) lines(density(0, bw = 1, kernel =  kernels[i]), col = i)\nlegend(1.5,.4, legend = kernels, col = seq(kernels), lty = 1, cex = .8, y.intersp = 1)\n\n\n\n\n\n\n\n\n\nCode\nh.f = sapply(kernels, function(k) density(kernel = k, give.Rkern = TRUE))\nh.f = (h.f[\"gaussian\"] / h.f)^ .2\nh.f\n\n\n    gaussian epanechnikov  rectangular   triangular     biweight       cosine \n   1.0000000    1.0100567    0.9953989    1.0071923    1.0088217    1.0079575 \n   optcosine \n   1.0099458 \n\n\nCode\nbw = bw.SJ(x) ## escolha automática\nplot(density(x, bw = bw), main = \"Larguras de banda equivalentes\")\nfor(i in 2:length(kernels)) lines(density(x, bw = bw, adjust = h.f[i], \n                                                           kernel = kernels[i]), col = i)\nlegend(55, 0.035, legend = kernels, col = seq(kernels), lty = 1)\nUm problema prático importante na estimação de densidades via kernel é como escolher a largura de banda \\(h_n\\). Note que dadas condições como \\(h_n\\to 0\\) e \\(nh_n\\to\\infty\\), ainda existem muitas opções para \\(h_n\\). Então, de certo modo, a ordem de convergência ou divergência não resolve o problema. Uma solução para esse problema é conhecida como compensação de viés-variância. Antes de entrarmos nos detalhes, vamos primeiro apressentar um resultado em relação ao viés assintótico do estimador kernel. Aqui, o viés é definido como \\[\n    \\mbox{viés}\\big(\\widehat{f}(x)\\big)=\\mbox{E}\\big(\\widehat{f}(x)\\big)-f(x),\n\\] para um dado \\(x\\).\nDemonstração. Observemos que \\[\n\\begin{array}\n    \\mbox{E}\\big(\\widehat{f}(x)\\big) & = & \\frac{1}{n}\\sum_{i=1}^n \\frac{1}{h_n}\\int K\\Big( \\frac{x-y}{h_n}\\Big)f(y)\\mbox{d}y \\\\\n    & = & \\int K(u)f(x-h_n u)\\mbox{d}u \\, = \\, f(x)+\\int K(u)\\big( f(x-h_n u)-f(x)\\big)\\mbox{d}u\\cdot\n\\end{array}\n\\] Utilizando então o teorema da convergência dominada completa-se a demonstração.&#9609\nDemonstração. A demonstração é baseada na expansão de Taylor, \\[\n    f(x-h_nu) = f(x)-h_nuf'(x)+\\frac{h_n^2u^2}{2}f''(x)-\\frac{h_n^3u^3}{6}f'''(\\epsilon),\n\\] sendo \\(\\epsilon\\) fica entre \\(x-h_nu\\) e \\(x\\). Os detalhes são deixados como um exercício.\nUma medida de precisão do estimador é o erro quadrático médio (EQM), dado por \\[\n    EQM\\big(\\widehat{f}(x)\\big) = \\mbox{E}\\big(\\widehat{f}(x)-f(x)\\big)^2\\cdot\n\\] é fácil mostrar que o \\(EQM\\) combina o viés e a variância de tal maneira que \\[\n    EQM\\big(\\widehat{f}(x)\\big) = \\mbox{viés}\\big(\\widehat{f}(x)\\big)^2 + \\mbox{Var}\\big(\\widehat{f}(x)\\big)\\cdot\n\\] Vemos que, sob as condições \\(h_n\\to 0\\) e \\(nh_n\\to \\infty\\) e se ignorarmos os termos de baixa ordem, temos \\[\n    EQM\\big(\\widehat{f}(x)\\big) \\approx \\frac{h_n^4}{4}\\big(f''(x)\\big)^2\\tau^4 + \\frac{f(x)}{nh_n}\\gamma^2,\n\\] onde \\(\\tau^2=\\int u^2K(u)\\mbox{d}u\\) e \\(\\gamma^2=\\int K^2(u)\\mbox{d}u\\). O termo à direita da expressão acima é minimizada quando \\[\n    h_n = \\left( \\frac{\\gamma^2f(x)}{\\tau^4\\big(f''(x)\\big)^2} \\right)^{\\frac{1}{5}}n^{-\\frac{1}{5}}\\cdot\n\\] Note ainda que a expressão acima não é a solução ideal, isso porque \\(f\\) é desconhecida na prática. No entanto, dá-nos pelo menos alguma ideia sobre a taxa ideal de convergência a zero, sendo esta \\(h_n=O(n^{-\\frac{1}{5}})\\).\nQuando \\(f\\) é desconhecida, uma abordagem natural seria substituí-lo por um estimador e, assim, obter uma largura de banda ideal estimada. Uma complicação é que a largura de banda ideal depende de \\(x\\) mas, idealmente, gostaríamos de usar uma largura de banda que funcionasse para diferentes \\(x\\) dentro de um certo intervalo, se não todos os \\(x\\). Para obter uma largura de banda ideal que não depende de \\(x\\), integramos os dois lados da expressão de \\(EQM\\) em relação a \\(x\\). Isto nos leva a \\[\n    \\int EQM\\big(\\widehat{f}(x)\\big)\\mbox{d}x \\approx \\frac{\\tau^4h_n^4}{4}\\int \\big(f''(x)\\big)^2 \\mbox{d}x +\n    \\frac{\\gamma^2}{nh_n}\\int f(x)\\mbox{d}x \\, = \\, \\frac{\\tau^4\\theta^2h_n^4}{4} + \\frac{\\gamma^2}{nh_n},\n\\] com \\(\\theta^2=\\int \\big(f''(x)\\big)^2 \\mbox{d}x\\). Pelo mesmo argumento, o lado direito acima é minimizado quando\n\\[\n    h_n = \\left( \\frac{\\gamma^2}{\\tau^4\\theta^2} \\right)^{\\frac{1}{5}}n^{-\\frac{1}{5}}\\cdot\n\\] Desta vez, o \\(h_n\\) ideal não depende de \\(x\\). Além disso, a integral do \\(EQM\\) ou o \\(IEQM\\) mínimo é dado por \\[\n    IEQM \\, = \\, \\int EQM\\big(\\widehat{f}(x)\\big)\\mbox{d}x \\, = \\, \\frac{5}{4}\\big( \\tau\\gamma^2 \\big)^\\frac{4}{5}\\theta^{\\frac{2}{5}}\n    n^{-\\frac{4}{5}}\\cdot\n\\]\nUma implicação é a seguinte. Note que o \\(IEQM\\) depende do kernel \\(K\\) através de \\(c_K=\\big( \\tau\\gamma^2 \\big)^\\frac{4}{5}\\). Mostrou-se que para os kernels comumente usados, tais como aqueles listados, o desempenho dos estimadores de kernel correspondentes é quase o mesmo em termos dos valores de \\(c_K\\). Voltando ao problema sobre a estimação da largura de banda ideal, vemos que tudo o que precisamos é encontrar um estimador consistente de \\(\\theta^2\\). Se \\(f\\) é a função de densidade da distribuição normal com desvio padrão \\(\\sigma\\), então pode ser mostrado que \\(\\theta^2=3/8\\sqrt{\\pi}\\sigma^5\\). Naturalmente, se alguém souber que \\(f\\) é normal, então a estimação da densidade não-paramétrica não seria necessária, porque um método paramétrico provavelmente seria melhor. Em geral, pode-se expandir \\(f\\) em torno da densidade gaussiana usando a expansão de Edgeworth.\nUtilizando a abordagem acima, Hjort and Jones (1996) obteveram o seguinte estimador ótimo para a largura de banda \\[\n    \\widehat{h}_n = \\widehat{h}_0\\left(1+\\frac{35}{48}\\widehat{\\gamma}_4+\\frac{35}{32}\\widehat{\\gamma}_3^2+\n        \\frac{385}{1024}\\widehat{\\gamma}_4^2\\right)^{-\\frac{1}{5}},\n\\] onde \\(\\widehat{h}_0\\) é a estimativa ideal da largura de banda assumindo que \\(f\\) é normal, isto é, com \\(\\theta^2\\) substituído por \\(3/8\\sqrt{\\pi}\\sigma^5\\) ou mais explicitamente \\[\n    \\widehat{h}_0=1.06\\left(\\frac{\\widehat{\\sigma}}{n^{\\frac{1}{5}}}\\right),\n\\] chamamos \\(\\widehat{h}_0\\) a largura de banda da linha de base e \\(\\widehat{\\sigma}^2\\) é a variância amostral dada por \\[\n    \\widehat{\\sigma}^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\overline{X})^2\\cdot\n\\] Além disso, \\(\\widehat{\\gamma}_3\\) e \\(\\widehat{\\gamma}_4\\) são os estimadores dos coefcientes de assimetria de amostra e curtose, dado por \\[\n    \\widehat{\\gamma}_3=\\frac{1}{(n-1)\\widehat{\\sigma}^3}\\sum_{i=1}^n (X_i-\\overline{X})^3\n\\] e \\[\n    \\widehat{\\gamma}_4=\\frac{1}{(n-1)\\widehat{\\sigma}^4}\\sum_{i=1}^n (X_i-\\overline{X})^4-3,\n\\] respectivamente. Houve outras abordagens para a seleção da largura de banda ótima, incluindo o método de validação cruzada. Ambos procedimentos foram programados na função density.\nExemplo. Utilaremos os dados simulados da distribuíão \\(N(0,1)\\) no exemplo anterior para com isso mostrarmos o histograma e o estimador Kernel da função de densidade.\nCode\nset.seed(1340)\nx = rnorm(50)\npar(mar=c(4,2,1,1))\nhist(x, main = NULL, freq = FALSE, breaks = \"Sturges\")\nbox()\nlines(density(x, bw = \"nrd0\"), col = \"red\")\n\n\n\n\n\n\n\n\n\nCode\npar(mar=c(4,2,1,1))\nhist(x, main = NULL, freq = FALSE, breaks = \"Scott\")\nbox()\nlines(density(x, bw = \"bcv\"), col = \"red\")\n\n\nWarning in bw.bcv(x): mínimo ocorreu em uma das extremidades do intervalo",
    "crumbs": [
      "Módulo I"
    ]
  },
  {
    "objectID": "ModuloI/index.html#estimação-de-densidades",
    "href": "ModuloI/index.html#estimação-de-densidades",
    "title": "Módulo I",
    "section": "",
    "text": "Definição. Seja \\(f\\) a derivada de \\(F\\); por isso pode-se expressar como \\[\nf(x)=\\lim_{h\\to 0}\\frac{F(x+h)-F(x-h)}{2h}\\cdot\n\\] Então, dizemos que \\(\\widehat{f}\\), definido por \\[\n\\widehat{f}(x)=\\frac{\\widehat{F}(x+h)-\\widehat{F}(x-h)}{2h},\n\\] é o histograma, sendo que \\(\\widehat{F}\\) é a função de distribuição empírica.\n\n\n\n\n\n\n\n\nTeorema\n\n\n\nSeja \\(f\\) a função de densidade da função de distribuição \\(F\\). Então, com probabilidade 1,\n\\[\n    \\widehat{f}(x)\\sim Binomial(n,p),\n\\] com \\(p=F(x+h)-F(x-h)\\). Assim, o comportamento assintótico do histograma pode ser derivado da distribuição binomial como \\[\n    \\mbox{E}\\big(\\widehat{f}(x)\\big)=\\frac{F(x+h)-F(x-h)}{2h}\n\\] e \\[\n    \\mbox{Var}\\big(\\widehat{f}(x)\\big)=\\frac{p(1-p)}{4nh^2}\\cdot\n\\]\n\n\n\n\n\n\n\n\n\nDefinição. O estimador kernel da função de densidade é dado por \\[\n\\widehat{f}(x)=\\frac{1}{nh_n}\\sum_{i=1}^n K\\Big( \\frac{x-X_i}{h_n}\\Big),\n\\] onde \\(K(\\cdot)\\) é uma função conhecida como kernel.\n\n\n\n\n\n\n\n\n\n\n\nTeorema\n\n\n\nSuponhamos que \\(f\\) seja contínua e limitada. Então o viés do estimador kernel de densidade converge a zero quando \\(h_n\\to 0\\), para todo \\(x\\).\n\n\n\n\n\n\n\n\n\nTeorema\n\n\n\nSuponhamos que \\(f\\) seja contínua três vezes diferenciável, com terceira derivada limitada na vizinhança de \\(x\\) e \\(K\\) satisfazendo \\[\n    \\int K^2(u)\\mbox{d}u&lt;\\infty \\qquad \\mbox{e} \\qquad \\int |u|^3K(u)\\mbox{d}u&lt;\\infty\\cdot\n\\]\n\n\nSe \\(h_n\\to 0\\) quando \\(n\\to\\infty\\), temos que \\[\n        \\mbox{viés}\\big(\\widehat{f}(x)\\big) = \\frac{h_n^2}{2}f''(x)\\int u^2K(u)\\mbox{d}u + o(h_n^2)\\cdot\n    \\]\n\n\nSe, além disso \\(nh_n\\to\\infty\\) quando \\(n\\to\\infty\\), então temos \\[\n        \\mbox{Var}\\big(\\widehat{f}(x)\\big) = \\frac{f(x)}{nh_n}\\int K^2(u)\\mbox{d}u + o\\big((nh_n)^{-1}\\big)\\cdot\n    \\]",
    "crumbs": [
      "Módulo I"
    ]
  },
  {
    "objectID": "ModuloI/index.html#exercícios",
    "href": "ModuloI/index.html#exercícios",
    "title": "Módulo I",
    "section": "Exercícios",
    "text": "Exercícios\n\nSeja \\(T(X_1,\\cdots,X_n)\\) uma estatística simétrica nas observaçóes. Mostre que \\(T\\) pode ser escrita como função das estatísticas de ordem. Por outro lado, se \\(T(X_1,\\cdots,X_n)\\) pode ser escrita como função das estatísticas de ordem, \\(T\\) é simétrica nas observações.\nSejam \\(X_1,X_2,\\cdots,X_m\\) e \\(Y_1,Y_2,\\cdots,Y_n\\) amostras independentes de duas distribuições absolutamente contínuas. Encontre o estimador não viciado de mínima variância de:\n\n\n\\(\\mbox{E}(XY)\\)\n\\(\\mbox{Var}(X+Y)\\)\n\n\nSeja \\((X_1,Y_1), (X_2,Y_2), \\cdots, (X_n,Y_n)\\) uma amostra aleatória com distribuição absolutamente contínua bivariada. Encontre o estimador não viciado de mínima variância de:\n\n\n\\(\\mbox{E}(XY)\\)\n\\(\\mbox{Var}(X+Y)\\)\n\n\nConsidere \\((\\mathcal{R},\\mathcal{B},P_\\theta)\\) um espaço de probabilidade e \\(\\mathcal{P}=\\{P_\\theta \\, : \\theta\\in\\Theta\\}\\). Seja \\(A\\) um elemento da \\(\\sigma\\)-álgebra de Borel e considere \\(d(\\theta)=P_\\theta(A)\\).\n\n\nA função \\(d\\) é estimável? Se sim, qual é o grau?\nEncontre o estimador não viciado de mínima variância de \\(d\\), baseado em uma amostra de tamanho \\(n\\) e assumindo que \\(\\mathcal{P}\\) seja a classe de todas as distribuições contínuas.",
    "crumbs": [
      "Módulo I"
    ]
  },
  {
    "objectID": "ModuloI/Aula01/index.html",
    "href": "ModuloI/Aula01/index.html",
    "title": "1  Estatística Não Paramétrica",
    "section": "",
    "text": "1.1 Introdução\nEm todos os problemas de inferência estatística considerados, assumimos que a distribuição da variável aleatória amostrada seja conhecida a menos, talvez, para alguns parâmetros. Na prática, entretanto, a forma funcional da distribuição é raramente ou nunca conhecida. Por conseguinte, é desejável conceber alguns procedimentos que estejam livres desta hipótese relativa à distribuição.\nPara entender a ideia de estatística não-paramétrica, primeiro requeremos uma compreensão de conceitos da estatística básica paramétrica. Conceitos elementares introduzem o conceito de teste de significância estatística com base na distribuição amostral de uma estatística particular. Em resumo, se tivermos um conhecimento básico da distribuição subjacente de uma variável, poderemos fazer previsões sobre como, em amostras repetidas de tamanho igual, essa estatística específica se comportará, isto é, como será distribuída.\nEstudamos aqui alguns procedimentos que são comumente referidos como métodos sem distribuição ou não paramétricos. O termo livre de distribuição refere-se ao fato de que nenhuma suposição é feita sobre a distribuição subjacente, exceto que a função de distribuição sendo amostrada seja absolutamente contínua ou puramente discreta. O termo não paramétrico refere-se ao fato de não haver parâmetros envolvidos no sentido tradicional do termo parâmetro utilizado até o momento.\nGrosseiramente falando, um procedimento não-paramétrico é um procedimento estatístico que possui certas propriedades desejáveis que mantêm suposições relativamente leves em relação às populações subjacentes das quais os dados são obtidos.\nNos dois exemplos seguintes mostramos distribuições conhecidas que são livres de parâmetros.\nCode\n# Definindo os valores para o eixo x1 e x2\nx1 &lt;- seq(-6, 6, length.out = 1000)\nx2 &lt;- seq(0, 10, length.out = 1000)\n\n# Graus de liberdade para a distribuição t-Student e qui-quadrado\ndf_t &lt;- 3\ndf_chi &lt;- 3\n\n# Calculando os valores de densidade das distribuições\ndensidade_t &lt;- dt(x1, df = df_t)\ndensidade_chi &lt;- dchisq(x2, df = df_chi)\n\n# Configurando o layout da plotagem\npar(mfrow = c(1, 2))\n\n# Plotando a distribuição t-Student\nplot(x1, densidade_t, type = \"l\", lwd = 2, col = \"blue\", \n     main = \"Distribuição t-Student (3)\",\n     xlab = \"x\", ylab = \"Densidade\")\n\n# Plotando a distribuição qui-quadrado\nplot(x2, densidade_chi, type = \"l\", lwd = 2, col = \"green\", \n     main = \"Distribuição Qui-Quadrado (3)\",\n     xlab = \"x\", ylab = \"Densidade\")\nO desenvolvimento repetido e contínuo de procedimentos estatísticos não paramétricos nas últimas décadas deve-se às seguintes vantagens de técnicas não paramétricas:\nMas têm desvantagens:",
    "crumbs": [
      "Módulo I",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Estatística Não Paramétrica</span>"
    ]
  },
  {
    "objectID": "ModuloI/Aula01/index.html#introdução",
    "href": "ModuloI/Aula01/index.html#introdução",
    "title": "1  Estatística Não Paramétrica",
    "section": "",
    "text": "Métodos não-paramétricos exigem poucas suposições sobre as populações subjacentes das quais os dados são obtidos. Em particular, os procedimentos não paramétricos abandonam a suposição tradicional de que as populações subjacentes sejam normais.\nOs procedimentos não paramétricos permitem que o usuário obtenha p-valores exatos para testes, probabilidades de cobertura exatas para intervalos de confiança, taxas exatas de erros experimentais para procedimentos de comparação múltipla e probabilidades exatas de cobertura para faixas de confiança sem confiar nas suposições de que as populações subjacentes sejam normais.\nAs técnicas não paramétricas são frequentemente, embora nem sempre, mais fáceis de aplicar do que as suas contrapartes teóricas normais.\nOs procedimentos não paramétricos são geralmente muito fáceis de entender.\nEmbora, à primeira vista, a maioria dos procedimentos não- paramétricos pareça sacrificar muito as informações básicas nas amostras, investigações de eficiência teórica mostraram que esse não é o caso. Normalmente, os procedimentos não-paramétricos são apenas ligeiramente menos eficientes do que os seus concorrentes de teoria normal quando as populações subjacentes são normais e podem ser moderadamente ou muito mais eficientes que os concorrentes quando as populações subjacentes não são normais.\n\n\n\nMétodos não paramétricos são relativamente insensíveis a observações distantes.\nOs procedimentos não paramétricos são aplicáveis em muitas situações em que os procedimentos teóricos normais não podem ser utilizados. Muitos procedimentos não-paramétricos exigem apenas as classificações das observações em vez da magnitude real das observações, enquanto os procedimentos paramétricos exigem as magnitudes.\nNem todos os procedimentos desenvolvidos na estatística paramétrica podem ser aplicados à estatística não-paramétrica.",
    "crumbs": [
      "Módulo I",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Estatística Não Paramétrica</span>"
    ]
  },
  {
    "objectID": "ModuloI/Aula02/index.html",
    "href": "ModuloI/Aula02/index.html",
    "title": "2  Estatísticas de ordem",
    "section": "",
    "text": "2.1 Amostras aleatórias\nConsidere-se um experimento estatístico que culmina em desfechos \\(x\\), que são os valores assumidos por uma variável aleatória \\(X\\). Seja \\(F\\) a função de distribuição de \\(X\\). Na prática, \\(F\\) não será completamente conhecida, isto é, um ou mais parâmetros associados com \\(F\\) serão desconhecidos. O trabalho de um estatístico é estimar esses parâmetros desconhecidos ou testar a validade de certas afirmações sobre eles. Ele pode, por exemplo, obter \\(n\\) observações independentes de \\(X\\). Isso significa que ele observa \\(n\\) valores \\(x1, x2, \\cdots , x_n\\) assumidos da variável aleatória \\(X\\). Cada \\(x_i\\) pode ser considerado como o valor assumido pela variável aleatória \\(X_i, i = 1, 2, \\cdots , n\\) onde \\(X_1, X_2, \\cdots, X_n\\) são variáveis aleatórias independentes com distribuição comum \\(F\\) . Os valores observados \\((x_1, x_2, \\cdots, x_n)\\) são então valores assumidos por \\((X_1, X_2, \\cdots, X_n)\\). O conjunto \\((X_1, X_2, \\cdots , X_n)\\) é, então, uma amostra de tamanho \\(n\\) da distribuição da população \\(F\\) . O conjunto de \\(n\\) valores \\(x_1, x_2, \\cdots , x_n\\) é chamado de uma realização ou estimativa da amostra. Note-se que os possíveis valores do vector aleatório \\((X_1, X_2, \\cdots , X_n)\\) podem ser olhados como pontos em \\(\\mathbb{R}^n\\), os quais podem ser chamados de elementos do espaço amostral. Na prática podemos não observar \\(x_1, x_2, \\cdots, x_n\\) mas alguma função \\(g(x_1, x_2, \\cdots , x_n)\\). Então \\(g(x_1, x_2, \\cdots , x_n)\\) serão considerados os valores assumidos pela variável aleatória \\(g(X)\\).\nVamos agora formalizar esses conceitos.\nSe \\(X_1, X_2, \\cdots , X_n\\) é uma amostra aleatória de \\(F (\\cdot; \\theta)\\), a função de distribuição conjunta é dada por:\n\\[F(x_1, \\cdots, x_n; \\theta) = \\prod_{i=1}^{n} F(x_i; \\theta)\\]\nSegundo esta definição cada uma das variáveis na amostra isoladamente é uma estatística assim como funções destas que, eventualmente, podem não fornecer informações úteis. Duas das estatísticas mais comumente utilizadas são mostradas no exemplo a seguir.\nDeve-se lembrar que as estatísticas amostrais apresentadas neste exemplo \\(\\overline{X}_n, S_{n}^{2}\\) e outras que irão definir-se posteriormente são variáveis aleatórias, com todas as consequências que isso implica, enquanto os parâmetros populacionais \\(\\mu, \\sigma^2\\) e assim por diante são constantes fixas, que podem ser desconhecidas.\nExemplo. Seja \\(X \\sim Bernoulli(p)\\), onde \\(p\\) é desconhecido. A função de distribuição de \\(X\\), mostrada na Figura 2.1, é dada por\n\\[F(x) = p\\delta (x − 1) + (1 − p)\\delta(x),    x \\in \\mathbb{R},\\] onde a função \\(\\delta(\\cdot)\\) foi definida em (1.2) como \\(\\delta(x) = \\begin{cases} 1, \\ x \\ \\geq \\ 0, \\\\ 0, \\ x \\ &lt; 0 \\end{cases}\\), chamada de função delta.\nSuponha que cinco observações independentes de \\(X\\) sejam 0, 1, 1, 1, 0. Então 0, 1, 1, 1, 0 é uma realização da amostra \\(X_1, X_2, \\cdots , X_5\\). A estimativa da média amostral é\n\\[\\overline{x}_5 = \\displaystyle \\frac{0 + 1 + 1 + 1 + 0}{5} = 0, 6\\] o qual é o valor assumido pela variável aleatória \\(\\overline{X}_n\\). A estimativa da variância amostral é \\[S_{5}^{2} = \\sum_{i=1}{5} \\displaystyle \\frac{(x_i - \\overline{x}_5)}{5 - 1}= \\displaystyle \\frac{2 \\ \\times (0,6)^2 \\ + \\ 3 \\ \\times (0,4)^2}{4}= 0,3\\]\nsendo este o valor assumido pela variável aleatória \\(S_{5}^{2}\\).\nExemplo. Seja \\(X \\sim N (\\mu, \\sigma^2)\\), \\(\\mu\\) conhecida, \\(\\sigma^2\\) desconhecido e \\(X_1, \\cdots , X_n\\) uma amostra aleatória dessa distribuição. De acordo com nossa definição, a função \\(\\sum_{i=1}^{n} X_{i} / \\sigma^2\\) não é uma estatística. Suponha que cinco observações de \\(X\\) -0,864; 0,561; 2.355; 0,582 e -0,774. Então, a estimativa da média amostral é 0.372 e a estimativa da variância amostral é 1.648.",
    "crumbs": [
      "Módulo I",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estatísticas de ordem</span>"
    ]
  },
  {
    "objectID": "ModuloI/Aula02/index.html#amostras-aleatórias",
    "href": "ModuloI/Aula02/index.html#amostras-aleatórias",
    "title": "2  Estatísticas de ordem",
    "section": "",
    "text": "Definição. Seja \\(X\\) uma variável aleatória com função de distribuição \\(F\\) e \\(X_1, \\cdots, X_n\\) variáveis aleatórias independentes com distribuição comum \\(F\\). Chamaremos a coleção \\(X_1, \\cdots, X_n\\) de uma amostra aletória de tamanho \\(n\\) de \\(F\\) ou simplesmente como \\(n\\) observações independentes de \\(X\\).\n\n\n\n\nDefinição. Sejam \\(X_1, X_2, \\cdots, X_n\\) \\(n\\) observações independentes da variável aleatória \\(X\\) e seja \\(g: \\mathbb{R}^n \\to \\mathbb{R}\\) uma função real derivável. Então a variável aleatória \\(g(X_1, X_2, \\cdots, X_n)\\) é chamada de estatística, desde que não dependa de parâmetros desconhecidos.\n\n\n\nDefinição. Seja \\(X_1, X_2, \\cdots, X_n\\) uma amostra aleatória da função de distribuição \\(F\\). A estatística \\[\\overline{X}_n = \\sum_{i=1}^{n} \\displaystyle \\frac{Xi}{n},\\] é chamada de média amostral e a estatística \\[S_{n}^{2} = \\sum_{i=1}^{n} \\displaystyle \\frac{(X_i - \\overline{X}_n)^2}{n-1}\\] é chama de variância amostral.\n\n\n\n\n\n\n\n\n\n\nFigura 2.1: Representação da função de distribuição Bernoulli para três valores do parâmetro \\(p\\) = 0.3, 0.5 e 0.8. Observe que nesta curva a reta no intervalo (0, 1) depende de 1 - \\(p\\), isso porque a função \\(\\delta\\) é sempre zero para \\(x\\) - 1 nesse intervalo.",
    "crumbs": [
      "Módulo I",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estatísticas de ordem</span>"
    ]
  },
  {
    "objectID": "ModuloI/Aula02/index.html#estatísticas-de-ordem",
    "href": "ModuloI/Aula02/index.html#estatísticas-de-ordem",
    "title": "2  Estatísticas de ordem",
    "section": "2.2 Estatísticas de ordem",
    "text": "2.2 Estatísticas de ordem\nSeja (\\(X_1, X_2, \\cdots , X_n\\)) um vetor aleatório n-dimensional e (\\(x_1, x_2, \\cdots , x_n\\)) uma \\(n\\)-tupla assumida por (\\(X_1, X_2, \\cdots , X_n\\)). Vamos organizar \\(x_1, x_2, \\cdots , x_n\\) em ordem crescente de magnitude, para que \\[x_{(1)}\\le x_{(2)} \\le \\cdots ≤ x_{(n)},\\] onde \\(x_{(1)}\\) = min(\\(x_1, x_2, \\cdots , x_n\\)), \\(x_{(2)}\\) é o segundo menor valor em \\(x_1, \\cdots , x_n\\) e assim por diante, \\(x_{(n)}\\) = max(\\(x_1, \\cdots , x_n\\)). Se quaisquer dois \\(x_i\\), \\(x_j\\) forem iguais, a ordem não importa.\n\nDefinição. A função \\(X_{(k)}\\) de (\\(X_1, \\cdots ,X_n\\)) que assume o valor \\(x_{(k)}\\) em cada possível sequência (\\(x_1, x_2, \\cdots , x_n\\)) de valores assumidos por (\\(X_1,X_2, \\cdots , X_n\\)) é conhecida como a \\(k\\)-ésima estatística de ordem ou a estatística de ordem \\(k\\). O conjunto (\\(X_{(1)},X_{(2)}, \\cdots , X_{(n)}\\)) é chamado de estatísticas de ordem para (\\(X_1, X_2, \\cdots , X_n\\)).\n\nExemplo. Consideremos \\(X_1\\), \\(X_2\\), \\(X_3\\) três variáveis aleatórias discretas de maneira que, \\(X_1\\) e \\(X_3\\) sejam tais que assumam somente valores 0, 1 e que \\(X_2\\) assuma valores 1, 2, 3. O vetor aleatório (\\(X_1\\), \\(X_2\\), \\(X_3\\)) assume os valores: (0, 1, 0), (0, 2, 0), (0, 3, 0), (0, 1, 1), (0, 2, 1), (0, 3, 1), (1, 1, 0), (1, 2, 0), (1, 3, 0), (1, 1, 1), (1, 2, 1) e (1, 3, 1). Então \\(X_{(1)}\\) assume somente valores 0 ou 1; \\(X_{(2)}\\) assume somente valores 0 ou 1 e \\(X_{(3)}\\) assume somente valores 1, 2 ou 3.\n\n\n\n\n\n\nTeorema\n\n\n\nSeja (\\(X_{(1)}, X_{(2)}, \\cdots , X_{(n)}\\)) um vetor aleatório de dimensão \\(n\\) e seja \\(X_{(k)}, 1 \\le k \\le n\\), a \\(k\\)-ésima estatística de ordem. Então \\(X_{(k)}\\) é também uma variável aleatória.\n\n\nExercício. Na apresentação dos resultados a seguir assumiremos que \\(X_1, X_2, \\cdots , X_n\\) são variáveis aleatórias independentes e igualmente distribuídas contínuas com função de densidade \\(f\\) . Seja \\(\\{ X_{(1)}, X_{(2)}, \\cdots , X_{(n)} \\}\\) o conjunto das estatística de ordem para \\(X_1, X_2, \\cdots , X_n\\). Dado que todas as \\(X_i\\) são contínuas segue que, com probabilidade 1 \\[X_{(1)} \\le X_{(2)} \\le \\cdots \\le X_{(n)}·\\]\n\n2.2.1 2.2.1 Propriedades das estatísticas de ordem\nComeçaremos o estudo das propriedades encontrando a função de densidade conjunta de (X(1), X(2), , X(n)).\nDemonstração : A transformação de (X1, , Xn) a (X(1), , X(n)) não é biunívoca. De fato, existem um total de n! possíveis arranjos de x1, , xn em ordem crescente de magnitude. Assim, existem n! inversas para a transformação. Por exemplo, uma das n! permutações pode ser x4 &lt; x1 &lt; xn−1 &lt; x3 &lt; &lt; xn &lt; x2·\nA inversa correspondente é x4 = x(1), x1 = x(2), xn−1 = x(3), x3 = x(4) xn = x(n−1), x2 = x(n)· O determinante Jacobiano desta transformação é a matriz n×n identidade com as colunas reorganizadas, isto devido a que cada x(i) é igual a uma, e somente uma, das \\(X_1, X_2, \\cdots , X_n\\). Portanto J = ±1 e\nn f (x(2), x(n), x(4), x(1), , x(3), x(n−1))|J| = f (x(i)), i=1 quando x(1) &lt; x(2) &lt; &lt; x(n). A mesma expressão é válida para cada um dos n! arranjos. Segue então que\nf (x(1), , x(n)) = Todas as n! permutações\nn f (x(i)) i=1\n=  \nn! i=1\nf (x(i)), caso x(1) &lt; x(2) &lt; &lt; x(n) · 0, caso contrário\nExemplo 2.4 Sejam X1, , Xn variáveis aleatórias independentes com função de densidade comum f (x) = 1, se 0 &lt; x &lt; 1 · 0, caso contrário Então a função de densidade conjunta de X(1), X(2), , X(n) é\nf (x\n\n\n\n, , x\n\n\n\n) = n!, 0 &lt; x(1) &lt; x(2) &lt; &lt; x(n) 0, caso contrário\n· (2.5)\nEstamos confiados que como resultado do Teorema 2.2 temos funções de densidade. Vejamos neste exemplo se isso é realmente acontece. Consideremos, para simplificar, o caso n = 3 e verifiquemos se a integral da função de densidade em (2.5) é 1. Então\n∫∫∫\nf (x(1), x(2), x(3)) dx(1) dx(2) dx(3) = 6\n∫ 1 [∫ 1\n(∫ 1\ndx(3))\ndx(2)]\ndx(1)\n0 1 = 6 0\nx(1) [ x(1)\nx(2) (1 − x(2))\ndx(2)]\ndx(1)\n1 1 = 6 − x(1)\nx2 + (1) 2\ndx(1)\n= 1·\n0 Um detalhe interessante é que esta e outras propriedades demonstradas aqui somente são válidas quando as variáveis aleatórias são contínuas. Isso não significa que estatísticas de ordem não possam ser definidas no caso discreto. O que estamos dizendo é que estas propriedades somente podem ser demonstradas no caso contínuo.\nExemplo 2.5 Consideremos a situação em que temos somente três variáveis aleatórias independentes X1, X2 e X3 com\ndistribuição geométrica de parâmetro p, isto é, P (X = x; p) = (1 − p)px, x = 0, 1, 2, Encontremos P (X(1) &lt; X(2) &lt; X(3)). Nesta situação a probabilidade requerida pode ser escrita como: P (X(1) &lt; X(2) &lt; X(3)) = 1 − P (X1 = X2 ̸= X3) − P (X1 = X3 ̸= X2) −P (X2 = X3 ̸= X1) − P (X1 = X2 = X3) a qual pode ser escrita como P (X(1) &lt; X(2) &lt; X(3)) = 1 − 3P (X1 = X2 ̸= X3) − P (X1 = X2 = X3) = 1 − 3 [P (X1 = X2) − P (X1 = X2 = X3)] − P (X1 = X2 = X3) = 1 − 3P (X1 = X2) + 2P (X1 = X2 = X3)·\nNão é difícil perceber que\ne que\nP (X1 = X2) =\n(1 p)2 1 − p2 , (1 − p)3\ndo qual obtemos que\nP (X1 = X2 = X3) =\n1 − p3 ,\n6p3\nP (X(1) &lt; X(2) &lt; X(3)) = (1 − p)(1 + p + p2)· As propriedades das estatísticas de ordem que serão demonstradas valerão somente caso as variáveis sejam contínuas. Isto deve-se a que, caso as variáveis sejam discretas, a probabilidade\nP (X(1) = X(2) = = X(n)) ̸= 0,\ncomo vai ser mostrado no seguinte exemplo. Acontece que o fato da probabilidade das estatística de ordem poderem coincidir, com probabilidade diferente de zero, altera a estrutura da demonstração e não nos permite obtermos estes resultados para o caso discreto.\nExemplo 2.6 Sejam \\(X_1, X_2, \\cdots , X_n\\) variáveis aleatórias independentes assumindo somente 0 e 1 com probabilidade 1/2. Observemos que\nn n P (X(1) = X(2) = = X(n)) = ∏ P (X(k) = 0) + ∏ P (X(k) = 1)\nk=1 n = P (Xk k=1\nk=1 n = 0) + P (Xk k=1\n1 = 1) = 2n−1 ·\nEstudemos agora o comportamento marginal, ou seja, nos interessa agora encontrar a função de distribuição marginal de cada estatística de ordem.\nDemonstração : Partimos da expressão da função de densidade conjunta das estatísticas de ordem obtida no Teorema 2.2. Então,\nfr(x(r)) = n!f (x(r))\n∫ x(r) ∫ x(r−1)\n∫ x(2) ∫ +∞ ∫ +∞\n∫ +∞ ∏\nf (ti) dtn dtr+1 dt1 dtr−1\n−∞ −∞\n−∞ x(r)\nx(r+1)\nx(n−1) i̸=r\n= n!f (x(r))\n[1 − \\(F\\) (x(r))]n−r x(r)\n(n − r)!\n∫ x(2) r∏−1\nf (ti) dti\n[1 − \\(F\\) (x(r))]n−r [F (x(r))]r−1\n= n!f (x(r))\n(n − r)!\n· (r − 1)!\nComo utilidade deste teorema podemos mencionar o fato de agora podermos encontrar os momentos das es- tatística de ordem. Faremos isso como consequência do seguinte exemplo.\nExemplo 2.7 Sejam \\(X_1, X_2, \\cdots , X_n\\) variáveis aleatórias independentes U (0, 1). Então\nfr(x(r)) =  \nn!\n(r − 1)!(n − r)!\nxr−1(1 − x(r))n−r, se 0 &lt; x(r) &lt; 1 (1 ≤ r ≤ n) 0, caso contrário\nObservemos que, na situação do exemplo acima, X(r) ∼ Beta(r, n − r + 1), logo, valem os resultados da distribuição Beta e, por exemplo, E(X(r)) = r/(n + 1)· Para uma densidade qualquer e somente quatro variáveis aleatórias a forma da densidade marginal, de uma qualquer estatística de ordem, é mostrada no seguinte exemplo.\nExemplo 2.8 Sejam X1, X2, X3, X4 variáveis aleatórias independentes com densidade comum f. A função de densidade conjunta das estatísticas de ordem X(1), X(2), X(3), X(4) é\nf (x\n\n\n\n, x(2)\n, x(3)\n, x(4)\n) = 4!f (x(1))f (x(2))f (x(3))f (x(4)), se x(1) &lt; x(2) &lt; x(3) &lt; x(4) · 0, caso contrário\nVamos calcular a função de densidade marginal de X(2). Temos que, se x(1) &lt; x(2) &lt; x(3) &lt; x(4) f2(x(2)) = 4! ∫ ∫ ∫ ∫ \\(F\\) (t1)f (x(2))f (t3)f (t4) dt1 dt3 dt4\n= 4!f (x(2))\n∫ x(2) ∫ +∞ [∫ +∞\nf (t4) dt4]\nf (t3)f (t1) dt3 dt1\n−∞ x(2) t3\n= 4!f (x(2))\n∫ x(2) {∫ +∞\n[1 − \\(F\\) (t3)]f (t3) dt3}\nf (t1) dt1\n−∞ x(2)\n= 4!f (x(2))\nx(2) [1 F (x )]2 2 f (t1) dt1 = 4!f (x(2))\n[1 F (x )]2 2! F (x(2))·\nEvidentemente, a expressão acima coincide com o resultado apresentado no Teorema 2.3.\nDemonstração :\nfrs(x(r), x(s)) =\n∫ x(r)\n∫ t2\n∫ x(s)\n∫ x(s) ∫ +∞\n∫ +∞\n−∞ −∞\nx(r)\nts−2\nx(s)\ntn−1\nn!f (t1) \\(F\\) (tn) dtn dts+1 dts−1 dtr+1 dt1 dtr−1\n= n!\n∫ x(r)\n∫ t2\n∫ x(s)\n∫ x(s) [1 − \\(F\\) (x(s))]n−s\n−∞ −∞\nx(r)\nts−2\n(n − s)!\n×f (t1)f (t2) \\(F\\) (x(s)) dts−1 dtr+1 dt1 dtr−1\n= n!\n[1 − \\(F\\) (x(s))]n−s (n − s)! f (x(s))\nx(r)\n−∞\nt2 f (t1) \\(F\\) (x(r))× −∞\n[F (x(s)) − \\(F\\) (x(r))]s−r−1 × (s − r − 1)! dt1 dtr−1\nn!  \n= (n − s)!(s − r − 1)![1 − \\(F\\) (x\n\n\n\n)]n−s×\n[F (x(r))]r−1\ncaso x(r) &lt; x(s).\n×[F (x(s)) − \\(F\\) (x(r))]s−r−1f (x(s))f (x(r))\n, (r − 1)!\nDe modo semelhante, podemos mostrar que a função de densidade conjunta de X(k1), , X(km) se 1 ≤ k1 &lt;\nk2 &lt; &lt; km ≤ n, 1 ≤ m ≤ n, é dada por fk1k2···km (x(k1), x(k2), , x(km)) =\n(k1\n— 1)!(k2\n— k1\nn! × — 1)! × (n − km)!\n×Fk1−1(x(k ))f (x(k ))[F (x(k )) − \\(F\\) (x(k ))]k2−k1−1f (x(k )) × × ×[F (x(k )) − \\(F\\) (x(k ))]km−1−km−2−1f (x(k ))[1 − \\(F\\) (x(k ))]n−km \\(F\\) (x(k )), caso x(k1) &lt; x(k2) &lt; &lt; x(km) e zero noutras situações.\nExemplo 2.9 (Continuação do Exemplo 2.7) Sabemos que as variáveis aleatórias \\(X_1, X_2, \\cdots , X_n\\) são independentes e tem como função de densidade comum f (x) = 1, se 0 &lt; x &lt; 1 · 0, caso contrário Então, a função de densidade conjunta de X(r) e X(s) é dada por\nfrs\n(x(r)\n, x(s)) =\n n!\nxr−1(x(r) − x(s))s−r−1(1 − x(s))n−s (r − 1)!(s − r − 1)!(n − s)!\n, se x\n\n\n\n&lt; x(s) ·\n\nonde 1 ≤ r &lt; s ≤ n.\n0, caso contrário\nUma situação mais complexa é trabalharmos com funções de estatísticas de ordem. Não temos um resultado simples para o caso de qualquer funções destas estatísticas. Mas, no exemplo a seguir, podemos encontrar um resultado interessante para o comportamento da diferença de estatísticas de ordem.\nExemplo 2.10 Sejam X(1), X(2), X(3) as estatísticas de ordem das variáveis aleatórias independentes e igualmente distribuídas X1, X2, X3 com função de densidade comum { βe−xβ, se x ≥ 0 sendo β &gt; 0. Sejam Y1 = X(3) − X(2) e Y2 = X(2). Mostraremos que Y1 e Y2 são independentes. Para isso primeiro observemos que a função de densidade conjunta de X(2) e X(3) é dada por\nf23(x, y) =\n1!0!0!\n· 0, caso contrário\nA função de densidade conjunta de (Y1, Y2) é então f (y1, y2) = 3!β2(1 − e−y2β )e−y2βe−(y1+y2)β = [3!βe−2y2β (1 e−y2β )][βe−y1β ], se 0 &lt; y &lt; + , 0 &lt; y &lt; + = · 0, caso contrário Do qual segue que Y1 e Y2 são independentes. Duas estatísticas de ordem importantes são o máximo e mínimo. Nesses casos é possível encontrar, de maneira\nanalítica, expressões para a função de distribuição. Vejamos no teorema a seguir as expressões da função de distribuição das estatísticas de ordem X(1) e X(n).\nDemonstração : Exercício.\nAcerca da função de distribuição de qualquer estatística de ordem temos o resultado a seguir.\nDemonstração : O evento {X(k) ≤ x} ocorre se, e somente se, pelo menos k dos \\(X_1, X_2, \\cdots , X_n\\) são menores ou iguais a x, por isso o somatório começa em k.\nNos dois teoremas seguintes relacionamos a distribuição condicional de estatísticas de ordem, condicionadas em outra estatística de ordem, com a distribuição de estatísticas de ordem de uma população cuja distribuição é uma forma truncada da função de distribuição da população original \\(F\\) .\nDemonstração : A densidade condicional de X(j) dado que X(i) = xi calcula-se dividindo a densidade conjunta de X(i) e X(j), dada em (2.7), pela densidade marginal de X(i), esta obtida no Teorema 2.4. Temos então que, quando\ni &lt; j ≤ n e xi ≤ xj &lt; ∞,\nf (xj|X(i)\n= x ) = fij(xi, xj) i fi(xi) (n − i)! [ \\(F\\) (xj) − \\(F\\) (xi)]j−i−1\n[ 1 − \\(F\\) (xj)]n−j \\(F\\) (xj)\n(j − i − 1)!(n − j)! 1 − \\(F\\) (xi)\n1 − \\(F\\) (xi) 1 − \\(F\\) (xi)\nO resultado segue observando que \\(F\\) (xj ) − \\(F\\) (xi) e \\(F\\) (xj ) são, respectivamente, as funções de distribuição e de 1 − \\(F\\) (xi) 1 − \\(F\\) (xi) densidade truncando à esquerda em xi a distribuição \\(F\\) .\nNa demonstração do teorema anterior utiliza-se o conceito de distribuição truncada, o que é isso? define-se a seguir este conceito e incluem-se exemplos explicativos.\nCaso a variável aleatória X seja discreta com função de probabilidade P , a distribuição truncada de X é dada por\nP (X = x|X ∈ A) =\nP (X = x, X ∈ A) P (X ∈ A)\n=  \nP (X = x) P (X = a), se x ∈ A a∈A 0, se x ∈/ A\nNa situação X do tipo contínua, com função de densidade \\(F\\) , temos que\nP (X ≤ x|X ∈ A) =\nP (X ≤ x, X ∈ A) = P (X ∈ A)\n∫(−∞,x]∩A\nf (y) dy\n· (2.8)\nConcluindo então que, a função de densidade da distribuição truncada é dada por\nh(x) =\n ∫\nf (x) f (y) dy\n, caso x ∈ A,\n· (2.9)\n 0 A\ncaso x ∈/ A\nExemplo 2.11 Suponhamos X uma variável aleatória com distribuição normal padrão e A = ( , 0]. Então, P (X A) = 1/2, dado que X é simétrica e contínua. Para a densidade truncada temos que { 2f (x), caso − ∞ &lt; x ≤ 0,\nO truncamento é especialmente importante nos casos em que a distribuição \\(F\\) em questão não tem média finita. Se X é uma variável aleatória, truncamos X em algum c &gt; 0, onde c é finito, substituindo X por Xc = X caso |X| c e zero caso |X| &gt; c. Então Xc é X truncada em c e todos os momentos de Xc existem e são finitos. Na verdade, sempre podemos selecionar c suficientemente grande para que P (X ̸= Xc) = P (|X| &gt; c),\nseja arbitrariamente pequena. A distribuição de Xc é então dada por\nP (Xc ≤ x) = P (X ≤ x| |X| ≤ c) = no caso contínuo com função de densidade \\(F\\) e é dada por\nf (y) dy (−∞,x]∩[−c,+c] , P (|X| ≤ c)\nP (Xc = x) =\n \nP (X = x)\nP (X = a) a∈[−c,+c]\n, se x ∈ [−c, +c] ,\n0, se x ∈/ [−c, +c] no caso discreto. Observemos que, para algum α &gt; 0, E(|Xc|)α ≤ cα·\nExemplo 2.12 Caso X Cauchy(0, 1), sabemos que E(X) não existe. Seja c &gt; 0 um número finito, truncando X em c definimos\nEntão\nXc =\nX, caso |X| c, · 0, caso |X| &gt; c\n1 ∫ +c 1\n2 −1\nSendo que a função de densidade truncada é dada por\n 1 1\n1 , caso x ∈ [−c, +c],\nh(x) =\nDesta expressão obtemos que\n2 1 + x2 tan−1(c) ·  0, caso x ∈/ [−c, +c]\n1    ∫ +c   x   \ne também que\nE(Xc) =\n2 tan−1(c)\n−c 1 + x2\ndx = 0,\nE(Xc)2 =\n1    ∫ +c\nx2 dx =\nc   \n— 1·\n2 tan−1(c)\n−c 1 + x2\ntan−1(c)\nPor último, temos o seguinte resultado estabelecendo novamente relação entre estatísticas de ordem e distri- buições truncadas.\nDemonstração : A densidade condicional de X(i) dado que X(j) = xj calcula-se dividindo a densidade conjunta de X(i) e X(j), dada em (2.7), pela densidade marginal de X(j), esta obtida no Teorema 2.4. Temos então que, quando i &lt; j ≤ n e xi ≤ xj &lt; ∞, (j − i)! [ \\(F\\) (xi) ]i−1 [ \\(F\\) (xj) − \\(F\\) (xi)]j−i−1 \\(F\\) (xi) O resultado segue observando que \\(F\\) (xi)/F (xj) e \\(F\\) (xi)/F (xj) são, respectivamente, as funções de distribuição e de densidade truncando à direita em xj a distribuição \\(F\\) .\n\n\n2.2.2 2.2.2 Quantis\nLembremos que a função de distribuição \\(F\\) é contínua à direita e que o número de descontinuidades é, no máximo, enumerável. Estas são propriedades importantes que farão toda diferença na definição dos quantis amostrais, por isso, demonstraremos as propriedades mencionadas da função de distribuição. A prova de que \\(F\\) é contínua à direita advém do seguinte fato F (x + hn) − \\(F\\) (x) = P (x &lt; X ≤ x + hn), onde {hn} é uma sequência de números reais estritamente positivos tais que limn→∞ hn = 0. Segue, da propriedade de continuidade da função de probabilidade,1 que lim [F (x + hn) F (x)] = 0, n→∞ e, portanto, \\(F\\) é contínua à direita. Definamos por D o conjunto dos pontos de descontinuidade de \\(F\\) e seja D = {x ∈ D : P (X = x) ≥ 1 } , onde n é um inteiro positivo. Dado que \\(F\\) ( ) F ( ) = 1, o número de elementos em Dn não pode exceder n. Logicamente ∞ D = Dn n=1 e, então, o conjunto D é enumerável. Demonstrando-se assim a segunda propriedade importante mencionada da função de distribuição. Definimos a seguir o conceito de quantil teórico e depois mostramos a forma de cálculo.\n1A função de distribuição é contínua, devido a que P lim n→∞\nAn = lim n→∞\nP (An),\nse o limite limn→∞ An existir.\nA função \\(F\\) −1(t), 0 &lt; t &lt; 1 foi definida em (1.29) e é chamada de função inversa de \\(F\\) . O seguinte teorema fornece-nos propriedades úteis. Fica claro que as propriedades apresentadas no seguinte teorema nos permitirão o cálculo dos quantis e é por isso que dedicamos atenção a este conceito.\nDemonstração : Exercício.\nExemplo 2.13\nSeja X Exponencial(). Sabemos que a função de distribuição neste caso é \\(F\\) (x) = 1 e−x/. Resulta que a expressão de qualquer um dos quantis é possível de ser encontrada de maneira exata via\nobtendo-se que\nF (ξp) = p 1 − e−ξp/= p 1 − p = e−ξp/,\nξp = −ln(1 − p)\né a expressão teórica do p-ésimo quantil. Devemos mencionar que a expressão dos quantis está bem definida, no sentido de que o resultado é sempre positivo. Isto é importante porque devemos lembrar que a distribuição exponencial está definida somente para valores positivos, então o quantil teórico deve ser positivo, já que é um dos possíveis valores da variável. Observemos que caso \\(F\\) seja contínua e estritamente crescente, \\(F\\) −1 é definida como F −1(y) = x quando y = \\(F\\) (x)· Ainda podemos observar que, se x0 é um ponto de descontinuidade de \\(F\\) e supondo que F (x−) &lt; y &lt; \\(F\\) (x0) = \\(F\\) (x+) 0 0\nvemos que, embora não exista x tal que y = \\(F\\) (x), \\(F\\) −1(y) é definido como igual a x0. A situação na qual \\(F\\) não é estritamente crescente, por exemplo, caso da variável aleatória ser discreta, podemos escrever\nF (x) =\n= y, caso a ≤ x ≤ b ·  &gt; y, caso x &gt; b\nEntão, qualquer valor a x b poderia ser escolhido como x = \\(F\\) −1(y). A convenção é que, neste caso, definimos F −1(y) = a. Em particular ξ1/2 = \\(F\\) −1(1/2), (2.10) é chamada de mediana de \\(F\\) . Observemos que ξp satisfaz a desigualdade F (ξp− ) ≤ p ≤ \\(F\\) (ξp)· Exemplo 2.14 (Continuação do Exemplo 2.13) Caso p = 1/2, a mediana amostral será ξ1/2 = −ln(1/2) = 0.6931472.",
    "crumbs": [
      "Módulo I",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estatísticas de ordem</span>"
    ]
  },
  {
    "objectID": "ModuloI/Aula02/index.html#momentos-amostrais",
    "href": "ModuloI/Aula02/index.html#momentos-amostrais",
    "title": "2  Estatísticas de ordem",
    "section": "2.3 Momentos amostrais",
    "text": "2.3 Momentos amostrais\nNesta seção vamos estudar algumas estatísticas amostrais comumente utilizadas e suas distribuições.\nObservemos que nFn(x) é o número de Xk (1 ≤ k ≤ n) menores ou iguais a x. Se X(1), X(2), , X(n) são as estatísticas de ordem de \\(X_1, X_2, \\cdots , X_n\\) então claramente\nFn(x) =  \nk , se X n\n\n\n\n≤ x &lt; X\n(k+1)\n, (k = 1, 2, , n − 1)\n· (2.11)\n 1, se x ≥ X(n)\ncom esperança e variância\nVar[Fb\nE[Fn(x)] = \\(F\\) (x) (2.13)\nF (x)[1 − \\(F\\) (x)]\nDemonstração : Dado que δ(x − Xi), i = 1, 2, , n são variáveis aleatórias independentes igualmente distribuídas cada uma com função de probabilidade P [δ(x − Xi) = 1] = P (x − Xi ≥ 0) = \\(F\\) (x) e P [δ(x − Xi) = 0] = 1 − \\(F\\) (x), sua soma nF ∗(x) é uma variável aleatória com distribuição Binomial(n, p), onde p = \\(F\\) (x). As relações (2.12), (2.13) e (2.14) seguem-se imediatamente.\nDemonstração :\nE´ uma consequência da Lei dos Grandes Números .\nCorolário 2.12\nonde Z ∼ N (0, 1).\n√n[F (x) F (x)] √F (x)[1 − \\(F\\) (x)] −→ Z quando n → ∞,\nDemonstração :\nE´ consequência do Teorema do Limite Central.\nExemplo 2.15\nVamos apresentar o conceito de função distribuição empírica no caso de termos uma amostra aleatória da distribuição N (0, 1). A lista de comandos na linguagem de programação R está disponível abaixo. O primeiro comando destina-se a fixar o gerador de amostras e, assim, em qualquer momento podemos obter a mesma amostra aleatória. Na Figura 2.2 mostramos a forma da distribuição empírica, de três formas diferentes, para uma amostra de tamanho 12. A representação da função de distribuição empírica é realizada permitindo escolher qual utilizar segundo o agrado.\nlwd = 2\n−1.5 −1.0 −0.5 0.0 0.5\nx\n−1.5 −1.0 −0.5 0.0 0.5\nx\n−1.5 −1.0 −0.5 0.0 0.5\nx\nFigura 2.2: Representação da função de distribuição amostral ou empírica, de três formas diferentes, para uma amostra normal padrão de tamanho 12.\nA linhas de comando a seguir permitiram-nos gerar os gráficos na Figura 2.2: construímos : set.seed(5739); x=rnorm(12); Fn=ecdf(x) par(mar=c(5,4,3,1), cex=0.9) plot(Fn, main=““) plot(Fn, verticals = TRUE, do.points = FALSE, main=”“) plot(Fn , lwd = 2, main=”“); mtext(”lwd = 2”, adj = 1) xx=unique(sort(c(seq(-3, 2, length = 201), knots(Fn12)))) lines(xx, Fn(xx), col = “blue”) abline(v = knots(Fn), lty = 2, col = “gray70”) Observemos que a convergência da distribuição empírica, segundo o Teorema 2.10, é para cada valor de x. E´ possível fazer uma demonstração da convergência em probabilidade simultaneamente para todos os x, ou seja, da convergência uniforme.\nDemonstração : Seja ϵ &gt; 0. Escolhemos um inteiro k &gt; 1/ϵ e números −∞ = x0 &lt; x1 ≤ x2 ≤ ≤ xk−1 &lt; xk = ∞, tais que \\(F\\) (x−) ≤ j/k ≤ \\(F\\) (xj), para j = 1, , k − 1. Observe que se xj−1 &lt; xj, então\nPela Lei dos Grandes Números\nF (x−) − \\(F\\) (xj−1) ≤ ϵ·\nq.c. Fn(xj) −→ \\(F\\) (xj) e — q.c. −\npara j = 1, , k − 1. Consequentemente,\nFbn(xj ) −→ \\(F\\) (xj ),\n∗ − q.c. ∆n = max{|Fbn(xj) − \\(F\\) (xj)|, |Fn (xj ) − \\(F\\) (xj)|, j = 1, , k − 1} −→ 0·\nSeja x arbitrário e encontremos j tal que xj−1 &lt; x ≤ xj. Então, Fbn(x) − \\(F\\) (x) ≤ Fbn(x−) − \\(F\\) (xj−1) ≤ Fbn(x−) − \\(F\\) (x−) + ϵ,\ne\nIsto implica que\nFbn(x) − \\(F\\) (x) ≥ Fbn(xj−1) − \\(F\\) (x−) ≥ Fbn(xj−1) − \\(F\\) (xj−1) − ϵ·\nq.c. sup |Fn(x) F (x)| ∆n + ϵ ϵ· x Como isso vale para todo ϵ &gt; 0, o teorema segue.\nAgora, dado que \\(F\\) ∗(x) tem pontos de salto em Xi, i = 1, 2, , n é claro que existem todos os momentos de \\(F\\) ∗(x). Vamos considerar alguns valores típicos da função de distribuição \\(F\\) , chamados de estatísticas amostrais. Escrevamos a = 1 ∑ Xk, (2.15)\npara os momentos de ordem k ao redor do 0 (zero). Aqui ak, serão chamados de momentos amostrais de ordem k. Com esta notação\nO momento amostral central é definido por\nn a1 = Xi n i=1\n= X·\nb = 1 ∑(X − a )k = 1 ∑(X\n— X) · (2.16)\nLogicamente,\nk n i 1 i=1\nn i i=1\nb = 0 e b\n= (n − 1 ) S2·\nComo mencionado anteriormente, não chamamos b2 a variaˆncia amostral. S2\nserá chamada como a variaˆncia\namostral por razões que se tornarão claras posteriormente. Temos que b2 = a2 − a2· Para a função geradora de momentos de Fn podemos afirmar que n\nMFbn\n\n= 1 etXi · n\n\ni=1\nDefinições similares são realizadas para momentos amostrais de distribuições multivariadas. Por exemplo, se (X1, Y2), (X2, Y2), , (Xn, Yn) é uma amostra de uma distribuição bivariada, podemos escrever n n X = 1 ∑ X , Y = 1 ∑ Y\npara as duas médias amostrais e para os momentos de segunda ordem centrais escrevemos\nb20\nn = (Xi n i=1\n— X) , b02\nn = (Yi n i=1\n— Y ) ,\nMais uma vez, escrevemos\nb11\nn\nn = (Xi n i=1\n— X)(Yi\n— Y )·\nn\nS2 = 1 ∑(X\n— X)2, S2 = 1 ∑(Y\n— Y ) , (2.17)\npara as duas variaˆncias amostrais e para a covariância amostral utilizamos n\nS11\n= 1 (X n − 1 i=1\n— X)(Yi\n— Y )· (2.18)\nEm particular, o coeficiente de correlação amostral é definido por b11 S11\nR = 20\nb02\n= · S1S2\nPode ser demonstrado que |R| ≤ 1 e que os valores extremos ±1 ocorrem somente quando todos os pontos amostrais (X1, Y2), (X2, Y2), , (Xn, Yn) estão alinhados. Correspondendo a uma amostra \\(X_1, X_2, \\cdots , X_n\\) de observações em \\(F\\) , p-ésimo quantil amostral é definido como o p-ésimo quantil da função de distribuição amostral, ou seja, como \\(F\\) −1. Os quatis amostrais são definidos de maneira similar. Então, se 0 &lt; p &lt; 1, o quantil amostral de ordem p,\nr = [np] se n é um número par [np] + 1 se n é um número ímpar\n· (2.19)\nComo usual, [x] denota o maior inteiro x. Observe que, se [n] for par, podemos escolher qualquer valor entre X([np]) e X([np]+1) como o p-ésimo quantil amostral. Então, se p = 1 e n par podemos escolher qualquer valor entre X(n/2) e X(n/2+1), os dois valores do meio, como a mediana amostral. Habitualmente é escolhido o ponto médio. Assim, a mediana amostral é definida como\nξb1/2 = \nX((n+1)/2) se n é ímpar X(n/2) + X((n/2)+1) se n é par\n· (2.20)\nObserve que\n2 [n + 1] = (n + 1 )\n2 2 se n é ímpar. Consideraremos agora os momentos de características amostrais. Nos seguintes desenvolvimentos denotaremos E(Xk) = mk e E[(X µ)k] = µk como os momentos populacionais e os momentos populacionais centrais de k-ésima ordem, respectivamente. Nas situações onde utilizamos mk ou µk assumiremos que estes existem. Também, σ2 representará a variância populacional.\nDemonstração : Para provar (2.23) observemos que\n(∑n\nXi = ∑\nX3 + 3 ∑ ∑\nX2Xk + ∑ ∑ ∑ XiXjXk,\ni=1\ni=1\ni=1 j=1 j̸=k\ni=1 j=1 k=1 i̸=j, i̸=k j̸=k\ndesta expressão obtemos o resultado em (2.23). Similarmente\n(∑n )4\n( n )  n n n\n\n∑ ∑ ∑\nXi =\n∑ Xi ∑ X3 + 3 ∑ ∑ X2Xj +\ni j k\ni=1\ni=1\nn\n i=1\ni i=1 j=1 i̸=j n n\ni=1 j=1 k=1 i̸=j, i̸=k j̸=k\n= ∑ X4 + 4 ∑ ∑ XiX3 + 3 ∑ ∑ X2X2\ni i=1\ni=1 j=1 i̸=j\nj i j i=1 j=1 i̸=j\nn n n n n n n = +6 ∑ ∑ ∑ X2XjXk + ∑ ∑ ∑ ∑ XiXjXkXl·\ni=1 j=1 k=1 i̸=j, i̸=k j̸=k\ni=1 j=1 k=1 l=1 i̸=j, i̸=k, i̸=l j̸=k, j̸=l k̸=l\nUm detalhe importante é que os momentos centrais podem ser calculados a partir dos momentos, por exemplo, µ2 = E[(X − µ)2] = m2 − µ2, µ3 = E[(X − µ)3] = m3 − 3µm2 + 2µ3\ne assim por diante. Sabemos agora como calcular os momentos, até quarta ordem, de X. Vejamos a seguir como calcular os momentos centrais.\nDemonstração : Temos que µ (X) = E(X − µ)3 = E {∑n\n(X − µ)3} = ∑n\nE(X\n3 µ3 — µ) = ·\n3\nNo caso do quarto momento central\nn3 i=1 i\n1\nn3\n{∑n\ni=1 i n2\n}\nda qual obtemos que\nµ (X) = E(X µ)4 = E n4\ni=1\n(Xi − µ)4 ,\n1 µ4(X) =\nE(Xi − µ)4 +\n\n1 + ∑ ∑\n\nE{(Xi − µ)2(Xj − µ)2}·\nn4 i=1\n2 n4\ni=1 j=1 i̸=j\nDesenvolvendo adequadamente chegamos ao resultado em (2.26).\nExemplo 2.16\n, Xn uma amostra aleatória da distribuição Gamma(α, β). Sabemos da Seção 1.2 que E(X) = αβ, Var(X) = αβ2\nmk = βk(α + k − 1)(α + k − 2) α, k ≥ 1· αβ2 E(X) = αβ, Var(X) = n 1 1 µ (X) = µ = (6α3β3 + 3α2β3 + 2αβ3)· 3 n2 3 n\nAté o momento estudamos como calcular os momentos da média amostral. Mais complexo é obter expressões para os momentos da variância amostral S2. O teorema a seguir dedica-se ao objetivo de encontrarmos expressões, até segunda ordem, dos momentos amostrais centrais. Como consequência deste resultado obtemos os momentos da variaˆncia amostral.\nDemonstração : Temos que\nn E(b2) = E n i=1\nX2 n2\nn 2 Xi i=1\n= m2 −\n1  n E \nX2 + ∑\n∑ X2Xj\nAgora\n= m2\n1 — n2 [nm2\n\nn(n − 1)µ2] = ( n − 1 ) (m\n\n— µ )·\nn2b2 =\nn\ni=1\n2\n(Xi − µ)2 − n(X − µ)2 ·\nEscrevendo Yi = Xi − µ, vemos que E(Yi) = 0, Var(Yi) = σ2 e E(Y 4) = µ4. Temos então que\nn2 E(b2) = E\nn i=1 n\n2\nY 2 − nY 2\nn n\n n n n \n= E ∑ Y 4 + ∑ ∑ Y 2Y 2 − 2 ∑ ∑ Y 2Y 2 + ∑ Y 4\n\n1 3 ∑ ∑\n\nY 2Y 2 + ∑\nY 4 ·\nSegue então que\nn i j i=1 j=1 i̸=j\ni=1\ni \n2 1 n2 E(b2) = nµ + n(n − 1)σ2 − [n(n − 1)σ4 + nµ ] + [3n(n − 1)σ4 + nµ ]\n= (n − 2 + 1 ) µ + (n − 2 + 3 ) (n − 1)µ2 · (µ\n= σ2)\nPortanto\nn 4 n 2 2\nVar(b2) = E(b2) − [ E(b2)]2\n= (n − 2 +\n1 ) µ4\n\n(n − 1) (n − 2 + 3 µ 2 —\n\n( n − 1 )2\n= (n − 2 +\n1 ) µ4\nµ2 + (n − 1)(3 − n) ,\ncomo afirmado. As relações (2.29) e (2.30) podem ser provadas de forma semelhante.\nEste é justamente o motivo pelo qual chamamos S2 e não b2 de variância amostral.\nExemplo 2.17 (Continuação do Exemplo 2.16)\ninteir Nesta situação, σ2 = αβ2, µ2 = σ2 e µ4 = m4 − 4m3µ + 6m2µ2 − 3µ4. Obtemos que E(S2) = αβ2 e Var)(S2) = µ4 + 3 − n α2β4· n n(n − 1)\nO seguinte resultado fornece uma justificativa para a nossa definição de covariaˆncia amostral.\nDemonstração : Do Corolário 2.17 sabemos que E(S2) = σ2 e E(S2) = σ2. Para provar que E(S11) = ρσ1σ2 1 1 2 2 observemos que Xi é independente de Xj, (i ̸= j) e de Yj, (i ̸= j). Temos que\nAgora\n(n − 1) E(S11) = E\nE{(Xi − X)(Yi − Y )} =\nn i=1\n(Xi − X)(Yi − Y )] ·\n( ∑n Yj\n∑n Yj\n∑n Xj ∑n\nYj )\ne segue que\n1 = E(XY ) − n [ E(XY ) + (n − 1) E(X) E(Y )] 1 − n [ E(XY ) + (n − 1) E(X) E(Y )] 1 − n2 [n E(XY ) + n(n − 1) E(X) E(Y )] = n − 1 [ E(XY ) E(X) E(Y )] n\n(n − 1) E(S11) = n ( ) [ E(XY ) − E(X) E(Y )], n − 1\nisto é\nE(S11) = E(XY ) − E(X) E(Y ) = Cov(X, Y ) = ρσ1σ2·\nA seguir, voltamos nossa atenção para as distribuições das características da amostra. Existem várias possi- bilidades. Se for necessária a distribuição exata o método de transformação de variáveis pode ser utilizado. As vezes, a técnica da função geradora de momentos pode ser aplicada. Assim, se \\(X_1, X_2, \\cdots , X_n\\) é uma amostra aleatória de uma população com distribuição para a qual existe a função geradora de momentos, a função geradora de momentos da média amostral X é dada por n M (t) = E(etXi/n) = [MX(t/n)]n , (2.32) i=1 onde MX é a função geradora de momentos da distribuição populacional. Se MX (t) tiver alguma forma conhecida seria possível escrever a função de probabilidade ou de densidade de X. Embora este método tem a desvantagem óbvia que se aplica apenas à distribuições para as quais existem todos os momentos, veremos sua efetividade na situação importante de amostras da distribuição normal.\nExemplo 2.18\nSeja \\(X_1, X_2, \\cdots , X_n\\) uma amostra aleatória de tamanho n da distribuição Gama(α, 1). Nesta situação podemos encontrar a função de densidade de X. Temos que\nMX (t) = [MX\n(t/n)]n = 1 , t (1 − t/n)αn n\n&lt; 1,\nda qual obtemos que X ∼ Gama(nα, 1/n).\nExemplo 2.19\nSeja \\(X_1, X_2, \\cdots , X_n\\) uma amostra aleatória da distribuição Uniforme no intervalo (0, 1). Considere a média geométrica\nYn =\nn\ni=1\n1/n Xi ·\nSabemos que log(Yn) = (1/n) ∑n log(Xi) e, desta forma, log(Yn) é a média amostral de log(X1), , log(Xn). A função de densidade comum de log(X1), , log(Xn) é\nex, se x &lt; 0 f (x) = , 0, caso contrário\nque é a distribuição exponencial negativa com parâmetro β = 1. Vemos que a função geradora de momentos de log(Yn) é dada por\nMlog(Yn)\nn (t) = E(et log(Xi)/n) = , (1 t/n)n i=1\ne a função de densidade de log(Yn) é dada por\nflog(Yn)\n\n= \n\nnn Γ(n)[−y]\nn−1\neny\n, se − ∞ &lt; y &lt; 0 ·\n 0, caso contrário\nSegue então que Yn tem por função de densidade\nfYn\n\n= \n\nnn y Γ(n)\nn−1\n[− log(y)]\nn−1\n, se 0 &lt; y &lt; 1 ·\n\nVoltemos ao quantil amostral de ordem p,\n0, caso contrário\nξbp, o qual sabemos é ou X([np]) ou X([np]+1) dependendo se [np] é\num número par ou ímpar, como definido em (2.19). Simplificando, vamos discutir as propriedades de X([np]), onde p ∈ (0, 1) e n é grande. Isso, por sua vez, nos informará sobre as propriedades de ξp. Primeiro observemos que, se U1, U2, , Un é uma amostra aleatória da distribuição U (0, 1) então, pelo Teorema 2.3, temos que\ndo qual obtemos que\nU([np]) ∼ Beta([np], n − [np] + 1),\n[np]\nE(U([np])) =\nn + 1\nn−→→∞ p,\nCov(U , U\n) = n np1\n−→ p (1 − p )·\nUtilizando este resultado e a desigualdade de Chebychev, demonstramos que U −P→ p· (2.33)\nIsso gera a questão\nξbp −→ ξp?\nqualquer seja a distribuição da amostra aleatória X1, , Xn. Para respondermos a pergunta acima vamos utilizar o Lema de Hoeffding, ou seja, para respondermos se o quantil amostral de ordem p converge em probabilidade para o quantil teórico correspondente, utilizaremos o seguinte resultado devido a Hoeffding (1963).\nDemonstração : Dado que as variáveis aleatórias são limitadas ao intervalo (0, 1), sabemos que ehX ≤ (1 − X) + Xeh, isto deve-se a que a função exponencial ehX é convexa e, portanto, seu gráfico é limitado por cima no intervalo 0 ≤ X ≤ 1 pela linha que conecta as ordenadas X = 0 e X = 1. Então E(ehX ) ≤ (1 − E(X)) + E(X)eh· (2.35)\nSeja Sn = ∑n\nXi. Sabemos que\nP (Sn − E(Sn) ≥ nt) = E(1[Sn− E(Sn)−nt≥0]),\ntambém sabemos que\n1[Sn− E(Sn)−nt≥0] ≤ exp (h(Sn − E(Sn) − nt)),\nqualquer seja h uma constante positiva arbitrária. Então P Sn − E(Sn) ≥ nt ≤ E eh(Sn− E(Sn)−nt) (2.36) e como estamos assumindo que as variáveis são independentes, podemos escrever\n( ( ))\n∏ ( ( ))\nEscrevendo µi = E(Xi) temos, pela expressão em (2.35) que E(eh(Xi−µi)) ≤ e−hµi ((1 − µi) + µieh) = ef(h), (2.38) onde \\(F\\) (h) = −hµi + ln(1 − µi + µieh). As primeiras duas derivadas são:\n′ µi\n′′ µie−h(1 − µi)\nf (h) = −µi + e−h(1 − µ ) + µ\ne f (h) = [µi\n\ne−h(1 − µ )]2 ·\nµi\nNa segunda derivada, escolhendo u = µ + e−h(1 − µ ) 0 &lt; u &lt; 1. Portanto, \\(F\\) ′′(h) ≤ 1 . Pela série de Taylor\n\nvemos que este quociente é da forma u(1 − u), sendo\nEntão, pela expressão em (2.38)\nf (h) ≤\nf (0) + \\(F\\) ′(0)h +\n1 h2 = 8\n1 h2· 8\nSubstituindo em (2.36) temos que\nE(eh(Xi−µi)) ≤ e 1 h2 ·\nP (Sn\n— E(Sn\n) ≥ nt) ≤ e−nht+ 1 nh2 ,\ne o mínimo no expoente é atingido quando h = 4t. Então, o mínimo do limite superior da probabilidade é exp(−2nt2).\nDevemos lembrar que esta não é a única maneira de termos uma taxa de convergência para Teorema do Limite Central. Por exemplo, se Y1, Y2, , Yn forem variáveis aleatórias independentes e identicamente distribuídas, utilizando o Teorema de Berry-Esseen2, temos que\n( ∑n ∑\n) ( √ Var(Y1))\nC E|Y1\n— E(Y1)|\nP i=1\nXi −\ni=1\nE(Xi) ≥ nt ≤ Φ t n\n\n√n\n\nVar3/2(Y ) ·\n2\nDemonstração : Berry (1941); Esseen (1942).\nPode-se consultar o livro de Feller (1971) para uma demonstração moderna.\nExemplo 2.20\nCaso a amostra aleatória seja Bernoulli(µ), temos que n Xk ∼ Binomial(n, µ)· i=1 Então, segundo a desigualdade de Hoeffding\nP (X − µ ≥ t) ≤ exp(−2nt2)· Uma vantagem da desigualdade no Lema de Hoeffding é que não assume-se conhecimento da variância e, em geral, o limite da probabilidade é mais acurado do que outras desigualdades. Caso as variáveis aleatórias sejam limitadas como a ≤ Xi ≤ b, com a &lt; b, o limite superior da desigualdade (2.34) seria exp − 2nt2/(b − a)2 .\nExemplo 2.21\nSejam X1, , Xn variáveis aleatórias com distribuição U ( 1, 1). Nesta situação E(X) = 0, a = 1 e b = 1. A desigualdade de Hoeffding assume a forma P (X ≥ t) ≤ exp ( − nt2/2)·\nDemonstração : Para ϵ &gt; 0 qualquer, podemos escrever P (|ξp − ξp| &gt; ϵ) = P (ξp &gt; ξp + ϵ) + P (ξp &lt; ξp − ϵ)· Pelo Teorema 2.9, podemos escrever P (ξbp &gt; ξp + ϵ) = P (p &gt; Fbn(ξp + ϵ))\nn = P i=1\n1[Xi&gt;ξp+ϵ] &gt; n(1 − p))\nn = P i=1\nVi −\n∑i=1\nE(Vi) &gt; nδ1),\nonde Vi = 1[Xi&gt;ξp+ϵ] e δ1 = \\(F\\) (ξp + ϵ) − p. Da mesma forma, P (ξbp &lt; ξp − ϵ) = P (p &gt; Fbn(ξp − ϵ))\nn = P i=1\nWi −\n∑i=1\nE(Wi) &gt; nδ2),\nonde Wi = 1[Xi&lt;ξp−ϵ] e δ2 = p − \\(F\\) (ξp − ϵ) − p. Portanto, utilizando o Lema de Hoeffding (Lema 2.20), temos P (ξbp &gt; ξp + ϵ) ≤ exp(−2nδ2) P (ξbp &lt; ξp − ϵ) ≤ exp(−2nδ2)· Colocando δϵ = min{δ1, δ2}, a prova está completa.\nDemonstramos que\nlim P (|ξbp − ξp| &gt; ϵ) ≤ lim 2 exp(−2nδ2) = 0,\no qual significa que ξp −→ ξp. Em outras palavras, sempre que ξp seja solução única da desigualdade \\(F\\) (ξp ) ≤ p ≤ \\(F\\) (ξp), 0 &lt; p &lt; 1, o quantil amostral converge em probabilidade para o quantil populacional e isto sempre acontece nas distribuições contínuas. Um detalhe importante é que para demonstrarmos a convergência em probabilidade de ξp utilizamos o Lema de Hoeffding e ele depende da existência da esperança. O seguinte resultado fornece a distribuição assintótica da r-ésima estatística de ordem amostral de uma po- pulação com uma função de distribuição \\(F\\) , absolutamente contínua, e função de densidade \\(F\\) .\nDemonstração : Vamos demonstrar somente para o caso p = 1/2. Observemos que ξ1/2 é mediana única dado que f (ξ1/2) &gt; 0. Primeiro, consideremos que n seja ímpar, por exemplo, n = 2m − 1, logo P [√n(X(m) − \\(F\\) −1(1/2)) ≤ t] = P (X(m) ≤ t/√n + \\(F\\) −1(1/2))·\nSeja Sn o número de X que excedem t/ n + F (1/2). Então\nPercebemos que\nt X(m) ≤ √n + F\n(1/2) se, e somente se, Sn ≤ m − 1 =\nn − 1 · 2\nSn ∼ Binomial(n, 1 − \\(F\\) (F −1(1/2) + t/√n))· Fazendo pn = 1 − \\(F\\) (F −1(1/2) + t/√n), temos que\nP [√n(X\n\n\n\n— F −1(1/2)) ≤ t] = P (Sn\n≤ n − 1 )\n( Sn − npn 1 (n − 1) − npn )\n= P\nUtilizando o Teorema de Berry-Esseen, temos que\n√npn(1 − pn) ≤ √npn(1 − p ) · n\n{ ( n − 1 ) ( 1 (n − 1) − npn )}\nlim P n→∞\nSn ≤ 2\n— Φ √np\n(1 − pn)\n= 0·\nEscrevendo\n1 (n − 1) − npn npn(1 − pn)\n=\n√n( 1 − pn) 1/2 √n( − 1 + \\(F\\) (t/√n + \\(F\\) −1(1/2)))\n= 2t\n1/2 F (t/√n + \\(F\\) −1(1/2)) − \\(F\\) (F −1(1/2))\n−→ 2tf\n(F −1(1/2))·\nEntão\n( 1 (n − 1) − npn )\n( ( −1 ))\nΦ npn ou\n(1 − pn)\n≈ Φ 2tf F\n(1/2)\n√n(X\n\n− F\n\n( 1 )) −D→ N (0,\n4f 2\n1 (F −1(1/2)\n)) ·\nQuando n é par, digamos n = 2m, ambos P (√n X(m) F −1(1/2) t) quanto P (√n X(m+1) F −1(1/2) t) convergem a Φ(2tf (F −1(1/2))).\nObserve que o quantil amostral de ordem p, assintótica\nξbp, como consequência do Teorema 2.23, tem por distribuição\nN (ξ , 1 p(1 − p)) , onde ξp é o correspondente quantil populacional e \\(F\\) é a função de densidade populacional. Por exemplo, suponha temos uma amostra aleatória da di√stribuição N (µ, σ2) de tamanho n. Seja ξb1/2 a mediana amostral obtida dessa b ( πσ2 )\nTambém devemos ter em consideração que para demonstrarmos o Teorema 2.23 utilizamos a Teorema de Berry- Esseen, o qual depende da existência dos primeiros dois momentos da variável aleatória. Com isso, caso X Cauchy(µ, σ), o Teorema 2.23 não se aplica.",
    "crumbs": [
      "Módulo I",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estatísticas de ordem</span>"
    ]
  },
  {
    "objectID": "ModuloI/Aula02/index.html#gráficos-descritivos",
    "href": "ModuloI/Aula02/index.html#gráficos-descritivos",
    "title": "2  Estatísticas de ordem",
    "section": "2.4 Gráficos descritivos",
    "text": "2.4 Gráficos descritivos\nVejamos alguns conjuntos de dados disponíveis na linguagem de programação R (R Core Team, 2014), especifi- camente na libraria datasets, que nos permitiram mostrar a utilidade dos momentos amostrais para resumir as informações contidas nos dados. Para consultar estes conjuntos de dados basta digitar library(help = “datasets”) Alguns dos diversos exemplos disponíveis serão apresentados aqui.\nExemplo 2.22 (Puromicina)\nOs dados sobre a velocidade de uma reação enzimática são obtidos por Treloar (1974) e disponíveis no arquivo de dados Puromycin. O número de contagens por minuto de produto radioativo a partir da reação foi medida como uma função da concentração do substrato em partes por milhão (ppm) e a partir destas contagens a taxa\ninicial (ou velocidade) da reação foi calculada (contagens/min/min). O experimento foi realizado uma vez com a enzima tratada com puromicina e depois com a enzima não tratada. A estrutura destes dados tem 23 linhas e 3 colunas, cada coluna contendo as informações das variáveis: conc: um vector numérico de concentrações de substrato (ppm); rate: um vector numérico de taxas de reação instantânea (contagens/min/min); state: um fator com níveis treated (tratada) ou untreated (não tratada). Para a leitura e observação dos nomes das variáveis utilizamos os comandos a seguir: data(Puromycin) names(Puromycin) Uma maneira de obtermos estatísticas descritivas é utilizando as linhas de comando a seguir: summary(rate[state==’treated’]) Min. 1st Qu. Median Mean 3rd Qu. Max. 47.0 104.5 145.5 141.6 193.2 207.0 e summary(rate[state==’untreated’]) Min. 1st Qu. Median Mean 3rd Qu. Max. 51.0 85.0 115.0 110.7 137.5 160.0 para o caso da variável rate, as concentrações, obtidas as estatísticas descritivas segundo os níveis do fator state, se as concentrações foram ou não tratadas com puromicina. No caso das estatísticas descritivas acerca das concentrações de substrato, variável conc, temos: summary(conc[state==’treated’]) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.020 0.060 0.165 0.345 0.560 1.100 e summary(conc[state==’untreated’]) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.0200 0.0600 0.1100 0.2764 0.3900 1.1000 Os valores mínimos é máximos foram registrados sempre com os nomes de Min. e Max., respectivamente. O primeiro e terceiro quantis ou quantis de 25% e 75% respectivos são registrados com os nomes 1st Qu. e 3rd Qu. e, finalizando, o resumo de informações de estatísticas de posição temos os valores de medianas (Median) e médios (Mean).\nExemplo 2.23 (Rock )\nMedições em 48 amostras de rochas de um reservatório de petróleo estão disponíveis no arquivo de dados rock. Este conjunto de dados contem 48 linhas e 4 colunas numéricas, descritas a seguir: area área do espaço de poros, em pixels de 256 por 256; peri perímetro em pixels; shape perímetro/sqrt(area) perm permeabilidade em mili-Darcies. Doze amostras do núcleo de reservatórios de petróleo foram amostrados por 4 seções transversais. Cada amostra foi medida no núcleo para a permeabilidade e cada seção transversal tem uma área total de poros, perímetro total de poros e forma. A fonte destes dados é a BP Research e a análise das imagens foi de Ronit Katz, Oxford University. Na geologia, a permeabilidade é a medida da capacidade de um material (tipicamente uma rocha) para transmitir fluídos. E´ de grande importância na determinação das características de fluxo dos hidrocarbonetos em reservatórios de petróleo e gás e da água nos aquíferos. A unidade de permeabilidade é o Darcy ou, mais habitualmente, o mili- Darcy ou mD.\n\n2.4.1 2.4.1 Gráfico de Boxplot\nEm 1977, John Tukey (Tukey, 1977) publicou uma proposta que posteriormente foi reconhecida como sendo um eficiente método para mostrar cinco número que sumarizam qualquer conjunto de dados. O gráfico proposto é chamado de boxplot (também conhecido como box and whisker plot) e resume as seguintes medidas de posição estatísticas: mediana, quantis inferior e superior e os valores mínimos e máximos. Os quantis inferior e superior entendem-se serem os quantis de 25% e 75%, respectivamente. No caso do exemplo 2.22, deixamos a disposição os dados digitando attach(Puromycin) e com isso podemos mudar o nome dos níveis do fator da forma state=factor(state,labels=c(’Tratada’,’N~ao tratada’)) Então, com os comandos a seguir geramos o gráfico de boxplot, tanto para a variável rate quanto para a variável conc, estas segundo os níveis do fator state. par(mar=c(5,4,3,1)) boxplot(rate ~ state, col = grey(c(0.4,1)), main=’Taxas de reaç~ao instant^anea’)\npara o caso do rate. Observemos que a primeira linha par(mar=c(5,4,3,1)) serve somente para dimensionar a janela gráfica. Para o caso da variável conc utilizamos comandos semelhantes. par(mar=c(5,4,3,1)) boxplot(conc ~ state, col = grey(c(0.4,1)), main=’Concentraç~oes de substrato’) O resultado deste trabalho pode ser observado na Figura 2.3. Interpretemos o gráfico de boxplot. A caixa (box) propriamente contém a metade 50% dos data. O limite superior da caixa indica o percentil 75% dos dados e o limite inferior da caixa indica o percentil 25%. A distancia entre esses dois quantis é conhecida como inter-quantil. A linha na caixa indica o valor de mediana dos dados. Se a linha mediana dentro da caixa não é equidistante dos extremos, diz-se então que os dados são assimétricos. O boxplot da variável rate (esquerda na Figura 2.3) é um exemplo de dados simétricos já a situação da variável conc (direita na Figura 2.3) é um caso clássico de assimetria dos dados. Os extremos do gráfico indicam os valores mínimo e máximo, a menos que valores outliers3 estejam presentes, nesse caso o gráfico de estende ao máximo de 1.5 vezes da distância inter-quantil. Os pontos fora do gráfico são então outliers ou suspeitos de serem outliers. Mais elegante seria utilizar a biblioteca de funções ggplot2, para isso, digitamos: library(ggplot2) Para gerar os gráficos de boxplot respectivos, fazemos: par(mar=c(5,4,3,1)) qplot(state, rate, geom=c(“boxplot”, “jitter”), main=“Taxas de reaç~ao instant^anea”, xlab=““, ylab=” “) e par(mar=c(5,4,3,1)) qplot(state, conc, geom=c(”boxplot”, “jitter”), main=“Concentraç~oes de substrato”, xlab=““, ylab=” “)\n3Em estatística, outlier, valor aberrante ou valor atípico, é uma observação que apresenta um grande afastamento das demais observações em uma amostra. A existência de outliers implica, tipicamente, em prejuízos a interpretação dos resultados dos testes estatísticos aplicados as amostras.\nTaxas de reação instantânea Concentrações de substrato\nTratada Não tratada Tratada Não tratada\nFigura 2.3: Gráfico de boxplot da variável rate à esquerda e da variável conc à direita, segundo os níveis do fator state, se a enzima foi tratada ou não com puromicida. Gráfico gerado utilizando a função R boxplot,\nTaxas de reação instantânea Concentrações de substrato\n200\n0.9\n150\n0.6\n100 0.3\n50\nTratada Não tratada\n0.0\nTratada Não tratada\nFigura 2.4: Gráfico de boxplot da variável rate à esquerda e da variável conc à direita, segundo os níveis do fator state, se a enzima foi tratada ou não com puromicida. Gráfico gerado utilizando a função R qplot, opção geom=c(”boxplot”, ”jitter”).\nobtendo-se assim os gráficos na Figuras 2.4. Além de melhor qualidade gráfica acrescentamos os pontos observados no boxplot, isso permite termos uma ideia também da dispersão dos dados. Vejamos as vantagens do boxplots. Mostra graficamente a posição central dos dados (mediana) e a tendência. Fornece algum indicativo de simetria ou assimetria dos dados. Ao contrário de muitas outras formas de mostrar os dados, o boxplots mostra os outliers. Utilizando o boxplot para cada variável categórica no mesmo gráfico, pode-se facilmente comparar os dados. Esta é a situação no exemplo na Figura 2.3, podemos observar o comportamento das variáveis rate e conc segundo os níveis do fator state. Um detalhe do boxplot é que ele tende a enfatizar as caudas da distribuição, que são os pontos ao extremo nos dados. Também fornece detalhes da distribuição dos dados. Mostrar o histograma (Seção 2.4.2) em conjunto com o boxplot ajuda a entender a distribuição dos dados, constituindo estes dos gráficos ferramentas importantes na análise exploratória. Logicamente, o comportamento dos dados dentro da caixa (box), como podemos perceber nas figuras 2.3 e 2.4, permanece um mistério. Isso porque caso estejam os dados bem espalhados ou não, o gráfico boxplot continua mostrando uma caixa. Somente perceberemos algum comportamento diferente se o valor da mediana estiver mais próximo de um dos extremos desta caixa. Para tentar diminuir essa limitação foi sugerido uma melhoria, obtendo-se o chamada boxplot entalhado (notched boxplot). Com as linhas de comando a seguir se obtém os gráficos na Figura 2.5.\npar(mar=c(5,4,3,1)) boxplot(rate ~ state, col = grey(c(0.4,1)), notch=TRUE, main=’Taxas de reaç~ao instant^anea’)\ne par(mar=c(5,4,3,1)) boxplot(conc ~ state, col = grey(c(0.4,1)), notch=TRUE, main=’Concentraç~oes de substrato’)\nObserva-se que a única diferença é a inclusão da opção notch=TRUE, permanecendo todas as outras instruções iguais. Mais elaborado é o chamado violin plot, mistura de boxplot com estimação de densidade, tema este tratado na Seção 4.3. Este gráfico, introduzido no artigo Hintze & Nelson (1998), sinergicamente combina o gráfico de boxplot e a estimação da densidade, também chamado de histograma suavizado, em uma única tela que revela a estrutura encontrada nos dados. Com as linhas de comando a seguir se obtém os gráficos na Figura 2.6.\npar(mar=c(5,4,3,1)) qplot(state, rate, geom = c(“violin”, “jitter”), notch=TRUE, main=“Taxas de reaç~ao instant^anea”, xlab=““, ylab=” “)\ne par(mar=c(5,4,3,1)) qplot(state, conc, geom=c(“violin”, “jitter”), notch=TRUE, main=“Concentraç~oes de substrato”, xlab=““, ylab=” “)\nEste gráfico é similar ao boxplot excepto que mostra também a densidade de probabilidade dos dados. Pode incluir também um marcador para a média dos dados e uma caixa que indica a distância interquartil, como nos gráficos boxplot. O objetivo do gráfico violin plot é o mesmo do que o boxplot original porém, considera de alguma maneira o comportamento dos dados dentro da caixa (box). Assim, percebemos melhor a distribuição dos dados dentro do intervalo interquartil.\nTaxas de reação instantânea Concentrações de substrato\nTratada Não tratada Tratada Não tratada\nFigura 2.5: Gráfico de boxplot entalhado da variável rate à esquerda e da variável conc à direita, segundo os níveis do fator state, se a enzima foi tratada ou não com puromicida. Gráfico gerado utilizando a função R qplot, opção geom=c(”boxplot”, ”jitter”), notch=TRUE.\nTaxas de reação instantânea Concentrações de substrato\n200\n0.9\n150\n0.6\n100 0.3\n50\nTratada Não tratada\n0.0\nTratada Não tratada\nFigura 2.6: Gráfico de violin plot da variável rate à esquerda e da variável conc à direita, segundo os níveis do fator state, se a enzima foi tratada ou não com puromicida. Gráfico gerado utilizando a função R qplot, opção geom=c(”vioplot”, ”jitter”), notch=TRUE.\n\n\n2.4.2 2.4.2 Histograma\nUm histograma é uma representação gráfica da função de probabilidades ou da função de densidade de um conjunto de dados independentes e foi introduzido pela primeira vez por Karl Pearson4. A representação mais comum do histograma é um gráfico de barras verticais. A palavra histograma é de origem grega, derivada de duas: histos que pode significar testemunha no sentido de aquilo que se vê, como as barras verticais do histograma, e da também palavra grega gramma que significa desenhar, registrar ou escrever. Histograma Histograma com a curva norma\n−2 −1 0 1 2 Dados simulados\n−2 −1 0 1 2 Dados simulados\nFigura 2.7: Gráfico de histograma para dados simulados.\nPara construir um exemplo controlado do gráfico de histograma, simulamos uma amostra de tamanho 150 da distribuição normal padrão, com o comando x=rnorm(150) e, depois, construímos um gráfico colorido com as linhas de comando par(mar=c(5,4,2,1)) hist(x, breaks=12, col=“red”, xlab=“Dados simulados”, ylab=’Frequ^encia’, main=“Histograma”) box() Posteriormente, acrescentamos a este gráfico uma linha com a densidade normal par(mar=c(5,4,2,1)) h=hist(x, breaks=10, col=“red”, xlab=“Dados simulados”, ylab=’Frequ^encia’, main=“Histograma com a curva normal”) xfit=seq(min(x),max(x),length=40) yfit=dnorm(xfit,mean=mean(x),sd=sd(x)) yfit=yfitdiff(h$mids[1:2])length(x) lines(xfit, yfit, col=“blue”, lwd=2) box()\n4Pearson, K. (1895). Contributions to the Mathematical Theory of Evolution. II. Skew Variation in Homogeneous Material. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 186: 343-414.\nDesta forma geramos os gráficos na Figura 2.7. A ideia é mostrar que o histograma assemelha-se ao gráfico da densidade normal, a densidade dos dados.\nHistograma c2(6) Histograma c2(6)\n2 4 6 8 10 12 14 14 intervalos\n2 4 6 8 10 12 14 26 intervalos\nFigura 2.8: Histogramas da distribuição χ2 com 6 graus de liberdade. Número de intervalos 14 e 26, respectivamente.\nO histograma é um gráfico composto por retângulos justapostos em que a base de cada um deles corresponde ao intervalo de classe e a sua altura à respectiva frequência. A construção de histogramas tem caráter preliminar em qualquer estudo e é um importante indicador da distribuição de dados. Pode indicar se uma distribuição aproxima-se de uma densidade normal como pode indicar mistura de densidades, quando os dados apresentam várias modas. Os histogramas podem ser um mau método para determinar a forma de uma distribuição porque são fortemente influenciados pelo número de intervalos utilizados. Por exemplo, decidimos gerar 50 amostras da densidade χ2(6), da forma set.seed(5678) z=rchisq(50, df=6) Os gráficos de histogramas correspondentes com 14 e 26 intervalos são apresentados na Figura 2.8 e foram gerados com as linhas de comando\npar(mar=c(5,4,2,1)) hist(z, breaks=14, col=“blue”, main=expression(paste(’Histograma ’, chi^2,’(6)’)), ylab=’Frequ^encia’, xlab=’14 intervalos’) box()\ne\npar(mar=c(5,4,2,1)) hist(z, breaks=26, col=“blue”, main=expression(paste(’Histograma ’, chi^2,’(6)’)), ylab=’Frequ^encia’, xlab=’26 intervalos’) box()\nNa Figura 2.9 podemos observar os gráficos de histograma obtidos das variáveis descritas no Exemplo 2.23. A situação em (a) representa o caso em a distribuição dos dados de assemelha à distribuição normal, já a situação descrita no gráfico em (b) mostra-se uma mistura de densidades, percebemos a existência de duas modas. (a) Área do espaço de poros (b) Perímetro em pixels\n0 4000 8000 12000 pixels de 256 x 256\n0 1000 3000 5000\nFigura 2.9: Histogramas das variáveis no Exemplo 2.23.\nOutras situações no mesmo exemplo, mas diferentes variáveis, são descritas nos gráficos na Figura 2.10. Nessa figura apresentamos dois gráficos, chamados de (c) e (d), nesta figura. Correspondem, como podemos observar, à distribuições assimétricas e descrevem os dados coletados nas variáveis shape e perm do arquivo de dados Rock, Exemplo 2.23. Os histogramas foram pensados somente para o caso de variáveis contínuas, porém é uma descrição discreta delas. Logicamente, também podemos utiliza-los em situações de variáveis aleatórias discretas, nada impede isso. Estas figuras foram geradas utilizando a configuração padrão do comando hist, isto é, utilizamos uma maneira automática de determinar o número de intervalos, mais adiante dedicamos maior atenção a diferentes formas de calcular este número. Como pode ter sido observado, além de não ficar claro como determinar o número de intervalos nem como delimitar os intervalos, também não ficou claro o que queremos realmente observar com o gráfico desta função. Vejamos agora uma definição mais clara do histograma, esta definição nos permitirá obter propriedades impor- tantes.\n\nPerímetro/sqrt(Área) (d) Permeabilidade\n\n0.1 0.2 0.3 0.4 0.5\n0 200 600 1000 1400 mili−Darcies\nFigura 2.10: Histogramas das variáveis no Exemplo 2.23.\nFoi provado por Robertson (1967) que, dados os intervalos I1, I2, , Ik, o histograma \\(F\\) é um estimador de máxima verossimilhança5 dentre os estimadores expressados como funções simples e semicontínuas superiormente, isto se o fecho de cada intervalos contiver duas ou mais observac¸ões. Os gráficos apresentados nas figuras 2.7, 2.9 e 2.10 são histogramas também segundo a proposta de Robertson (1967). Pode-se observar que este estimador tem duas limitações importantes: a dependência do comprimento do intervalo e o fato de o histograma não constituir uma função contínua. A primeira destas limitações foi amplamente estudada por Wegman (1975). Ele provou que os pontos extremos de cada intervalo Ik devem ser coincidentes com observações e que, se o número mínimo de observações em cada intervalo aumente, conforme aumenta o tamanho da amostra, o estimador \\(F\\) é consistente6. A segunda limitação importante do histograma, isto é, o fato de ele não constituir uma função contínua, incentivou diversos estudos na procura de estimadores contínuos da função de densidade. No Capítulo 3, a Seção 4.3 dedica-se a mostrar estimadores contínuos da função de densidade.\n5Os estimadores de máxima verossimilhanc¸a serão estudados na Seção 4.2 6Estimadores consistentes serão estudados na Seção 3.1.1\nCálculo automático do número de intervalos num histograma Uma questão importante é determinar de maneira automatizada o número de intervalos disjuntos que serão utili- zados para a construção do gráfico. Uma primeira forma de escolher o número de intervalos foi dada por Sturges (1926) e que constitui a forma padrão no R. Conhecida como fórmula de Sturges é dada por k = [log2(n) + 1], (2.41) isto significa que o número de intervalos é a parte inteira do logaritmo base 2 do número de observações mais 1. Outras expressões comumente utilizadas são a fórmula de Scott (Scott, 1979) h = 3.5s/√3 n, onde s é o desvio padrão e a fórmula de Freedman Diacconi (Freedman & Diaconis, 1981) h = 2IQR(x)/√3 n, onde IRQ é a diferença entre o terceiro e o primeiro quantil.\nExemplo 2.24\nNa libraria de funções R robustbase temos disponíveis dados do teor de cálcio e do pH em amostras de colo coletadas em diferentes comunidades da região de Condroz, na Bélgica. Podemos ler estes dados digitando as linhas de comando abaixo, primeiro para escolher a libraria de funções e depois para selecionar os dados. library(robustbase) data(condroz) Temos registadas duas variáveis: Ca que registra o tero de cálcio na amostra de solo e o pH, o pH corres- pondente. Construímos histogramas da variável Ca segundo a três formas de escolha do número de intervalos e os apresentamos na Figura 2.11. Os dados deste exemplo foram publicados em: Hubert, M. and Vandervieren, E. (2006). An Adjusted Boxplot for Skewed Distributions, Technical Report TR-06-11, KULeuven, Section of Statistics, Leuven.\nSturges\nScott\nFreedman−Diaconis\n0 1000 2000 3000 4000 Ca\n0 1000 2000 3000 4000 Ca\n0 1000 2000 3000 4000 Ca\nFigura 2.11: Diferentes histogramas da variável Ca no Exemplo 2.24.\n\n\n2.4.3 2.4.3 Gráficos para verificar normalidade\nUm primeiro gráfico chamado de qq-norm permite a comparação de duas distribuições de probabilidades traçando seus quantis uns contra os outros. Depois exploramos um gráfico mais recente, conhecido como worm plot (gráfico de minhoca), consistindo numa determinada coleção de de qq-norm.\nQQ-norm O gráfico quantil-quantil ou qq-plot, proposto por Wilk & Gnanadesikan (1968), é um dispositivo gráfico explo- ratório utilizado para verificar a validade de um pressuposto de distribuição para um conjunto de dados. Em geral, a ideia básica é a de calcular o valor teoricamente esperado para cada ponto de dados com base na distribuição em questão. Se os dados de fato seguirem a distribuição assumida os pontos deste gráfico formarão aproximadamente uma linha reta. Percebemos que podemos verificar com este gráfico qualquer densidade contínua, eventualmente pode ser uti- lizado também para funções de probabilidade. O qq-plot vai apresentar-se como uma linha reta se a densidade assumida estiver correta. Vejamos o caso particular de verificarmos se a densidade é normal, nesta situação o gráfico qq-plot será chamado de qq-norm. Primeiro consideraremos a situação da densidade normal padrão. Seja z1, z2, , zn uma amostra aleatória de uma distribuição normal com média µ = 0 e desvio padrão σ = 1. As estatísticas de ordem amostrais são z(1) ≤ z(2) ≤ ≤ z(n)· Estes valores desempenharão o papel dos quantis da amostra. Agora, quais devemos tomar como os quantis teóricas correspondentes? Se a função de distribuição cumulada da densidade normal padrão fosse denotada por Φ, usando a notação quantil, se ξq é o q-ésimo quantil de uma distribuição normal, então Φ(ξq) = q, ou seja, a probabilidade de uma amostra normal ser inferior a ξq é, de fato, apenas q. Considere o primeiro valor ordenado z(1). O que podemos esperar que o valor Φ(z(1)) seja? Intuitivamente, esperamos que essa seja a probabilidade de assumir um valor no intervalo (0, 1/n). Do mesmo modo, espera-se que Φ(z(2)) seja a probabilidade de assumir um valor no intervalo (1/n, 2/n). Continuando, esperamos que Φ(z(n)) seja a probabilidade de assumir um valor no intervalo (n 1)/n, 1). Assim, o quantil teórico desejamos seja definido pelo inverso da função de distribuição acumulada normal padrão. Em particular, o quantil teórico correspondente ao quantil empírico z(i) deve ser\npara i = 1, 2, , n.\nξ = q i − 0, 5 , q n\nQQ−plot nomal\nQQ−plot nomal\nQQ−plot nomal\n−3 −2 −1 0 1 2 3 Quantis teóricos\n−3 −2 −1 0 1 2 3 Quantis teóricos\n−3 −2 −1 0 1 2 3 Quantis teóricos\nFigura 2.12: Diferentes qqplot para dados normais.\nNa Figura 2.12, a esquerda acima exibimos o qq-norm de uma pequena amostra normal de tamanho 5. Os restantes quadros na Figura 2.12 exibem as plotagens de qq-norm para amostras normais de tamanhos n = 100 e\nn = 1000, respectivamente. Como o tamanho da amostra aumenta, os pontos encontram-se mais perto da linha y = x. Estes gráficos (Figura 2.12) foram gerados utilizando as linhas de comando: set.seed(1278) x=rnorm(5) qqnorm(x, xlim=c(-3,3), ylim=c(-3,3), cex=0.6, pch=19, ylab=’Quantis amostrais’, xlab=’Quantis teóricos’, main=’QQ-plot nomal’) qqline(x,col=“red”) text(-1,2,’n=5’) para a situação de amostra de tamanho 5. A primeira linha de comando serve para fixar o gerador de números laetórios e, dessa forma, podermos simular sempre a mesma amostra e reproduzir o gráfico idêntico. Nas outras situações somente muda-se o tamanho da amostra que se quer gerar.\nQQ−plot nomal QQ−plot nomal\n−3 −2 −1 0 1 2 3 Quantis teóricos\n−3 −2 −1 0 1 2 3 Quantis teóricos\nFigura 2.13: Diferentes qqplot para dados não normais. Assim, os comandos para gerar o segundo e terceiro gráficos são: x=rnorm(100) qqnorm(x, xlim=c(-3,3), ylim=c(-3,3), cex=0.6, pch=19, ylab=’Quantis amostrais’, xlab=’Quantis teóricos’, main=’QQ-plot nomal’) qqline(x,col=“red”) text(-1,2,’n=100’)\nx=rnorm(1000) qqnorm(x, xlim=c(-3,3), ylim=c(-3,3), cex=0.6, pch=19, ylab=’Quantis amostrais’, xlab=’Quantis teóricos’, main=’QQ-plot nomal’) qqline(x,col=“red”) text(-1,2,’n=1000’) Caso os dados não forem padronizados bastar aplicar a transformação (X − µ)/σ, onde X representa os dados originais e µb e σb representam os estimadores dos parâmetros µ e σ, respectivamente.\nEstes gráficos podem indicar afastamentos da normalidade por isso apresentamos duas situações de dados não simétricos e com cuadas pesadas. Na Figura 2.13, mostramos o que acontece se os dados forem da distribuição t-Student(8) e da distribuição χ2(5), sempre de tamanho n = 1000. Observe, em particular, que os dados a partir da distribuição t-Student seguem a curva normal bem de perto até os últimos pontos em cada extremo. Na outra situação o afastamento da distribuição normal é evidente. Foi mencionado que o qq-norm é uma situação particular do qq-plot devido a este último permitir comparar os quantis amostrais com os quantis distribucionais. Com isto queremos dizer que o qq-plot serve para verificar se os dados forem t-Student ou χ2(5), por exemplo. Na Figura 2.14 apresentamos a aparência dos gráficos qq-plot caso queira-se verificar se as amostras seguem distribuição t-Student(8) ou χ2(5), respectivamente.\nQQ plot para t−Student(8)\n−4 −2 0 2 4 t−Student(8)\nQQ plot para c2(5)\n0 5 10 15 20 c2(5)\nFigura 2.14: Diferentes qqplot para dados não normais. Os gráficos na Figura 2.14 foram gerados pelas linhas de comandos qqplot(qt(ppoints(1000), df = 8), x, cex=0.6, pch=19, main = “QQ plot para t-Student(8)”, xlab=“t-Student(8)”) qqline(x, distribution = function(p) qt(p, df = 8), prob = c(0.1, 0.6), col = 2) no caso t-Student(8) e qqplot(qchisq(ppoints(1000), df = 5), x, cex=0.6, pch=19, main = expression(“QQ plot para” ~~ {chi^2}(5)), xlab=expression({chi^2}(5))) qqline(x, distribution = function(p) qchisq(p, df = 5), prob = c(0.1, 0.6), col = 2) para o caso χ2(5).\nWorn plot O worm-plot é uma série de parcelas de gráficos qq-plot retificados. Constitui uma ferramenta de diagnóstico para visualização de quão bem um modelo estatístico se ajusta aos dados, para encontrar locais em que o ajuste pode ser melhorado e para comparar o ajuste de diferentes modelos. Na Figura 2.15 mostramos este gráfico para duas situações: a esquerda os dados são normais e a direita os dados são t-Student com 8 graus de liberdade. Nesta situação aparece bem a qualidade da observação com esta\n−4 −2 0 2 4 Unit normal quantile\n−4 −2 0 2 4 Unit normal quantile\nFigura 2.15: Diferentes worm-plot para dados normais.\nfigura. Se os dados forem normais o curva worm-plot ou gráfico de minhoca deve aparentar um verme achatado, os pontos próximos a curva vermelha e com poucas oscilações. Quando aplicamos este gráfico ao caso t-Student percebemos uma oscilação grande no verme e com pontos fugindo da banda de confiança. Isso comprova que os dados não seguem como referência a distribuição normal.\n−4 −2 0 2 4 Unit normal quantile\n−4 −2 0 2 4 Unit normal quantile\nFigura 2.16: Diferentes worm-plot para dados não normais. As linhas a seguir mostram os comandos necessários para gerar os gráficos na Figura 2.15. Utilizamos a libraria de comandos R gamlss (Rigby & Stasinopoulos, 2005).\nlibrary(gamlss) x=rnorm(1000) wp(gamlss(x~1), cex=0.6) x=rt(1000, df=8) wp(gamlss(x~1), cex=0.6)\nNa Figura 2.16, a esquerda temos o caso de dados com distribuição χ2(5) e a direita dados com distribuição Cauchy padrão. Nestas situações fica claro que os dados não são normais. Oa gráficos na figura foram gerados pelas linhas de comando a seguir. x=rchisq(1000, df=5) wp(gamlss(x~1), cex=0.6) x=rcauchy(1000) wp(gamlss(x~1), cex=0.6)",
    "crumbs": [
      "Módulo I",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estatísticas de ordem</span>"
    ]
  },
  {
    "objectID": "ModuloI/Aula02/index.html#exercícios",
    "href": "ModuloI/Aula02/index.html#exercícios",
    "title": "2  Estatísticas de ordem",
    "section": "2.5 Exercícios",
    "text": "2.5 Exercícios\nExercícios da Seção 2.1 1. Seja X ∼ Bernoulli( 1 ) e considere todas as possíveis amostras aleatórias de tamanho n = 3. Calcule Xn e S2 cada uma das\n2 n oito amostras. Encontre a função de probabilidade de Xn e S2. 2. Um dado é lançado. Seja X o valor da face superior que aparece e X1, X2 duas observações independentes de X. Encontre a função de probabilidade de Xn. 3. Seja X1, , Xn uma amostra aleatória de alguma população. Mostre que (n − 1)Sn\nmax |Xi Xn| &lt; 1≤i≤n\nonde Sn é a raiz quadrada positiva da variância amostral S2.\n√n ,\nExercícios da Seção 2.2 1. Seja (X(1), X(2), , X(n)) o conjunto das estatísticas de ordem de n variáveis aleatórias independentes \\(X_1, X_2, \\cdots , X_n\\) com função de densidade comum\nf (x) =\nβe−xβ, se x 0 · 0, caso contrário\n\nMostre que X(s) e X(r) − X(s) são independentes para quaisquer r &gt; s.\nEncontre a função de densidade de X(r+1) − X(r).\nSeja Z1 = nX(1), Z2 = (n − 1)(X(2) − X(1)), Z3 = (n − 2)(X(3) − X(2)), …, Zn = ((X(n) − X(n−1))). Prove que (Z1, Z2, , Zn) e (\\(X_1, X_2, \\cdots , X_n\\)) são identicamente distribuídas.\n\n\nProvar o Teorema 2.1\nSejam \\(X_1, X_2, \\cdots , X_n\\) variáveis aleatórias com distribuição geométrica de parâmetros p1, p2, , pn, respectivamente. Prove que Nn = min(\\(X_1, X_2, \\cdots , X_n\\)) têm também distribuição geométrica de parâmetro n p = 1 − (1 − pi)· i=1\nAs X1, , Xn variáveis aleatórias independentes e identicamente distribuídas tem por função de probabilidade BN (1; p) se, e somente se, Nn = min(X1, , Xn) tem distribuição geométrica de parâmetro 1 − (1 − p)n.\nSejam \\(X_1, X_2, \\cdots , X_n\\) variáveis aleatórias independentes e igualmente distribuídas com função de densidade comum\n\nf (x) =\nσ 0, se x ≤ \nMostre que X(1), X(2) − X(1), X(3) − X(2), , X(n) − X(n−1) são independentes. 6. Sejam \\(X_1, X_2, \\cdots , X_n\\) variáveis aleatórias independentes e igualmente distribuídas com função de distribuição acumulada comum\nF (t) =\ntα, se 0 &lt; t &lt; 1  1, se t ≥ 1\npara α &gt; 0. Mostre que X(i)/X(n), i = 1, 2, , n − 1 e X(n) são independentes. 7. Sejam X1 e X2 duas variáveis aleatórias discretas independentes com função de probabilidade comum P (X = x) = (1 − )x−1, x = 1, 2, ; 0 &lt; &lt; 1· Mostre que X(1) e X(2) − X(1) são independentes. 8. Sejam X1, , Xn duas variáveis aleatórias independentes com função de densidade comum \\(F\\) . Encontre a função de densidade de X(1) e de X(n).\n\nSejam X(1), X(2), , X(n) as estatísticas de ordem de n variáveis aleatórias independentes e igualmente distribuídas \\(X_1, X_2, \\cdots , X_n\\) com função de densidade comum f (x) = 1 se 0 &lt; x &lt; 1 · 0, caso contrário Prove que Y1 = X(1)/X(2), Y2 = X(2)/X(3), , Yn−1 = X(n−1)/X(n) e Yn = X(n) são independentes. Encontre a função de densidade conjunta de Y1, Y2, , Yn.\nSejam X1.X2, , Xn variáveis aleatórias independentes identicamente distribuídas não negativas contínuas. Prove que se E|X| &lt; ∞, então E|X(r)| &lt; ∞. Definamos Mn = X(n) = max(\\(X_1, X_2, \\cdots , X_n\\)). Mostre que ∫ ∞\n\nEncontre E(Mn) em cada uma das seguintes situações: a) Xk tem como função de distribuição comum \\(F\\) (x) = 1 − e−xβ, se x ≥ 0. b) Xk tem como função de distribuição comum \\(F\\) (x) = x, se 0 &lt; x &lt; 1.\n\nProvar que, qualquer seja a amostra aleatória X1.X2, , Xn sempre cumpre-se que X(1) ≤ X ≤ X(n).\nDemonstrar o Teorema 2.5.\nDemonstrar o Teorema 2.9.\n\nExercícios da Seção 2.3 1. Demonstre o Corolário 2.17. 2. Demonstre o Corolário 2.18.\n\nSeja X1, , Xn uma amostra aleatória Poisson(). Encontre Var(S2) e compare-a com Var(X). Observe que E(X) = = E(S2).\nSeja \\(X_1, X_2, \\cdots , X_n\\) uma amostra aleatória da função de distribuição \\(F\\) e seja \\(F\\) ∗(x) a função de distribuição amostral. Encontre Cov[F ∗(x), \\(F\\) ∗(y)] para números reais fixos x, y. n n\nSeja \\(F\\) ∗ a função de distribuição empírica de uma amostra aleatória com função de distribuição teórica \\(F\\) . Prove que { ∗ ϵ } 1\nSejam \\(X_1, X_2, \\cdots , X_n\\) n observacões independentes da variável aleatória X. Encontre a distribuição amostral de X, a média amostral, se:\n\n\nX ∼ P ();\nX ∼ Cauchy(1, 0);\nX ∼ χ2(m).\n\n\nSeja X1, , Xn uma amostra aleatória Poisson(). Encontre Var(S2) e compare-a com Var(X). Observe que E(X) = = E(S2).\nDemonstre o Teorema 2.23. [Dica: para quaisquer reais µ e σ &gt; 0, encontre a função de densidade de (U(r) − µ)/σ e mostre que as variáveis padronizadas de U(r), (U(r) − µ)/σ, são assintoticamente N (0, 1) sob as condições do teorema.]\nProvar que o momentos amostral central b1 é sempre zero.",
    "crumbs": [
      "Módulo I",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estatísticas de ordem</span>"
    ]
  }
]