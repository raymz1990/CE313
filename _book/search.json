[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CE313 - Estatística Não-Paramétrica",
    "section": "",
    "text": "Apresentação\nCE313 - Estatística não-paramétrica Departamento de Estatística - UFPR Professor: Fernando Lucambio Pérez Primeiro Semestre 2024 (15/Fevereiro/2024 - 29/Junho/2024).",
    "crumbs": [
      "Apresentação"
    ]
  },
  {
    "objectID": "index.html#objetivos",
    "href": "index.html#objetivos",
    "title": "CE313 - Estatística Não-Paramétrica",
    "section": "Objetivos",
    "text": "Objetivos\nOs métodos de inferência estatística não paramétricos ou de distribuição livre, são procedimentos matemáticos para testes de hipóteses e modelos de regressão que, diferentemente da estatística paramétrica, não fazem suposições sobre a distribuição de probabilidade das variáveis a serem consideradas. Objetivo geral: Espera-se que, ao final da disciplina, o aluno deva saber identificar o uso de testes não-paramétricos e lidar de forma apropriada com problemas práticos.\nObjetivos específicos: Identificar situações nas quais procedimentos não-paramétricos podem ser aplicados, selecionar testes não-paramétrico adequados para um problema em estudo, construir as hipóteses correspondentes e aplicar os procedimentos escolhidos utilizando funções R para esta finalidade.",
    "crumbs": [
      "Apresentação"
    ]
  },
  {
    "objectID": "index.html#ementa",
    "href": "index.html#ementa",
    "title": "CE313 - Estatística Não-Paramétrica",
    "section": "Ementa",
    "text": "Ementa\nEstatística paramétrica e não paramétrica. Testes não paramétricos para uma, duas ou mais amostras. Estimação não paramétrica de densidades. Introdução aos modelos não paramétricos de regressão.\nLocal: Laboratório B. Horário: quarta-feira 19:00h, sexta-feira 20:45h. Nota: a nota final será a soma das notas obtidas nos quatro trabalhos assíncronos programados.",
    "crumbs": [
      "Apresentação"
    ]
  },
  {
    "objectID": "index.html#referências-bibliográficas-básicas",
    "href": "index.html#referências-bibliográficas-básicas",
    "title": "CE313 - Estatística Não-Paramétrica",
    "section": "Referências bibliográficas básicas",
    "text": "Referências bibliográficas básicas\n\nConover, W.J. (1999). Practical nonparametric statistics. 3rd. ed. New York: Chichester: John Wiley & Sons (Asia).\nGibbons, J.D. (1993). Nonparametric Statistics: An Introduction, Newbury Park: Sage Publications.\nSiegel, S. and Castellan, N.H. (2006). Estatística não-paramétrica para ciências do comportamento. RS: Artmed.\nLucambio, F. (2023). Estatística não paramétrica",
    "crumbs": [
      "Apresentação"
    ]
  },
  {
    "objectID": "index.html#referências-bibliográficas-complementares",
    "href": "index.html#referências-bibliográficas-complementares",
    "title": "CE313 - Estatística Não-Paramétrica",
    "section": "Referências bibliográficas complementares",
    "text": "Referências bibliográficas complementares\n\nHastie, T.; Tibshirani, R. and Friedman, J. (2013). The elements of statistical learning: Data mining, inference, and prediction. Springer New York.\nHollander, M. and Wolfe, D.A. (1999). Nonparametric statistical methods. 2nd. ed. New York: John Wiley & Sons.\nKloke, J. and McKean, J.W. (2015). Nonparametric statistical methods using R. Boca Raton: CRC Press.\nSilverman, B.W. (1994). Nonparametric Regression and Generalized Linear Models: A Roughness Penalty Approach, London: Editora Chapman & Hall.\nWasserman, L. (2006). All of nonparametric statistics: New York: Springer.",
    "crumbs": [
      "Apresentação"
    ]
  },
  {
    "objectID": "index.html#conteúdo-programático",
    "href": "index.html#conteúdo-programático",
    "title": "CE313 - Estatística Não-Paramétrica",
    "section": "Conteúdo Programático",
    "text": "Conteúdo Programático\nMÓDULO I: Estimação não paramétrica\n\nAula 01 - Estatística não paramétrica 1\nAula 02 - Estatísticas de ordem\nAula 03 -\nAula 04 -\nAula 05 -\nAula 06 -\n\n\nMÓDULO II: Problemas de amostra única\n\nAula 07 -\nAula 08 -\nAula 09 -\nAula 10 -\nAula 11 -\nAula 12 -\nAula 13 -\nAula 14 -\n\n\nMÓDULO III: Procedimentos em k amostras\n\nAula 15 -\nAula 16 -\nAula 17 -\nAula 18 -\nAula 19 -\nAula 20 -\nAula 21 -\nAula 22 -\n\n\nMÓDULO IV: Regressão não paramétrica\n\nAula 23 -\nAula 24 -\nAula 25 -\nAula 26 -\nAula 27 -\nAula 28 -\nAula 29 -\nAula 30 -\n\n\nMÓDULO V: Redes neurais\n\nAula 31 -\nAula 32 -\nAula 33 -\nAula 34 -\nAula 35 -\nAula 36 -",
    "crumbs": [
      "Apresentação"
    ]
  },
  {
    "objectID": "Modulo I/modulo-01.html",
    "href": "Modulo I/modulo-01.html",
    "title": "Módulo I",
    "section": "",
    "text": "Warning\n\n\n\nÚltima atualização: 03 de agosto de 2018.\n\n\nSejam \\(x_{1},  x_{2}, \\cdots  x_{n}\\), variáveis aleatórias independentes identicamente distribuídas com distribuição comum \\(L(X)\\) e seja \\(P\\) a classe de todas as possíveis distribuições de \\(X\\) que consiste de todas as distribuições absolutamente contínuas ou discretas.\n\nDefinição. A estatística \\(T(X)\\) é suficiente para a família de distribuições \\(P\\) se a distribuição condicional de \\(X|T=t\\) é a mesma, seja qual for a função de distribuição \\(F∈P\\).\n\nExemplo. Sejam \\(x_{1},  x_{2}, \\cdots  x_{n}\\) variáveis aleatórias independentes igualmente distribuídas com distribuição absolutamente contínua e seja \\(T = (X_{(1)}, \\cdots, X_{(n)})\\) a estatística de ordem. Então: \\[f(x|T = t) = 1n!,\\] e vemos que \\(T\\) é uma estatística suficiente para a família das distribuições absolutamente contínuas.\nDefinição. A família de distribuições P é completa se somente a função zero for o estimador não viesado de 0, isto é, EF(h(X))=0, para todo F∈P implica que h(X)=0 . Isto para todo x exceto para um conjunto nulo em relação a cada F∈P .\nExemplo. Seja X∼Uniforme(0,θ) , onde θ∈(0,∞) . Mostraremos que esta família de distribuições é completa. Precisamos mostrar que Eθ(g(X))=∫θ0g(x)1θdx=0,∀θ&gt;0, se, e somente se, g(x)=0 , para todo x . Em geral, esse resultado segue a teoria da integração. Se g for contíua, diferenciamos ambos os lados de ∫θ0g(x)dx=0, para obter g(θ)=0 , para todo θ&gt;0 . Agora, seja X1,X2,⋯,Xn uma amostra aleatória da distribuição Uniforme(0,θ) . Também, seja X(n)=max(X1,X2,⋯,Xn) . Então, a função de densidade de X(n) é dada por fX(n)(x|θ)=nθ−nxn−1,0&lt;x&lt;θ, zero caso contrário. Vemos por um argumento semelhante que X(n) é completa, o que é o mesmo do que dizer que {fX(n)(x|θ);θ&gt;0} é uma família de densidades completa. Claramente X(n) é suficiente.\nDefinição. Uma estatística T(X) é dita ser completa em relação a uma classe de distribuições P se a classe de distribuições induzidas de T for completa.\nClaro que todos os exemplos já encontrados de estatísticas completas ou famílias completas de distribuições para o caso paramétrico podem ser aplicados nesta situação.\nTeorema. A estatística de ordem (X(1),⋯,X(n)) é uma estatística suficiente e completa desde que a amostra X1,X2,⋯,Xn seja composta de variáveis aleatórias independentes identicamente distribuídas do tipo discreta ou contínua. Demonstração Ver Fraser (1965).▉\nDefinição. Diz-se que uma função real g(F) é estimável se tiver um estimador não viciado, isto é, se existe uma estatística T(X) tal que EF(T(X))=g(F), para todo F∈P .\nExemplo. Se P é a classe de todas as distribuições para as quais o segundo momento existe, X é um estimador não viciado de μ(F) , a média da população. Similarmente μ2(F)=VarF(X), é também estimável e um estimador não viciado é S2=1n−1∑i=1n(Xi−X¯¯¯¯)2⋅ Da mesma forma, X¯¯¯¯−Y¯¯¯¯ é um estimador não viciado de E(X)−E(Y) , 1n{número de X&gt;c} é um estimador não viciado de PF(X&gt;c) e assim por diante.\nDefinição. O grau m , m≥1 , de um parâmetro estimável g(F) é o menor tamanho de amostra para o qual o parâmetro é estimável, ou seja, é o menor n para o qual existe um estimador não viciado T(X1,⋯,Xn) com EF(T(X))=g(F), para todo F∈P .\nExemplo. O parâmetro g(F)=PF(X&gt;c), onde c é uma constante conhecida, têm grau 1. Também μ(F) é estimável com grau 1, para isto assumimos que existe ao menos um F∈P tal que μ(F)≠0 . Acontece que μ2(F) é estimável com grau 2, desde que μ2(F) não seja estimável de forma não viciada por somente uma observação. Ao menos duas observações são necessárias. De maneira similar, μ2(F) têm grau 2.\nDefinição. Um estimador não viciado de algum parâmetro baseado no tamanho mínimo de amostra, ou seja, com amostra iagual ao grau m é chamado de kernel.\nExemplo. Seja X1,⋯,Xn uma amostra aleatória com distribuição F . Então Xi é o kernel de μ(F) ; XiXj , i≠j , é o kernel de μ2(F) e cada T(Xi,Xj)=X2i−XiXj,i=1,⋯,n,i≠j, é kernel de μ2(F) .\nTeorema. Existe um kernel simétrico para cada parâmetro estimável. Demonstração Seja T(X1,⋯,Xm) um kernel para g(F) . Também é Ts(X1,⋯,Xm)=1m!∑PT(Xi1,Xi2⋯,Xim) um kernel para g(F) , onde a soma P acontece sobre todas as m! permutacoes de {1,2,⋯,m} .▉\nExemplo. Seja X1,⋯,Xn uma amostra aleatória com distribuição F . Um kernel simétrico para μ2(F) é Ts(Xi,Xj)=12(T(Xi,Xj)+T(Xj,Xi))=12(Xi−Xj)2,i=1,⋯,n,i≠j⋅\nDefinição. Seja g(F) um parâmetro estimável de grau m e X1,X2,⋯,Xn uma amostra aleatória de F de tamanho n , n≥m . Correspondendo a qualquer kernel T(X1,⋯,Xn) de g(F) , definimos a U-estatística para a amostra como U(X1,X2,⋯,Xn)=1(nm)∑CTs(Xi1,Xi2,⋯,Xin), onde o índice da soma C percorre todas as (nm) permutações de m inteiros (i1,i2,⋯,im) escolhidos de {1,2,⋯,n} e Ts é um kernel simétrico, como definido na demonstração do Teorema anterior.\nClaramente, a U-estatística definida é simétrica nos X e EF(U(X))=g(F), para todo F .\nExemplo. Seja X1,⋯,Xn uma amostra aleatória com distribuição F . Para estimarmos μ(F) a U-estatística é dada por U(X1,X2,⋯,Xn)=1n∑i=1nXi⋅ Para estimarmos μ2(F) , um kernel simétrico é Ts(Xi1,Xi2)=12(Xi1−Xi2)2, para i1=1,2,⋯,n , i1≠i2 . A correspondente U-estatística é U(X)=1(n2)∑i1&lt;i212(Xi1−Xi2)2=1n−1∑i=1n(Xi−X¯¯¯¯)2=S2⋅\nDe maneira similar, para estimarmos μ2(F) , o kernel simétrico é Ts(Xi1,Xi2)=Xi1Xi2 e a correspondente U-estatística é U(X)=1(n2)∑i&lt;jXiXj=1n(n−1)∑i≠jXiXj⋅ Para estimarmos μ3(F) um kernel simétrico seria Ts(Xi1,Xi2,Xi3)=Xi1Xi2Xi3 , sendo que a U-estatística é U(X)=1(n3)∑i&lt;j&lt;kXiXjXk=1n(n−1)(n−2)∑i≠j≠kXiXjXk⋅ O seguinte resultado mostra a importância da U-estatística.\nTeorema. Seja P a classe de todas as distribuições absolutamente contínuas ou discretas. Qualquer função estimável g(F) , F∈P , tem um estimador único que é não viciado, simétrico nas observações e uniformemente de variância mínima entre todos os estimadores não viciados. Demonstração Seja X1,⋯,Xn uma amostra aleatória de F , F∈P e seja T(X1,⋯,Xn) um estimador não viciado de g(F) . Considere o conjunto de todas as n! permutações de {1,2,⋯,n} e indexá-los adequadamente. Seja {i1,i2,⋯,in} o i-ésimo deste conjunto e seja Ti=Ti(X1,X2,⋯,Xn)=T(Xi1,Xi2⋯,Xin),i=1,2,⋯,n!⋅ Seja T¯¯¯¯=1n!∑i=1n!Ti⋅ Claro que E(T)=g(F) e Var(T¯¯¯¯)=≤E(1n!∑n!i=1Ti)2−(g(F))2E(1(n!)2∑n!i=1T2i)−(g(F))2=E(T2)−(g(F))2=Var(T)⋅ A igualdade se mantém se, e somente se, Ti(X1,X2,⋯,Xn)=αn!,i=1,2,⋯,n!, para todos os pontos no espaço amostral, exceto talvez para um conjunto nulo, em que α é uma constante. Segue-se que T(X) é simétrico nos argumentos X1,X2,⋯,Xn com probabilidade 1 e T¯¯¯¯ é idêntico a T . A exclusividade é deixada como exercício.▉\nTeorema. Seja T(X1,⋯,Xn) um estimador não viciado para g(F) , F∈P . A correspondente U-estatística é essencialmente o único estimador não viciado uniformemente de mínima variância. Demonstração Consequência do teorema anterior.▉\nDe acordo com os teoremas acima precisamos apenas considerar estimadores que sejam simétricos nas observações e tudo o que devemos fazer é torná-las não viciados. Este procedimento leva a um estimador não viciado com a menor variância na classe de todos os estimadores não viciados do parâmetro. Por exemplo, como consequência destes teoremas, X¯¯¯¯ e S2 são os únicos estimadores não viciados uniformemente de variância mínima de μ(F) e μ2(F) , respectivamente.\nExemplo. Seja P a classe de todas as distribuições absolutamente contínuas e X1,X2,⋯,Xn uma amostra aleatória de tamanho n . Para estimarmos g(F)=PF(X1&gt;c), onde c é uma constante fixa, definimos Yi={1,0,Xi&gt;c,Xi≤ci=1,2,⋯,n⋅ Considere agora T(Y1,Y2,⋯,Yn)=∑i=1nαiYi, como um estimador de g(F) . Para encontrar o estimador não viciado de mínima variância de g simetrizamos T nos Y1,Y2,⋯,Yn . Isso acontece se αi=α , i=1,2,⋯,n e T(Y)=α∑ni=1Yi . Para T ser não viciado, temos que EF(T)=α∑i=1nEF(Yi)=αng(F), de maneira que α=1n . Portanto, 1n∑ni=1Yi é o estimador não viciado de mínima variância; também VarF(T)=g(F)(1−g(F))n≤14n⋅ Além disso, Yi têm distribuição Bernoulli , de modo a 1n(T−g(F))(g(F)(1−g(F)))12⟶DZ,n→∞, onde Z∼N(0,1) . Este resultado pode ser usado para encontrar limites de confiança em g(F) .\nSeja P a classe de todas as distribuições absolutamente contínuas na reta real. Sejam F,G∈P e definamos a função distância Δ(F,G) como segue: Δ(F,G)=∫∞−∞(F(x)−G(x))2F′(x)+G′(x)2dx⋅ Esta função satisfaz as seguintes propriedades:\nΔ(F,G)=0 se, e somente se, F=G . Δ(F,G)=Δ(G,F) . Δ(F,G)&gt;0 . Por outro lado, vamos supor que F(x)≠G(x) para algum x1 , onde F(x1)−G(x1)=d&gt;0 . Dado que F e G são distribuições absolutamente contínuas, existe um x0&lt;x1 , tal que F(x0)−G(x0)=d2eF(x)−G(x)≥d2, para x0≤x≤x1 . Dado que tanto F quanto G são ambas não decrescentes, pelo menos um dos F e G deve aumentar pelo menos d/2 quando x varia de x0 a x1 . Então Δ(F,G)≥∫x1x0(F(x)−G(x))2F′(x)+G′(x)2dx≥(d2)2d/22&gt;0⋅ Exemplo. Encontremos um estimador não viciado de mínima variância para Δ(F,G) . Sejam X1,X2,⋯,Xm uma amostra aleatória de F e Y1,Y2,⋯,Yn uma amostra aleatória de G , independentes. Consideramos que F,G∈P . Primeiro mostramos que g(F,G)=P({max(X1,X2)&lt;min(Y1,Y2)}⋃{max(Y1,Y2)&lt;min(X1,X2)})=13+2Δ(F,G)⋅ Temos que g(F,G)=P({max(X1,X2)&lt;min(Y1,Y2)})+P({max(Y1,Y2)&lt;min(X1,X2)}) e P(max(X1,X2)≤x)=F2(x),P(min(Y1,Y2)≥y)=[1−G(y)]2⋅ Então g(F,G)======∫∞−∞[1−G(y)]22F(y)F′(y)dy+∫∞−∞[1−F(x)]22G(x)G′(x)dx∫∞−∞[1+G2(y)−2G(y)]2F(y)F′(y)dy+∫∞−∞[1+F2(x)−2F(x)]2G(x)G′(x)dx2+∫∞−∞2[G2(x)F(x)F′(x)+F2(x)G(x)G′(x)−2F(x)G(x)(F′(x)+G′(x))]dx3−2∫∞−∞((F(x)+G(x))2−(F(x)−G(x))2)(F′(x)+G′(x)2)dx3−8∫∞−∞(F(x)+G(x)2)2(F′(x)+G′(x)2)dx+2Δ(F,G)3−83+2Δ(F,G)=g(F,G)⋅ Para utilizarmos os teoremas acima, vamos definir φ(X1,X2,Y1,Y2)={1,0,se max(X1,X2)&lt;min(Y1,Y2) ou se max(Y1,Y2)&lt;min(X1,X2)caso contrário Então φ(X1,X2,Y1,Y2) é um estimador não viciado de g(F,G) e de fato é um kernel de g(F,G) . A U-estatística correspondente, portanto, deve ser o estimador não viciado de mínima variância. Nós temos U(X,Y)=1(m2)(n2)∑i1&lt;i2∑k1&lt;k2φ(Xi1,Xi2,Yk1,Yk2), de maneira que U é o estimador não viciado de mínima variância de g(F,G) , assim como o estimador não viciado de mínima variância de Δ(F,G) é Δˆ(F,G)=12U(X,Y)−16⋅\nExemplo. Seja P a classe de todas as funções de distribuição absolutamente contínuas na reta real e X1,X2,⋯,Xm e Y1,Y2,⋯,Yn duas amostras aleatórias independentes de F e G , respectivamente, com F,G∈P . Queremos estimar ρ(F,G)=P(X&lt;Y)⋅ Com esse objetivo, vamos definir Zij={1,0,Xi&lt;YjXi≥Yj para cada par Xi,Yj , i=1,2,⋯,m e j=1,2,⋯,n . Então ∑i=1mZij é o número de vezes que X&lt;Yj e ∑j=1nZij é o número de vezes que Xi&lt;Y . Mann and Whitney (1947) sugeriram utilizar o estimador U/mn , onde U=∑i=1m∑j=1nZij e E(U)=mnE(Zij)=mnP(X&lt;Y)⋅ Então ρˆ(F,G)=Umn, é não viciado para ρ . Além disso, ρˆ é simétrico em X e Y , de modo que tem uma variância mínima. Para calcular a variância mínima, temos E(U2)=∑i∑j∑h∑kE(ZijZhk), onde ZijZhk={1,0,se Xi&lt;Yj e Xh&lt;Ykcaso contrário, de modo a E(ZijZhk)=P(Xi&lt;Yj,Xh&lt;Yk)=⎧⎩⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪∫F(x)G′(x)dx,∫(1−G(x))2F′(x)dx,∫F2(x)G′(x)dx,(∫F(x)G′(x)dx)2, caso i=h,j=k caso i=h,j≠k caso i≠h,j=k caso i≠h,j≠k⋅ Há mn termos com i=h,j=k ; m(m−1)n termos com i≠h,j=k ; mn(n−1) termos com i=h,j≠k e m(m−1)n(n−1) termos com i≠h,j≠k . Segue que E(U2)=mn∫F(x)G′(x)dx+mn(n−1)∫(1−G(x))2F′(x)dx+m(m−1)n∫F2(x)G′(x)dx+m(m−1)n(n−1)(∫F(x)G′(x)dx)2, que leva à variância de U . Em particular, se F=G , então Var(U)=mn(m+n+1)12⋅\nI.1 Estimação de densidades\nDe certa forma, problemas de estimação não-paramétrica são extensões de problemas de estimação paramétrica, mas a natureza do primeiro é bem diferente do último. Considere, por exemplo, a situação de observações independentes identicamente distribuídas, digamos X1,X2,⋯,Xn . Em um problema paramétrico, assumimos que a distribuição de Xi é F(⋅;θ) , a qual é totalmente especificada até o vetor de parâmetros θ ; então o problema é essencialmente a estimaçã de θ . Em um problema não-paramétrico, a distribuição é totalmente desconhecida com, talvez, algumas restriçõs em propriedades gerais e, portanto, é denotada por F .\nAqui consideramos estimadores de F em termos de função de densidade f . A função de densidade tem a vantagem de fornecer uma representação visualmente mais informativa da distribuição subjacente. Por exemplo, o histograma geralmente dá uma ideia aproximada da forma da distribuição. Este último ficou como o único estimador de densidade não paramétrico até 1950. Por essa razão, nossa discussão começará com os histogramas.\nEmbora o histograma seja usado extensivamente, não é tão frequente que seja necessária uma definição matemática. Uma maneira de defini-lo é através da função de densidade empírica.\nDefinição. Seja f a derivada de F ; por isso pode-se expressar como f(x)=limh→0F(x+h)−F(x−h)2h⋅ Então, dizemos que fˆ , definido por fˆ(x)=Fˆ(x+h)−Fˆ(x−h)2h, é o histograma, sendo que Fˆ é a função de distribuição empírica.\nO parâmetro h é chamado de largura de banda. Podemos escrever fˆ , definido acima como, fˆ(x)=12nh∑i=1n11(x−h;x+h)(Xi)⋅ Podemos excrever a função de densidade como f(x)=limh→01h(F(x+h)−F(x−h)) , mas não se pode definir daqui o histograma porque, então, esse limite é zero ou infinito e assim em algum momento é preciso parar, em outras palavras, não se pode chegar muito perto de zero.\nTeorema. Seja f a função de densidade da função de distribuição F . Então, com probabilidade 1, fˆ(x)∼Binomial(n,p), com p=F(x+h)−F(x−h) . Assim, o comportamento assintótico do histograma pode ser derivado da distribuição binomial como E(fˆ(x))=F(x+h)−F(x−h)2h e Var(fˆ(x))=p(1−p)4nh2⋅ Demonstração. Exercício.▉\nDeste teorema segue que fˆ(x) é um estimador consistente pontual de f(x) quando h→0 e nh→∞ . A seguir, o processo de limite é entendido como h=hn , de maneira que hn→0 e nhn→∞ . Estas condições podem ser interpretadas como se fosse necessário hn ir a zero, mas não muito rápido. Isso é exatamente o que temos especulado, exceto que agora temos a taxa exata de convergência, que pode ser escrita como hn=o(n) .\nExemplo. Utilaremos dados simulados da distribuíão N(0,1) , com isso mostramos o histograma destes 50 dados utilizando duas formas diferentes de encontrarmos uma expressão para hn , a chamada largura de banda.\n\n\nCode\nset.seed(1340)\nx = rnorm(50)\npar(mar=c(4,2,1,1))\nhist(x, main = NULL, freq = FALSE, breaks = \"Sturges\")\nbox()\n\n\n\n\n\n\n\n\n\nCode\npar(mar=c(4,2,1,1))\nhist(x, main = NULL, freq = FALSE, breaks = \"Scott\")\nbox()\n\n\n\n\n\n\n\n\n\nNeste exemplo utilizamos duas formas de escolher a largura de banda hn dentre três diferentes possibilidades programadas na função hist. Por padrão escolhe-se breaks = “Sturges”, porposto por Sturges (1929), o qual sugere que hn=max(X1,⋯,Xn)−min(X1,⋯,Xn)1+3.322ln(n)⋅ A segunda situação indica que o qual significa que se os dados provêm da distribuição Normal temos que hn=3.49sn−1/3 sendo s o desvio padrão estimado. Esta proposta deve-se à Scott (1979).\nEmbora o histograma é um estimador consistente quando hn→0 e nhn→∞ , verifica-se que se pode fazer melhor. A melhoria também é motivada por uma preocupação prática: o histograma não é uma função suave, uma propriedade que se pode esperar que qualquer função de densidade real tenha.\nDefinição. O estimador kernel da função de densidade é dado por fˆ(x)=1nhn∑i=1nK(x−Xihn), onde K(⋅) é uma função conhecida como kernel.\nÉ tipicamente assumido que K seja não-negativa, simétrica em torno de zero e satisfaz ∫K(u)du=1 . Claro que o histograma é um caso especial do estimador do kernel se K for escolhido como a função de densidade da distribuição Uniforme(−1,1) . O último não é uma função suave e é por isso que o histograma não é suave; mas escolhendo K como uma função suave, tem-se um estimador de f que seja suave.\nPor exemplo, escolhendo a função de densidade N(0,1) , temos por resultado o conhecido como kernel Gaussiano e assim também utilizando a densidade de densidade Beta simétrica, dada por K(u)=Γ(ν+3/2)Γ(1/2)Γ(ν+1)(1−u2)ν,−1&lt;u&lt;1, e K(u)=0 caso contrário. Os casos especiais ν=0,1,2,3 correspondem às funções kernel uniforme, Epanechnikov, biweight e triweight, respectivamente.\n\nDiferentes kernel em RLarguras de banda equivalentes\n\n\n\n\nCode\nkernels = eval(formals(density.default)$kernel)\nplot (density(0, bw = 1), xlab = \"\", main = \"Diferentes kernel em R\")\nfor(i in 2:length(kernels)) lines(density(0, bw = 1, kernel =  kernels[i]), col = i)\nlegend(1.5,.4, legend = kernels, col = seq(kernels), lty = 1, cex = .8, y.intersp = 1)\n\n\n\n\n\n\n\n\n\nCode\nh.f = sapply(kernels, function(k) density(kernel = k, give.Rkern = TRUE))\nh.f = (h.f[\"gaussian\"] / h.f)^ .2\nh.f\n\n\n    gaussian epanechnikov  rectangular   triangular     biweight       cosine \n   1.0000000    1.0100567    0.9953989    1.0071923    1.0088217    1.0079575 \n   optcosine \n   1.0099458 \n\n\n\n\n\n\nCode\nbw = bw.SJ(x) ## escolha automática\nplot(density(x, bw = bw), main = \"Larguras de banda equivalentes\")\nfor(i in 2:length(kernels)) lines(density(x, bw = bw, adjust = h.f[i], \n                                                           kernel = kernels[i]), col = i)\nlegend(55, 0.035, legend = kernels, col = seq(kernels), lty = 1)\n\n\n\n\n\n\n\n\n\n\n\n\nUm problema prático importante na estimação de densidades via kernel é como escolher a largura de banda hn . Note que dadas condições como hn→0 e nhn→∞ , ainda existem muitas opções para hn . Então, de certo modo, a ordem de convergência ou divergência não resolve o problema. Uma solução para esse problema é conhecida como compensação de viés-variância. Antes de entrarmos nos detalhes, vamos primeiro apressentar um resultado em relação ao viés assintótico do estimador kernel. Aqui, o viés é definido como viés(fˆ(x))=E(fˆ(x))−f(x), para um dado x .\nTeorema. Suponhamos que f seja contínua e limitada. Então o viés do estimador kernel de densidade converge a zero quando hn→0 , para todo x . Demonstração. Observemos que E(fˆ(x))==1n∑ni=11hn∫K(x−yhn)f(y)dy∫K(u)f(x−hnu)du=f(x)+∫K(u)(f(x−hnu)−f(x))du⋅ Utilizando então o teorema da convergência dominada completa-se a demonstração.▉\nTeorema. Suponhamos que f seja contínua três vezes diferenciável, com terceira derivada limitada na vizinhança de x e K satisfazendo ∫K2(u)du&lt;∞e∫|u|3K(u)du&lt;∞⋅ Se hn→0 quando n→∞ , temos que viés(fˆ(x))=h2n2f′′(x)∫u2K(u)du+o(h2n)⋅ Se, além disso nhn→∞ quando n→∞ , então temos Var(fˆ(x))=f(x)nhn∫K2(u)du+o((nhn)−1)⋅ Demonstração. A demonstração é baseada na expansão de Taylor, f(x−hnu)=f(x)−hnuf′(x)+h2nu22f′′(x)−h3nu36f′′′(ϵ), sendo ϵ fica entre x−hnu e x . Os detalhes são deixados como um exercício.▉\nUma medida de precisão do estimador é o erro quadrático médio (EQM), dado por EQM(fˆ(x))=E(fˆ(x)−f(x))2⋅ É fácil mostrar que o EQM combina o viés e a variância de tal maneira que EQM(fˆ(x))=viés(fˆ(x))2+Var(fˆ(x))⋅ Vemos que, sob as condiçotilde;es hn→0 e nhn→∞ e se ignorarmos os termos de baixa ordem, temos EQM(fˆ(x))≈h4n4(f′′(x))2τ4+f(x)nhnγ2, onde τ2=∫u2K(u)du e γ2=∫K2(u)du . O termo à direita da expressão acima é minimizada quando hn=(γ2f(x)τ4(f′′(x))2)15n−15⋅ Note ainda que a expressão acima não é a solução ideal, isso porque f é desconhecida na prática. No entanto, dá-nos pelo menos alguma ideia sobre a taxa ideal de convergência a zero, sendo esta hn=O(n−15) .\nQuando f é desconhecida, uma abordagem natural seria substituí-lo por um estimador e, assim, obter uma largura de banda ideal estimada. Uma complicação é que a largura de banda ideal depende de x mas, idealmente, gostaríamos de usar uma largura de banda que funcionasse para diferentes x dentro de um certo intervalo, se não todos os x . Para obter uma largura de banda ideal que não depende de x , integramos os dois lados da expressão de EQM em relação a x . Isto nos leva a ∫EQM(fˆ(x))dx≈τ4h4n4∫(f′′(x))2dx+γ2nhn∫f(x)dx=τ4θ2h4n4+γ2nhn, com θ2=∫(f′′(x))2dx . Pelo mesmo argumento, o lado direito acima é minimizado quando hn=(γ2τ4θ2)15n−15⋅ Desta vez, o hn ideal não depende de x . Além disso, a integral do EQM ou o IEQM mínimo é dado por IEQM=∫EQM(fˆ(x))dx=54(τγ2)45θ25n−45⋅ Uma implicação é a seguinte. Note que o IEQM depende do kernel K através de cK=(τγ2)45 . Mostrou-se que para os kernels comumente usados, tais como aqueles listados, o desempenho dos estimadores de kernel correspondentes é quase o mesmo em termos dos valores de cK . Voltando ao problema sobre a estimação da largura de banda ideal, vemos que tudo o que precisamos é encontrar um estimador consistente de θ2 . Se f é a função de densidade da distribuição normal com desvio padrão σ , então pode ser mostrado que θ2=3/8π−−√σ5 . Naturalmente, se alguém souber que f é normal, então a estimação da densidade não-paramétrica não seria necessária, porque um método paramétrico provavelmente seria melhor. Em geral, pode-se expandir f em torno da densidade gaussiana usando a expansão de Edgeworth.\nUtilizando a abordagem acima, Hjort and Jones (1996) obteveram o seguinte estimador ótimo para a largura de banda hˆn=hˆ0(1+3548γˆ4+3532γˆ23+3851024γˆ24)−15, onde hˆ0 é a estimativa ideal da largura de banda assumindo que f é normal, isto é, com θ2 substituído por 3/8π−−√σ5 ou mais explicitamente hˆ0=1.06(σˆn15), chamamos hˆ0 a largura de banda da linha de base e σˆ2 é a variância amostral dada por σˆ2=1n−1∑i=1n(Xi−X¯¯¯¯)2⋅ Além disso, γˆ3 e γˆ4 são os estimadores dos coefcientes de assimetria de amostra e curtose, dado por γˆ3=1(n−1)σˆ3∑i=1n(Xi−X¯¯¯¯)3 e γˆ4=1(n−1)σˆ4∑i=1n(Xi−X¯¯¯¯)4−3, respectivamente. Houve outras abordagens para a seleção da largura de banda ótima, incluindo o método de validação cruzada. Ambos procedimentos foram programados na função density.\nExemplo. Utilaremos os dados simulados da distribuíão N(0,1) no exemplo anterior para com isso mostrarmos o histograma e o estimador Kernel da função de densidade.\n\nset.seed(1340) x = rnorm(50) par(mar=c(4,2,1,1)) hist(x, main = NULL, freq = FALSE, breaks = “Sturges”) box() lines(density(x, bw = “nrd0”), col = “red”) par(mar=c(4,2,1,1)) hist(x, main = NULL, freq = FALSE, breaks = “Scott”) box() lines(density(x, bw = “bcv”), col = “red”)\n\nI.2 Exercícios Seja T(X1,⋯,Xn) uma estatística simétrica nas observaçóes. Mostre que T pode ser escrita como função das estatísticas de ordem. Por outro lado, se T(X1,⋯,Xn) pode ser escrita como função das estatísticas de ordem, T é simétrica nas observações.\nSejam X1,X2,⋯,Xm e Y1,Y2,⋯,Yn amostras independentes de duas distribuições absolutamente contínuas. Encontre o estimador não viciado de mínima variância de: (a) E(XY)\n\nVar(X+Y)\n\nSeja (X1,Y1),(X2,Y2),⋯,(Xn,Yn) uma amostra aleatória com distribuição absolutamente contínua bivariada. Encontre o estimador não viciado de mínima variância de: (a) E(XY)\n\nVar(X+Y)\n\nCosidere (R,B,Pθ) um espaço de probabilidade e P={Pθ:θ∈Θ} . Seja A um elemento da σ -álgebra de Borel e considere d(θ)=Pθ(A) . (a) A função d é estimável? Se sim, qual é o grau? (b) Encontre o estimador não viciado de mínima variância de d , baseado em uma amostra de tamanho n e assumindo que P seja a classe de todas as distribuições contínuas.",
    "crumbs": [
      "Módulo I"
    ]
  },
  {
    "objectID": "Modulo I/Aula 01/aula01.html",
    "href": "Modulo I/Aula 01/aula01.html",
    "title": "1  Estatística Não Paramétrica",
    "section": "",
    "text": "1.1 Introdução\nEm todos os problemas de inferência estatística considerados, assumimos que a distribuição da variável aleatória amostrada seja conhecida a menos, talvez, para alguns parâmetros. Na prática, entretanto, a forma funcional da distribuição é raramente ou nunca conhecida. Por conseguinte, é desejável conceber alguns procedimentos que estejam livres desta hipótese relativa à distribuição.\nPara entender a ideia de estatística não-paramétrica, primeiro requeremos uma compreensão de conceitos da estatística básica paramétrica. Conceitos elementares introduzem o conceito de teste de significância estatística com base na distribuição amostral de uma estatística particular. Em resumo, se tivermos um conhecimento básico da distribuição subjacente de uma variável, poderemos fazer previsões sobre como, em amostras repetidas de tamanho igual, essa estatística específica se comportará, isto é, como será distribuída.\nEstudamos aqui alguns procedimentos que são comumente referidos como métodos sem distribuição ou não paramétricos. O termo livre de distribuição refere-se ao fato de que nenhuma suposição é feita sobre a distribuição subjacente, exceto que a função de distribuição sendo amostrada seja absolutamente contínua ou puramente discreta. O termo não paramétrico refere-se ao fato de não haver parâmetros envolvidos no sentido tradicional do termo parâmetro utilizado até o momento.\nGrosseiramente falando, um procedimento não-paramétrico é um procedimento estatístico que possui certas propriedades desejáveis que mantêm suposições relativamente leves em relação às populações subjacentes das quais os dados são obtidos.\nNos dois exemplos seguintes mostramos distribuições conhecidas que são livres de parâmetros.\nCode\n# Definindo os valores para o eixo x1 e x2\nx1 &lt;- seq(-6, 6, length.out = 1000)\nx2 &lt;- seq(0, 10, length.out = 1000)\n\n# Graus de liberdade para a distribuição t-Student e qui-quadrado\ndf_t &lt;- 3\ndf_chi &lt;- 3\n\n# Calculando os valores de densidade das distribuições\ndensidade_t &lt;- dt(x1, df = df_t)\ndensidade_chi &lt;- dchisq(x2, df = df_chi)\n\n# Configurando o layout da plotagem\npar(mfrow = c(1, 2))\n\n# Plotando a distribuição t-Student\nplot(x1, densidade_t, type = \"l\", lwd = 2, col = \"blue\", \n     main = \"Distribuição t-Student (3)\",\n     xlab = \"x\", ylab = \"Densidade\")\n\n# Plotando a distribuição qui-quadrado\nplot(x2, densidade_chi, type = \"l\", lwd = 2, col = \"green\", \n     main = \"Distribuição Qui-Quadrado (3)\",\n     xlab = \"x\", ylab = \"Densidade\")\nO desenvolvimento repetido e contínuo de procedimentos estatísticos não paramétricos nas últimas décadas deve-se às seguintes vantagens de técnicas não paramétricas:\nMas têm desvantagens:",
    "crumbs": [
      "Módulo I",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Estatística Não Paramétrica</span>"
    ]
  },
  {
    "objectID": "Modulo I/Aula 01/aula01.html#introdução",
    "href": "Modulo I/Aula 01/aula01.html#introdução",
    "title": "1  Estatística Não Paramétrica",
    "section": "",
    "text": "Métodos não-paramétricos exigem poucas suposições sobre as populações subjacentes das quais os dados são obtidos. Em particular, os procedimentos não paramétricos abandonam a suposição tradicional de que as populações subjacentes sejam normais.\nOs procedimentos não paramétricos permitem que o usuário obtenha p-valores exatos para testes, probabilidades de cobertura exatas para intervalos de confiança, taxas exatas de erros experimentais para procedimentos de comparação múltipla e probabilidades exatas de cobertura para faixas de confiança sem confiar nas suposições de que as populações subjacentes sejam normais.\nAs técnicas não paramétricas são frequentemente, embora nem sempre, mais fáceis de aplicar do que as suas contrapartes teóricas normais.\nOs procedimentos não paramétricos são geralmente muito fáceis de entender.\nEmbora, à primeira vista, a maioria dos procedimentos não- paramétricos pareça sacrificar muito as informações básicas nas amostras, investigações de eficiência teórica mostraram que esse não é o caso. Normalmente, os procedimentos não-paramétricos são apenas ligeiramente menos eficientes do que os seus concorrentes de teoria normal quando as populações subjacentes são normais e podem ser moderadamente ou muito mais eficientes que os concorrentes quando as populações subjacentes não são normais.\n\n\n\nMétodos não paramétricos são relativamente insensíveis a observações distantes.\nOs procedimentos não paramétricos são aplicáveis em muitas situações em que os procedimentos teóricos normais não podem ser utilizados. Muitos procedimentos não-paramétricos exigem apenas as classificações das observações em vez da magnitude real das observações, enquanto os procedimentos paramétricos exigem as magnitudes.\nNem todos os procedimentos desenvolvidos na estatística paramétrica podem ser aplicados à estatística não-paramétrica.",
    "crumbs": [
      "Módulo I",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Estatística Não Paramétrica</span>"
    ]
  },
  {
    "objectID": "Modulo I/Aula 02/aula02.html",
    "href": "Modulo I/Aula 02/aula02.html",
    "title": "2  Estatísticas de ordem",
    "section": "",
    "text": "2.1 2. Momentos amostrais\nQuando estudadas as idéias e técnicas da teoria das probabilidades fundamentais, criamos um modelo matemático de um ensaio aleatório, associando-o com um espaço de amostragem no qual os acontecimentos aleatórios correspondem a um conjunto de uma certa \\(\\sigma\\)-álgebra. A noção de probabilidade definida nesta \\(\\sigma\\)-álgebra corresponde à noção de incerteza no resultado em qualquer realização do experimento aleatório.\nVamos começar o estudo de alguns problemas de estatística matemática. Suponha que buscamos informações sobre algumas características numéricas de um conjunto de elementos, chamado de uma população. Por razões de custo, de tempo ou simplesmente para não destruir todos os elementos amostrados podemos não querer ou não poder estudar cada elemento da população. Nosso objetivo é tirar conclusões sobre as características desconhecidas da população com base em informações sobre algumas características de uma amostra adequadamente selecionada.\nFormalmente, seja \\(X\\) uma variável aleatória que descreve a população sob investigação e seja \\(F\\) a função de distribuição de \\(X\\). Há duas possibilidades: ou \\(X\\) tem função de distribuição \\(F(\\cdot ; \\theta)\\) com uma forma funcional conhecida exceto, talvez, para o parâmetro \\(\\theta\\), o qual pode ser um vetor ou \\(X\\) tem uma função de distribuição \\(F\\) sobre a qual não sabemos nada, exceto talvez que \\(F\\) seja, digamos, absolutamente contínua. No primeiro caso, seja \\(\\Theta\\) o conjunto dos possíveis valores do parâmetro desconhecido \\(\\theta\\). Seguidamente, o trabalho de um estatístico é decidir com base em uma amostra selecionada adequadamente que membro ou membros da família \\({F (\\cdot ; \\theta) : \\theta \\ \\Theta}\\) pode representar a função de distribuição de \\(X\\). Problemas desse tipo são chamados de problemas de inferência estatística paramétrica e o espaço estatístico é dado por \\((\\mathbb{R}, {F (·; \\theta) : \\theta \\ \\Theta}, \\beta (\\mathbb{R}))\\), sendo \\(\\mathbb{R}\\) o conjunto dos reais na reta, {F (·; ) : } a família de distribuições de X e B(R) a \\(\\sigma\\)-álgebra dos Borelianos na reta. O caso em que nada se sabe sobre a forma funcional da função de distribuição F de X é claramente muito mais difícil. Problemas de inferência deste tipo de são o domínio de estudo da estatística não paramétrica. Neste livro abordamos os problemas da estatística paramétrica.\n2.1 Amostras aleatórias Considere-se um experimento estatístico que culmina em desfechos x, que são os valores assumidos por uma variável aleatória X. Seja F a função de distribuição de X. Na prática, F não será completamente conhecida, isto é, um ou mais parâmetros associados com F serão desconhecidos. O trabalho de um estatístico é estimar esses parâmetros desconhecidos ou testar a validade de certas afirmações sobre eles. Ele pode, por exemplo, obter n observações independentes de X. Isso significa que ele observa n valores x1, x2, · · · , xn assumidos da varia´vel aleatória X. Cada xi pode ser considerado como o valor assumido pela varia´vel aleatória Xi, i = 1, 2, · · · , n onde X1, X2, · · · , Xn são variáveis aleatórias independentes com distribuição comum F . Os valores observados (x1, x2, · · · , xn) são então valores assumidos por (X1, X2, · · · , Xn). O conjunto (X1, X2, · · · , Xn) é, então, uma amostra de tamanho n da distribuição da população F . O conjunto de n valores x1, x2, · · · , xn é chamado de uma realização ou estimativa da amostra. Note-se que os possíveis valores do vector aleatório (X1, X2, · · · , Xn) podem ser olhados como pontos em Rn, os quais podem ser chamados de elementos do espaço amostral. Na prática podemos não observar x1, x2, · · · , xn 57\nmas alguma função g(x1, x2, · · · , xn). Então g(x1, x2, · · · , xn) serão considerados os valores assumidos pela variável aleatória g(X). Vamos agora formalizar esses conceitos.\nSe X1, X2, · · · , Xn é uma amostra aleatória de F (·; ), a função de distribuição conjunta é dada por n F (x1, · · · , xn; ) = F (xi; )· (2.1) i=1\nSegundo esta definição cada uma das variáveis na amostra isoladamente é uma estatística assim como funções destas que, eventualmente, podem não fornecer informações u´teis. Duas das estatísticas mais comumente utilizadas são mostradas no exemplo a seguir.\nDeve-se lembrar que as estatísticas amostrais apresentadas neste exemplo Xn, S2\ne outras que irão definir-\nse posteriormente são varia´veis aleatórias, com todas as consequências que isso implica, enquanto os parâmetros populacionais µ, σ2 e assim por diante são constantes fixas, que podem ser desconhecidas. Exemplo 2.1 Seja X ∼ Bernoulli(p), onde p é desconhecido. A função de distribuição de X, mostrada na Figura 2.1, é dada\npor\nF (x) = pδ(x − 1) + (1 − p)δ(x), x ∈ R,\nonde a função δ(·) foi definida em (1.2) como δ(x) = 1, x 0, 0, x &lt; 0\n, chamada de função delta.\nSuponha que cinco observações independentes de X sejam 0, 1, 1, 1, 0. Então 0, 1, 1, 1, 0 é uma realização da amostra X1, X2, · · · , X5. A estimativa da média amostral é 0 + 1 + 1 + 1 + 0\nx5 =\n= 0, 6 5\no qual é o valor assumido pela variável aleatória Xn. A estimativa da variância amostral é 5 2 2 2 s2 = ∑ (xi − x5) = 2 × (0, 6) + 3 × (0, 4) = 0, 3\nsendo este o valor assumido pela variável aleatória S2.\n−1.0 −0.5 0.0 0.5 1.0 1.5 2.0 x\nFigura 2.1: Representação da função de distribuição Bernoulli para três valores do parâmetro p = 0.3, 0.5 e 0.8. Observe que nesta curva a reta no intervalo (0, 1) depende de 1 p, isso porque a função δ é sempre zero para x 1 nesse intervalo. Exemplo 2.2 Seja X ∼ N (µ, σ2), µ conhecida, σ2 de∑sconhecido e X1, · · · , Xn uma amostra aleatória dessa distribuição. De X são -0,864; 0,561; 2.355; 0,582 e -0,774. Então, a estimativa da média amostral é 0,372 e a estimativa da variância amostral é 1,648.\n2.2 Estatísticas de ordem Seja (X1, X2, · · · , Xn) um vetor aleatório n-dimensional e (x1, x2, · · · , xn) uma n-tupla assumida por (X1, X2, · · · , Xn). Vamos organizar x1, x2, · · · , xn em ordem crescente de magnitude, para que x(1) ≤ x(2) ≤ · · · ≤ x(n), onde x(1) = min(x1, x2, · · · , xn), x(2) é o segundo menor valor em x1, · · · , xn e assim por diante, x(n) = max(x1, · · · , xn). Se quaisquer dois xi, xj forem iguais, a ordem não importa.\nExemplo 2.3 Consideremos X1, X2, X3 três variáveis aleatórias discretas de maneira que, X1 e X3 sejam tais que assumam somente valores 0, 1 e que X2 assuma valores 1, 2, 3. O vetor aleatório (X1, X2, X3) assume os valores: (0, 1, 0), (0, 2, 0), (0, 3, 0), (0, 1, 1), (0, 2, 1), (0, 3, 1), (1, 1, 0), (1, 2, 0), (1, 3, 0), (1, 1, 1), (1, 2, 1) e (1, 3, 1). Então X(1) assume somente valores 0 ou 1; X(2) assume somente valores 0 ou 1 e X(3) assume somente valores 1, 2 ou 3.\nDemonstração : Exercício.\nNa apresentação dos resultados a seguir assumiremos que X1, X2, · · · , Xn são variáveis aleatórias independentes e igualmente distribuídas contínuas com função de densidade f . Seja {X(1), X(2), · · · , X(n)} o conjunto das estatística de ordem para X1, X2, · · · , Xn. Dado que todas as Xi são contínuas segue que, com probabilidade 1 X(1) ≤ X(2) ≤ · · · ≤ X(n)· 2.2.1 Propriedades das estatísticas de ordem Começaremos o estudo das propriedades encontrando a função de densidade conjunta de (X(1), X(2), · · · , X(n)).\nDemonstração : A transformação de (X1, · · · , Xn) a (X(1), · · · , X(n)) não é biunívoca. De fato, existem um total de n! possíveis arranjos de x1, · · · , xn em ordem crescente de magnitude. Assim, existem n! inversas para a transformação. Por exemplo, uma das n! permutações pode ser x4 &lt; x1 &lt; xn−1 &lt; x3 &lt; · · · &lt; xn &lt; x2·\nA inversa correspondente é x4 = x(1), x1 = x(2), xn−1 = x(3), x3 = x(4) · · · xn = x(n−1), x2 = x(n)· O determinante Jacobiano desta transformação é a matriz n×n identidade com as colunas reorganizadas, isto devido a que cada x(i) é igual a uma, e somente uma, das x1, x2, · · · , xn. Portanto J = ±1 e\nn f (x(2), x(n), x(4), x(1), · · · , x(3), x(n−1))|J| = f (x(i)), i=1 quando x(1) &lt; x(2) &lt; · · · &lt; x(n). A mesma expressão é válida para cada um dos n! arranjos. Segue então que\nf (x(1), · · · , x(n)) = Todas as n! permutações\nn f (x(i)) i=1\n=  \nn! i=1\nf (x(i)), caso x(1) &lt; x(2) &lt; · · · &lt; x(n) · 0, caso contrário\nExemplo 2.4 Sejam X1, · · · , Xn variáveis aleatórias independentes com função de densidade comum f (x) = 1, se 0 &lt; x &lt; 1 · 0, caso contrário Então a função de densidade conjunta de X(1), X(2), · · · , X(n) é\nf (x\n, · · · , x\n) = n!, 0 &lt; x(1) &lt; x(2) &lt; · · · &lt; x(n) 0, caso contrário\n· (2.5)\nEstamos confiados que como resultado do Teorema 2.2 temos funções de densidade. Vejamos neste exemplo se isso é realmente acontece. Consideremos, para simplificar, o caso n = 3 e verifiquemos se a integral da função de densidade em (2.5) é 1. Então\n∫∫∫\nf (x(1), x(2), x(3)) dx(1) dx(2) dx(3) = 6\n∫ 1 [∫ 1\n(∫ 1\ndx(3))\ndx(2)]\ndx(1)\n0 1 = 6 0\nx(1) [ x(1)\nx(2) (1 − x(2))\ndx(2)]\ndx(1)\n1 1 = 6 − x(1)\nx2 + (1) 2\ndx(1)\n= 1·\n0 Um detalhe interessante é que esta e outras propriedades demonstradas aqui somente são válidas quando as variáveis aleatórias são contínuas. Isso não significa que estatísticas de ordem não possam ser definidas no caso discreto. O que estamos dizendo é que estas propriedades somente podem ser demonstradas no caso contínuo. Exemplo 2.5 Consideremos a situação em que temos somente três variáveis aleatórias independentes X1, X2 e X3 com\ndistribuição geométrica de parâmetro p, isto é, P (X = x; p) = (1 − p)px, x = 0, 1, 2, · · · Encontremos P (X(1) &lt; X(2) &lt; X(3)). Nesta situação a probabilidade requerida pode ser escrita como: P (X(1) &lt; X(2) &lt; X(3)) = 1 − P (X1 = X2 ̸= X3) − P (X1 = X3 ̸= X2) −P (X2 = X3 ̸= X1) − P (X1 = X2 = X3) a qual pode ser escrita como P (X(1) &lt; X(2) &lt; X(3)) = 1 − 3P (X1 = X2 ̸= X3) − P (X1 = X2 = X3) = 1 − 3 [P (X1 = X2) − P (X1 = X2 = X3)] − P (X1 = X2 = X3) = 1 − 3P (X1 = X2) + 2P (X1 = X2 = X3)·\nNão é difícil perceber que\ne que\nP (X1 = X2) =\n(1 p)2 1 − p2 , (1 − p)3\ndo qual obtemos que\nP (X1 = X2 = X3) =\n1 − p3 ,\n6p3\nP (X(1) &lt; X(2) &lt; X(3)) = (1 − p)(1 + p + p2)· As propriedades das estatísticas de ordem que serão demonstradas valerão somente caso as variáveis sejam contínuas. Isto deve-se a que, caso as variáveis sejam discretas, a probabilidade\nP (X(1) = X(2) = · · · = X(n)) ̸= 0,\ncomo vai ser mostrado no seguinte exemplo. Acontece que o fato da probabilidade das estatística de ordem poderem coincidir, com probabilidade diferente de zero, altera a estrutura da demonstração e não nos permite obtermos estes resultados para o caso discreto. Exemplo 2.6 Sejam X1, X2, · · · , Xn variáveis aleatórias independentes assumindo somente 0 e 1 com probabilidade 1/2. Observemos que\nn n P (X(1) = X(2) = · · · = X(n)) = ∏ P (X(k) = 0) + ∏ P (X(k) = 1)\nk=1 n = P (Xk k=1\nk=1 n = 0) + P (Xk k=1\n1 = 1) = 2n−1 ·\nEstudemos agora o comportamento marginal, ou seja, nos interessa agora encontrar a função de distribuição marginal de cada estatística de ordem.\nDemonstração : Partimos da expressão da função de densidade conjunta das estatísticas de ordem obtida no Teorema 2.2. Então,\nfr(x(r)) = n!f (x(r))\n∫ x(r) ∫ x(r−1)\n· · ·\n∫ x(2) ∫ +∞ ∫ +∞\n· · ·\n∫ +∞ ∏\nf (ti) dtn · · · dtr+1 dt1 · · · dtr−1\n−∞ −∞\n−∞ x(r)\nx(r+1)\nx(n−1) i̸=r\n= n!f (x(r))\n[1 − F (x(r))]n−r x(r)\n(n − r)!\n· · ·\n∫ x(2) r∏−1\nf (ti) dti\n[1 − F (x(r))]n−r [F (x(r))]r−1\n= n!f (x(r))\n(n − r)!\n· (r − 1)!\nComo utilidade deste teorema podemos mencionar o fato de agora podermos encontrar os momentos das es- tatística de ordem. Faremos isso como consequência do seguinte exemplo. Exemplo 2.7 Sejam X1, X2, · · · , Xn variáveis aleatórias independentes U (0, 1). Então\nfr(x(r)) =  \nn!\n(r − 1)!(n − r)!\nxr−1(1 − x(r))n−r, se 0 &lt; x(r) &lt; 1 (1 ≤ r ≤ n) 0, caso contrário\nObservemos que, na situação do exemplo acima, X(r) ∼ Beta(r, n − r + 1), logo, valem os resultados da distribuição Beta e, por exemplo, E(X(r)) = r/(n + 1)· Para uma densidade qualquer e somente quatro variáveis aleatórias a forma da densidade marginal, de uma qualquer estatística de ordem, é mostrada no seguinte exemplo. Exemplo 2.8 Sejam X1, X2, X3, X4 variáveis aleatórias independentes com densidade comum f. A função de densidade conjunta das estatísticas de ordem X(1), X(2), X(3), X(4) é\nf (x\n, x(2)\n, x(3)\n, x(4)\n) = 4!f (x(1))f (x(2))f (x(3))f (x(4)), se x(1) &lt; x(2) &lt; x(3) &lt; x(4) · 0, caso contrário\nVamos calcular a função de densidade marginal de X(2). Temos que, se x(1) &lt; x(2) &lt; x(3) &lt; x(4) f2(x(2)) = 4! ∫ ∫ ∫ ∫ f (t1)f (x(2))f (t3)f (t4) dt1 dt3 dt4\n= 4!f (x(2))\n∫ x(2) ∫ +∞ [∫ +∞\nf (t4) dt4]\nf (t3)f (t1) dt3 dt1\n−∞ x(2) t3\n= 4!f (x(2))\n∫ x(2) {∫ +∞\n[1 − F (t3)]f (t3) dt3}\nf (t1) dt1\n−∞ x(2)\n= 4!f (x(2))\nx(2) [1 F (x )]2 2 f (t1) dt1 = 4!f (x(2))\n[1 F (x )]2 2! F (x(2))·\nEvidentemente, a expressão acima coincide com o resultado apresentado no Teorema 2.3.\nDemonstração :\nfrs(x(r), x(s)) =\n∫ x(r)\n· · ·\n∫ t2\n∫ x(s)\n· · ·\n∫ x(s) ∫ +∞\n· · ·\n∫ +∞\n−∞ −∞\nx(r)\nts−2\nx(s)\ntn−1\nn!f (t1) · · · f (tn) dtn · · · dts+1 dts−1 · · · dtr+1 dt1 · · · dtr−1\n= n!\n∫ x(r)\n· · ·\n∫ t2\n∫ x(s)\n· · ·\n∫ x(s) [1 − F (x(s))]n−s\n−∞ −∞\nx(r)\nts−2\n(n − s)!\n×f (t1)f (t2) · · · f (x(s)) dts−1 · · · dtr+1 dt1 · · · dtr−1\n= n!\n[1 − F (x(s))]n−s (n − s)! f (x(s))\nx(r)\n−∞\n· · ·\nt2 f (t1) · · · f (x(r))× −∞\n[F (x(s)) − F (x(r))]s−r−1 × (s − r − 1)! dt1 · · · dtr−1\n= (n − s)!(s − r − 1)![1 − F (x\n)]n−s×\n[F (x(r))]r−1\ncaso x(r) &lt; x(s).\n×[F (x(s)) − F (x(r))]s−r−1f (x(s))f (x(r))\n, (r − 1)!\nDe modo semelhante, podemos mostrar que a função de densidade conjunta de X(k1), · · · , X(km) se 1 ≤ k1 &lt;\nk2 &lt; · · · &lt; km ≤ n, 1 ≤ m ≤ n, é dada por fk1k2···km (x(k1), x(k2), · · · , x(km)) =\n(k1\n— 1)!(k2\n— k1\nn! × — 1)! × · · · (n − km)!\n×Fk1−1(x(k ))f (x(k ))[F (x(k )) − F (x(k ))]k2−k1−1f (x(k )) × · · · × ×[F (x(k )) − F (x(k ))]km−1−km−2−1f (x(k ))[1 − F (x(k ))]n−km f (x(k )), caso x(k1) &lt; x(k2) &lt; · · · &lt; x(km) e zero noutras situações. Exemplo 2.9 (Continuação do Exemplo 2.7) Sabemos que as variáveis aleatórias X1, X2, · · · , Xn são independentes e tem como função de densidade comum f (x) = 1, se 0 &lt; x &lt; 1 · 0, caso contrário Então, a função de densidade conjunta de X(r) e X(s) é dada por\nfrs\n(x(r)\n, x(s)) =\n n!\nxr−1(x(r) − x(s))s−r−1(1 − x(s))n−s (r − 1)!(s − r − 1)!(n − s)!\n, se x\n&lt; x(s) ·\n\nonde 1 ≤ r &lt; s ≤ n.\n0, caso contrário\nUma situação mais complexa é trabalharmos com funções de estatísticas de ordem. Não temos um resultado simples para o caso de qualquer funções destas estatísticas. Mas, no exemplo a seguir, podemos encontrar um resultado interessante para o comportamento da diferença de estatísticas de ordem. Exemplo 2.10 Sejam X(1), X(2), X(3) as estatísticas de ordem das variáveis aleatórias independentes e igualmente distribuídas X1, X2, X3 com função de densidade comum { βe−xβ, se x ≥ 0 sendo β &gt; 0. Sejam Y1 = X(3) − X(2) e Y2 = X(2). Mostraremos que Y1 e Y2 são independentes. Para isso primeiro observemos que a função de densidade conjunta de X(2) e X(3) é dada por\nf23(x, y) =\n1!0!0!\n· 0, caso contrário\nA função de densidade conjunta de (Y1, Y2) é então f (y1, y2) = 3!β2(1 − e−y2β )e−y2βe−(y1+y2)β = [3!βe−2y2β (1 e−y2β )][βe−y1β ], se 0 &lt; y &lt; + , 0 &lt; y &lt; + = · 0, caso contrário Do qual segue que Y1 e Y2 são independentes. Duas estatísticas de ordem importantes são o máximo e mínimo. Nesses casos é possível encontrar, de maneira\nanalítica, expressões para a função de distribuição. Vejamos no teorema a seguir as expressões da função de distribuição das estatísticas de ordem X(1) e X(n).\nDemonstração : Exercício.\nAcerca da função de distribuição de qualquer estatística de ordem temos o resultado a seguir.\nDemonstração : O evento {X(k) ≤ x} ocorre se, e somente se, pelo menos k dos X1, X2, · · · , Xn são menores ou iguais a x, por isso o somatório começa em k.\nNos dois teoremas seguintes relacionamos a distribuição condicional de estatísticas de ordem, condicionadas em outra estatística de ordem, com a distribuição de estatísticas de ordem de uma população cuja distribuição é uma forma truncada da função de distribuição da população original F .\nDemonstração : A densidade condicional de X(j) dado que X(i) = xi calcula-se dividindo a densidade conjunta de X(i) e X(j), dada em (2.7), pela densidade marginal de X(i), esta obtida no Teorema 2.4. Temos então que, quando\ni &lt; j ≤ n e xi ≤ xj &lt; ∞,\nf (xj|X(i)\n= x ) = fij(xi, xj) i fi(xi) (n − i)! [ F (xj) − F (xi)]j−i−1\n[ 1 − F (xj)]n−j f (xj)\n(j − i − 1)!(n − j)! 1 − F (xi)\n1 − F (xi) 1 − F (xi)\nO resultado segue observando que F (xj ) − F (xi) e f (xj ) são, respectivamente, as funções de distribuição e de 1 − F (xi) 1 − F (xi) densidade truncando à esquerda em xi a distribuição F .\nNa demonstração do teorema anterior utiliza-se o conceito de distribuição truncada, o que é isso? define-se a seguir este conceito e incluem-se exemplos explicativos.\nCaso a variável aleatória X seja discreta com função de probabilidade P , a distribuição truncada de X é dada por\nP (X = x|X ∈ A) =\nP (X = x, X ∈ A) P (X ∈ A)\n=  \nP (X = x) P (X = a), se x ∈ A a∈A 0, se x ∈/ A\nNa situação X do tipo contínua, com função de densidade f , temos que\nP (X ≤ x|X ∈ A) =\nP (X ≤ x, X ∈ A) = P (X ∈ A)\n∫(−∞,x]∩A\nf (y) dy\n· (2.8)\nConcluindo então que, a função de densidade da distribuição truncada é dada por\nh(x) =\n ∫\nf (x) f (y) dy\n, caso x ∈ A,\n· (2.9)\n 0 A\ncaso x ∈/ A\nExemplo 2.11 Suponhamos X uma variável aleatória com distribuição normal padrão e A = ( , 0]. Então, P (X A) = 1/2, dado que X é simétrica e contínua. Para a densidade truncada temos que { 2f (x), caso − ∞ &lt; x ≤ 0,\nO truncamento é especialmente importante nos casos em que a distribuição F em questão não tem média finita. Se X é uma variável aleatória, truncamos X em algum c &gt; 0, onde c é finito, substituindo X por Xc = X caso |X| c e zero caso |X| &gt; c. Então Xc é X truncada em c e todos os momentos de Xc existem e são finitos. Na verdade, sempre podemos selecionar c suficientemente grande para que P (X ̸= Xc) = P (|X| &gt; c),\nseja arbitrariamente pequena. A distribuição de Xc é então dada por\nP (Xc ≤ x) = P (X ≤ x| |X| ≤ c) = no caso contínuo com função de densidade f e é dada por\nf (y) dy (−∞,x]∩[−c,+c] , P (|X| ≤ c)\nP (Xc = x) =\n \nP (X = x)\nP (X = a) a∈[−c,+c]\n, se x ∈ [−c, +c] ,\n0, se x ∈/ [−c, +c] no caso discreto. Observemos que, para algum α &gt; 0, E(|Xc|)α ≤ cα· Exemplo 2.12 Caso X Cauchy(0, 1), sabemos que E(X) não existe. Seja c &gt; 0 um nu´mero finito, truncando X em c definimos\nEntão\nXc =\nX, caso |X| c, · 0, caso |X| &gt; c\n1 ∫ +c 1\n2 −1\nSendo que a função de densidade truncada é dada por\n 1 1\n1 , caso x ∈ [−c, +c],\nh(x) =\nDesta expressão obtemos que\n2 1 + x2 tan−1(c) ·  0, caso x ∈/ [−c, +c]\ne também que\nE(Xc) =\n2 tan−1(c)\n−c 1 + x2\ndx = 0,\nE(Xc)2 =\nx2 dx =\n— 1·\n2 tan−1(c)\n−c 1 + x2\ntan−1(c)\nPor u´ltimo, temos o seguinte resultado estabelecendo novamente relação entre estatísticas de ordem e distri- buições truncadas.\nDemonstração : A densidade condicional de X(i) dado que X(j) = xj calcula-se dividindo a densidade conjunta de X(i) e X(j), dada em (2.7), pela densidade marginal de X(j), esta obtida no Teorema 2.4. Temos então que, quando i &lt; j ≤ n e xi ≤ xj &lt; ∞, (j − i)! [ F (xi) ]i−1 [ F (xj) − F (xi)]j−i−1 f (xi) O resultado segue observando que F (xi)/F (xj) e f (xi)/F (xj) são, respectivamente, as funções de distribuição e de densidade truncando à direita em xj a distribuição F .\n2.2.2 Quantis Lembremos que a função de distribuição F é contínua à direita e que o nu´mero de descontinuidades é, no máximo, enumerável. Estas são propriedades importantes que farão toda diferença na definição dos quantis amostrais, por isso, demonstraremos as propriedades mencionadas da função de distribuição. A prova de que F é contínua à direita advém do seguinte fato F (x + hn) − F (x) = P (x &lt; X ≤ x + hn), onde {hn} é uma sequência de nu´meros reais estritamente positivos tais que limn→∞ hn = 0. Segue, da propriedade de continuidade da função de probabilidade,1 que lim [F (x + hn) F (x)] = 0, n→∞ e, portanto, F é contínua à direita. Definamos por D o conjunto dos pontos de descontinuidade de F e seja D = {x ∈ D : P (X = x) ≥ 1 } , onde n é um inteiro positivo. Dado que F ( ) F ( ) = 1, o nu´mero de elementos em Dn não pode exceder n. Logicamente ∞ D = Dn n=1 e, então, o conjunto D é enumera´vel. Demonstrando-se assim a segunda propriedade importante mencionada da função de distribuição. Definimos a seguir o conceito de quantil teórico e depois mostramos a forma de cálculo.\n1A função de distribuição é contínua, devido a que P lim n→∞\nAn = lim n→∞\nP (An),\nse o limite limn→∞ An existir.\nA função F −1(t), 0 &lt; t &lt; 1 foi definida em (1.29) e é chamada de função inversa de F . O seguinte teorema fornece-nos propriedades u´teis. Fica claro que as propriedades apresentadas no seguinte teorema nos permitirão o cálculo dos quantis e é por isso que dedicamos atenção a este conceito.\nDemonstração : Exercício.\nExemplo 2.13 Seja X Exponencial(). Sabemos que a função de distribuição neste caso é F (x) = 1 e−x/. Resulta que a expressão de qualquer um dos quantis é possível de ser encontrada de maneira exata via\nobtendo-se que\nF (ξp) = p 1 − e−ξp/= p 1 − p = e−ξp/,\nξp = −ln(1 − p)\né a expressão teórica do p-ésimo quantil. Devemos mencionar que a expressão dos quantis está bem definida, no sentido de que o resultado é sempre positivo. Isto é importante porque devemos lembrar que a distribuição exponencial está definida somente para valores positivos, então o quantil teórico deve ser positivo, já que é um dos possíveis valores da varia´vel. Observemos que caso F seja contínua e estritamente crescente, F −1 é definida como F −1(y) = x quando y = F (x)· Ainda podemos observar que, se x0 é um ponto de descontinuidade de F e supondo que F (x−) &lt; y &lt; F (x0) = F (x+) 0 0\nvemos que, embora não exista x tal que y = F (x), F −1(y) é definido como igual a x0. A situação na qual F não é estritamente crescente, por exemplo, caso da variável aleatória ser discreta, podemos escrever\nF (x) =\n= y, caso a ≤ x ≤ b ·  &gt; y, caso x &gt; b\nEntão, qualquer valor a x b poderia ser escolhido como x = F −1(y). A convenção é que, neste caso, definimos F −1(y) = a. Em particular ξ1/2 = F −1(1/2), (2.10) é chamada de mediana de F . Observemos que ξp satisfaz a desigualdade F (ξp− ) ≤ p ≤ F (ξp)· Exemplo 2.14 (Continuação do Exemplo 2.13) Caso p = 1/2, a mediana amostral será ξ1/2 = −ln(1/2) = 0.6931472.\n2.3 Momentos amostrais Nesta seção vamos estudar algumas estatísticas amostrais comumente utilizadas e suas distribuições.\nObservemos que nFn(x) é o nu´mero de Xk (1 ≤ k ≤ n) menores ou iguais a x. Se X(1), X(2), · · · , X(n) são as estatísticas de ordem de X1, X2, · · · , Xn então claramente\nFn(x) =  \nk , se X n\n≤ x &lt; X\n(k+1)\n, (k = 1, 2, · · · , n − 1)\n· (2.11)\n 1, se x ≥ X(n)\ncom esperança e variância\nVar[Fb\nE[Fn(x)] = F (x) (2.13)\nF (x)[1 − F (x)]\nDemonstração : Dado que δ(x − Xi), i = 1, 2, · · · , n são variáveis aleatórias independentes igualmente distribuídas cada uma com função de probabilidade P [δ(x − Xi) = 1] = P (x − Xi ≥ 0) = F (x) e P [δ(x − Xi) = 0] = 1 − F (x), sua soma nF ∗(x) é uma variável aleatória com distribuição Binomial(n, p), onde p = F (x). As relações (2.12), (2.13) e (2.14) seguem-se imediatamente.\nDemonstração :\nE´ uma consequência da Lei dos Grandes Nu´meros .\nCorolário 2.12\nonde Z ∼ N (0, 1).\n√n[F (x) F (x)] √F (x)[1 − F (x)] −→ Z quando n → ∞,\nDemonstração :\nE´ consequência do Teorema do Limite Central.\nExemplo 2.15 Vamos apresentar o conceito de função distribuição empírica no caso de termos uma amostra aleatória da distribuição N (0, 1). A lista de comandos na linguagem de programação R está disponível abaixo. O primeiro comando destina-se a fixar o gerador de amostras e, assim, em qualquer momento podemos obter a mesma amostra aleatória. Na Figura 2.2 mostramos a forma da distribuição empírica, de três formas diferentes, para uma amostra de tamanho 12. A representação da função de distribuição empírica é realizada permitindo escolher qual utilizar segundo o agrado.\nlwd = 2\n−1.5 −1.0 −0.5 0.0 0.5\nx\n−1.5 −1.0 −0.5 0.0 0.5\nx\n−1.5 −1.0 −0.5 0.0 0.5\nx\nFigura 2.2: Representação da função de distribuição amostral ou empírica, de três formas diferentes, para uma amostra normal padrão de tamanho 12.\nA linhas de comando a seguir permitiram-nos gerar os gráficos na Figura 2.2: construímos : set.seed(5739); x=rnorm(12); Fn=ecdf(x) par(mar=c(5,4,3,1), cex=0.9) plot(Fn, main=““) plot(Fn, verticals = TRUE, do.points = FALSE, main=”“) plot(Fn , lwd = 2, main=”“); mtext(”lwd = 2”, adj = 1) xx=unique(sort(c(seq(-3, 2, length = 201), knots(Fn12)))) lines(xx, Fn(xx), col = “blue”) abline(v = knots(Fn), lty = 2, col = “gray70”) Observemos que a convergência da distribuição empírica, segundo o Teorema 2.10, é para cada valor de x. E´ possível fazer uma demonstração da convergência em probabilidade simultaneamente para todos os x, ou seja, da convergência uniforme.\nDemonstração : Seja ϵ &gt; 0. Escolhemos um inteiro k &gt; 1/ϵ e nu´meros −∞ = x0 &lt; x1 ≤ x2 ≤ · · · ≤ xk−1 &lt; xk = ∞, tais que F (x−) ≤ j/k ≤ F (xj), para j = 1, · · · , k − 1. Observe que se xj−1 &lt; xj, então\nPela Lei dos Grandes Nu´meros\nF (x−) − F (xj−1) ≤ ϵ·\nq.c. Fn(xj) −→ F (xj) e — q.c. −\npara j = 1, · · · , k − 1. Consequentemente,\nFbn(xj ) −→ F (xj ),\n∗ − q.c. ∆n = max{|Fbn(xj) − F (xj)|, |Fn (xj ) − F (xj)|, j = 1, · · · , k − 1} −→ 0·\nSeja x arbitrário e encontremos j tal que xj−1 &lt; x ≤ xj. Então, Fbn(x) − F (x) ≤ Fbn(x−) − F (xj−1) ≤ Fbn(x−) − F (x−) + ϵ,\ne\nIsto implica que\nFbn(x) − F (x) ≥ Fbn(xj−1) − F (x−) ≥ Fbn(xj−1) − F (xj−1) − ϵ·\nq.c. sup |Fn(x) F (x)| ∆n + ϵ ϵ· x Como isso vale para todo ϵ &gt; 0, o teorema segue.\nAgora, dado que F ∗(x) tem pontos de salto em Xi, i = 1, 2, · · · , n é claro que existem todos os momentos de F ∗(x). Vamos considerar alguns valores típicos da função de distribuição F , chamados de estatísticas amostrais. Escrevamos a = 1 ∑ Xk, (2.15)\npara os momentos de ordem k ao redor do 0 (zero). Aqui ak, serão chamados de momentos amostrais de ordem k. Com esta notação\nO momento amostral central é definido por\nn a1 = Xi n i=1\n= X·\nb = 1 ∑(X − a )k = 1 ∑(X\n— X) · (2.16)\nLogicamente,\nk n i 1 i=1\nn i i=1\nb = 0 e b\n= (n − 1 ) S2·\nComo mencionado anteriormente, não chamamos b2 a variaˆncia amostral. S2\nserá chamada como a variaˆncia\namostral por razões que se tornarão claras posteriormente. Temos que b2 = a2 − a2· Para a função geradora de momentos de Fn podemos afirmar que n\nMFbn\ni=1\nDefinições similares são realizadas para momentos amostrais de distribuições multivariadas. Por exemplo, se (X1, Y2), (X2, Y2), · · · , (Xn, Yn) é uma amostra de uma distribuição bivariada, podemos escrever n n X = 1 ∑ X , Y = 1 ∑ Y\npara as duas médias amostrais e para os momentos de segunda ordem centrais escrevemos\nb20\nn = (Xi n i=1\n— X) , b02\nn = (Yi n i=1\n— Y ) ,\nMais uma vez, escrevemos\nb11\nn\nn = (Xi n i=1\n— X)(Yi\n— Y )·\nn\nS2 = 1 ∑(X\n— X)2, S2 = 1 ∑(Y\n— Y ) , (2.17)\npara as duas variaˆncias amostrais e para a covariância amostral utilizamos n\nS11\n= 1 (X n − 1 i=1\n— X)(Yi\n— Y )· (2.18)\nEm particular, o coeficiente de correlação amostral é definido por b11 S11\nR = 20\nb02\n= · S1S2\nPode ser demonstrado que |R| ≤ 1 e que os valores extremos ±1 ocorrem somente quando todos os pontos amostrais (X1, Y2), (X2, Y2), · · · , (Xn, Yn) estão alinhados. Correspondendo a uma amostra X1, X2, · · · , Xn de observações em F , p-ésimo quantil amostral é definido como o p-ésimo quantil da função de distribuição amostral, ou seja, como F −1. Os quatis amostrais são definidos de maneira similar. Então, se 0 &lt; p &lt; 1, o quantil amostral de ordem p,\nr = [np] se n é um nu´mero par [np] + 1 se n é um nu´mero ímpar\n· (2.19)\nComo usual, [x] denota o maior inteiro x. Observe que, se [n] for par, podemos escolher qualquer valor entre X([np]) e X([np]+1) como o p-ésimo quantil amostral. Então, se p = 1 e n par podemos escolher qualquer valor entre X(n/2) e X(n/2+1), os dois valores do meio, como a mediana amostral. Habitualmente é escolhido o ponto médio. Assim, a mediana amostral é definida como\nξb1/2 = \nX((n+1)/2) se n é ímpar X(n/2) + X((n/2)+1) se n é par\n· (2.20)\nObserve que\n2 [n + 1] = (n + 1 )\n2 2 se n é ímpar. Consideraremos agora os momentos de características amostrais. Nos seguintes desenvolvimentos denotaremos E(Xk) = mk e E[(X µ)k] = µk como os momentos populacionais e os momentos populacionais centrais de k-ésima ordem, respectivamente. Nas situações onde utilizamos mk ou µk assumiremos que estes existem. Também, σ2 representara´ a variância populacional.\nDemonstração : Para provar (2.23) observemos que\n(∑n\nXi = ∑\nX3 + 3 ∑ ∑\nX2Xk + ∑ ∑ ∑ XiXjXk,\ni=1\ni=1\ni=1 j=1 j̸=k\ni=1 j=1 k=1 i̸=j, i̸=k j̸=k\ndesta expressão obtemos o resultado em (2.23). Similarmente\n(∑n )4\n( n )  n n n\n\n∑ ∑ ∑\nXi =\n∑ Xi ∑ X3 + 3 ∑ ∑ X2Xj +\ni j k\ni=1\ni=1\nn\n i=1\ni i=1 j=1 i̸=j n n\ni=1 j=1 k=1 i̸=j, i̸=k j̸=k\n= ∑ X4 + 4 ∑ ∑ XiX3 + 3 ∑ ∑ X2X2\ni i=1\ni=1 j=1 i̸=j\nj i j i=1 j=1 i̸=j\nn n n n n n n = +6 ∑ ∑ ∑ X2XjXk + ∑ ∑ ∑ ∑ XiXjXkXl·\ni=1 j=1 k=1 i̸=j, i̸=k j̸=k\ni=1 j=1 k=1 l=1 i̸=j, i̸=k, i̸=l j̸=k, j̸=l k̸=l\nUm detalhe importante é que os momentos centrais podem ser calculados a partir dos momentos, por exemplo, µ2 = E[(X − µ)2] = m2 − µ2, µ3 = E[(X − µ)3] = m3 − 3µm2 + 2µ3\ne assim por diante. Sabemos agora como calcular os momentos, até quarta ordem, de X. Vejamos a seguir como calcular os momentos centrais.\nDemonstração : Temos que µ (X) = E(X − µ)3 = E {∑n\n(X − µ)3} = ∑n\nE(X\n3 µ3 — µ) = ·\n3\nNo caso do quarto momento central\nn3 i=1 i\n1\nn3\n{∑n\ni=1 i n2\n}\nda qual obtemos que\nµ (X) = E(X µ)4 = E n4\ni=1\n(Xi − µ)4 ,\n1 µ4(X) =\nE(Xi − µ)4 +\nE{(Xi − µ)2(Xj − µ)2}·\nn4 i=1\n2 n4\ni=1 j=1 i̸=j\nDesenvolvendo adequadamente chegamos ao resultado em (2.26).\nExemplo 2.16\n, Xn uma amostra aleatória da distribuição Gamma(α, β). Sabemos da Seção 1.2 que E(X) = αβ, Var(X) = αβ2\nmk = βk(α + k − 1)(α + k − 2) · · · α, k ≥ 1· αβ2 E(X) = αβ, Var(X) = n 1 1 µ (X) = µ = (6α3β3 + 3α2β3 + 2αβ3)· 3 n2 3 n\nAté o momento estudamos como calcular os momentos da média amostral. Mais complexo é obter expressões para os momentos da variância amostral S2. O teorema a seguir dedica-se ao objetivo de encontrarmos expressões, até segunda ordem, dos momentos amostrais centrais. Como consequência deste resultado obtemos os momentos da variaˆncia amostral.\nDemonstração : Temos que\nn E(b2) = E n i=1\nX2 n2\nn 2 Xi i=1\n= m2 −\n1  n E \nX2 + ∑\n∑ X2Xj\nAgora\n= m2\n1 — n2 [nm2\n— µ )·\nn2b2 =\nn\ni=1\n2\n(Xi − µ)2 − n(X − µ)2 ·\nEscrevendo Yi = Xi − µ, vemos que E(Yi) = 0, Var(Yi) = σ2 e E(Y 4) = µ4. Temos então que\nn2 E(b2) = E\nn i=1 n\n2\nY 2 − nY 2\nn n\n n n n \n= E ∑ Y 4 + ∑ ∑ Y 2Y 2 − 2 ∑ ∑ Y 2Y 2 + ∑ Y 4\nY 2Y 2 + ∑\nY 4 ·\nSegue então que\nn i j i=1 j=1 i̸=j\ni=1\ni \n2 1 n2 E(b2) = nµ + n(n − 1)σ2 − [n(n − 1)σ4 + nµ ] + [3n(n − 1)σ4 + nµ ]\n= (n − 2 + 1 ) µ + (n − 2 + 3 ) (n − 1)µ2 · (µ\n= σ2)\nPortanto\nn 4 n 2 2\nVar(b2) = E(b2) − [ E(b2)]2\n= (n − 2 +\n1 ) µ4\n( n − 1 )2\n= (n − 2 +\n1 ) µ4\nµ2 + (n − 1)(3 − n) ,\ncomo afirmado. As relações (2.29) e (2.30) podem ser provadas de forma semelhante.\nEste é justamente o motivo pelo qual chamamos S2 e não b2 de variância amostral.\nExemplo 2.17 (Continuação do Exemplo 2.16) inteir Nesta situação, σ2 = αβ2, µ2 = σ2 e µ4 = m4 − 4m3µ + 6m2µ2 − 3µ4. Obtemos que E(S2) = αβ2 e Var)(S2) = µ4 + 3 − n α2β4· n n(n − 1)\nO seguinte resultado fornece uma justificativa para a nossa definição de covariaˆncia amostral.\nDemonstração : Do Corolário 2.17 sabemos que E(S2) = σ2 e E(S2) = σ2. Para provar que E(S11) = ρσ1σ2 1 1 2 2 observemos que Xi é independente de Xj, (i ̸= j) e de Yj, (i ̸= j). Temos que\nAgora\n(n − 1) E(S11) = E\nE{(Xi − X)(Yi − Y )} =\nn i=1\n(Xi − X)(Yi − Y )] ·\n( ∑n Yj\n∑n Yj\n∑n Xj ∑n\nYj )\ne segue que\n1 = E(XY ) − n [ E(XY ) + (n − 1) E(X) E(Y )] 1 − n [ E(XY ) + (n − 1) E(X) E(Y )] 1 − n2 [n E(XY ) + n(n − 1) E(X) E(Y )] = n − 1 [ E(XY ) E(X) E(Y )] n\n(n − 1) E(S11) = n ( ) [ E(XY ) − E(X) E(Y )], n − 1\nisto é\nE(S11) = E(XY ) − E(X) E(Y ) = Cov(X, Y ) = ρσ1σ2·\nA seguir, voltamos nossa atenção para as distribuições das características da amostra. Existem várias possi- bilidades. Se for necessária a distribuição exata o método de transformação de varia´veis pode ser utilizado. As vezes, a técnica da função geradora de momentos pode ser aplicada. Assim, se X1, X2, · · · , Xn é uma amostra aleatória de uma população com distribuição para a qual existe a função geradora de momentos, a função geradora de momentos da média amostral X é dada por n M (t) = E(etXi/n) = [MX(t/n)]n , (2.32) i=1 onde MX é a função geradora de momentos da distribuição populacional. Se MX (t) tiver alguma forma conhecida seria possível escrever a função de probabilidade ou de densidade de X. Embora este método tem a desvantagem óbvia que se aplica apenas à distribuições para as quais existem todos os momentos, veremos sua efetividade na situação importante de amostras da distribuição normal. Exemplo 2.18 Seja X1, X2, · · · , Xn uma amostra aleatória de tamanho n da distribuição Gama(α, 1). Nesta situação podemos encontrar a função de densidade de X. Temos que\nMX (t) = [MX\n(t/n)]n = 1 , t (1 − t/n)αn n\n&lt; 1,\nda qual obtemos que X ∼ Gama(nα, 1/n). Exemplo 2.19 Seja X1, X2, · · · , Xn uma amostra aleatória da distribuição Uniforme no intervalo (0, 1). Considere a média geométrica\nYn =\nn\ni=1\n1/n Xi ·\nSabemos que log(Yn) = (1/n) ∑n log(Xi) e, desta forma, log(Yn) é a média amostral de log(X1), · · · , log(Xn). A função de densidade comum de log(X1), · · · , log(Xn) é\nex, se x &lt; 0 f (x) = , 0, caso contrário\nque é a distribuição exponencial negativa com parâmetro β = 1. Vemos que a função geradora de momentos de log(Yn) é dada por\nMlog(Yn)\nn (t) = E(et log(Xi)/n) = , (1 t/n)n i=1\ne a função de densidade de log(Yn) é dada por\nflog(Yn)\nnn Γ(n)[−y]\nn−1\neny\n, se − ∞ &lt; y &lt; 0 ·\n 0, caso contrário\nSegue então que Yn tem por função de densidade\nfYn\nnn y Γ(n)\nn−1\n[− log(y)]\nn−1\n, se 0 &lt; y &lt; 1 ·\n\nVoltemos ao quantil amostral de ordem p,\n0, caso contrário\nξbp, o qual sabemos é ou X([np]) ou X([np]+1) dependendo se [np] é\num nu´mero par ou ímpar, como definido em (2.19). Simplificando, vamos discutir as propriedades de X([np]), onde p ∈ (0, 1) e n é grande. Isso, por sua vez, nos informará sobre as propriedades de ξp. Primeiro observemos que, se U1, U2, · · · , Un é uma amostra aleatória da distribuição U (0, 1) então, pelo Teorema 2.3, temos que\ndo qual obtemos que\nU([np]) ∼ Beta([np], n − [np] + 1),\n[np]\nE(U([np])) =\nn + 1\nn−→→∞ p,\nCov(U , U\n) = n np1\n−→ p (1 − p )·\nUtilizando este resultado e a desigualdade de Chebychev, demonstramos que U −P→ p· (2.33)\nIsso gera a questão\nξbp −→ ξp?\nqualquer seja a distribuição da amostra aleatória X1, · · · , Xn. Para respondermos a pergunta acima vamos utilizar o Lema de Hoeffding, ou seja, para respondermos se o quantil amostral de ordem p converge em probabilidade para o quantil teórico correspondente, utilizaremos o seguinte resultado devido a Hoeffding (1963).\nDemonstração : Dado que as variáveis aleatórias são limitadas ao intervalo (0, 1), sabemos que ehX ≤ (1 − X) + Xeh, isto deve-se a que a função exponencial ehX é convexa e, portanto, seu gráfico é limitado por cima no intervalo 0 ≤ X ≤ 1 pela linha que conecta as ordenadas X = 0 e X = 1. Então E(ehX ) ≤ (1 − E(X)) + E(X)eh· (2.35)\nSeja Sn = ∑n\nXi. Sabemos que\nP (Sn − E(Sn) ≥ nt) = E(1[Sn− E(Sn)−nt≥0]),\ntambém sabemos que\n1[Sn− E(Sn)−nt≥0] ≤ exp (h(Sn − E(Sn) − nt)),\nqualquer seja h uma constante positiva arbitrária. Então P Sn − E(Sn) ≥ nt ≤ E eh(Sn− E(Sn)−nt) (2.36) e como estamos assumindo que as variáveis são independentes, podemos escrever\n( ( ))\n∏ ( ( ))\nEscrevendo µi = E(Xi) temos, pela expressão em (2.35) que E(eh(Xi−µi)) ≤ e−hµi ((1 − µi) + µieh) = ef(h), (2.38) onde f (h) = −hµi + ln(1 − µi + µieh). As primeiras duas derivadas são:\n′ µi\n′′ µie−h(1 − µi)\nf (h) = −µi + e−h(1 − µ ) + µ\ne f (h) = [µi\nvemos que este quociente é da forma u(1 − u), sendo\nEntão, pela expressão em (2.38)\nf (h) ≤\nf (0) + f ′(0)h +\n1 h2 = 8\n1 h2· 8\nSubstituindo em (2.36) temos que\nE(eh(Xi−µi)) ≤ e 1 h2 ·\nP (Sn\n— E(Sn\n) ≥ nt) ≤ e−nht+ 1 nh2 ,\ne o mínimo no expoente é atingido quando h = 4t. Então, o mínimo do limite superior da probabilidade é exp(−2nt2).\nDevemos lembrar que esta não é a u´nica maneira de termos uma taxa de convergência para Teorema do Limite Central. Por exemplo, se Y1, Y2, · · · , Yn forem variáveis aleatórias independentes e identicamente distribuídas, utilizando o Teorema de Berry-Esseen2, temos que\n( ∑n ∑\n) ( √ Var(Y1))\nC E|Y1\n— E(Y1)|\nP i=1\nXi −\ni=1\nE(Xi) ≥ nt ≤ Φ t n\nVar3/2(Y ) ·\n2\nDemonstração : Berry (1941); Esseen (1942).\nPode-se consultar o livro de Feller (1971) para uma demonstração moderna.\nExemplo 2.20 Caso a amostra aleatória seja Bernoulli(µ), temos que n Xk ∼ Binomial(n, µ)· i=1 Então, segundo a desigualdade de Hoeffding\nP (X − µ ≥ t) ≤ exp(−2nt2)· Uma vantagem da desigualdade no Lema de Hoeffding é que não assume-se conhecimento da variância e, em geral, o limite da probabilidade é mais acurado do que outras desigualdades. Caso as variáveis aleatórias sejam limitadas como a ≤ Xi ≤ b, com a &lt; b, o limite superior da desigualdade (2.34) seria exp − 2nt2/(b − a)2 . Exemplo 2.21 Sejam X1, · · · , Xn variáveis aleatórias com distribuição U ( 1, 1). Nesta situação E(X) = 0, a = 1 e b = 1. A desigualdade de Hoeffding assume a forma P (X ≥ t) ≤ exp ( − nt2/2)·\nDemonstração : Para ϵ &gt; 0 qualquer, podemos escrever P (|ξp − ξp| &gt; ϵ) = P (ξp &gt; ξp + ϵ) + P (ξp &lt; ξp − ϵ)· Pelo Teorema 2.9, podemos escrever P (ξbp &gt; ξp + ϵ) = P (p &gt; Fbn(ξp + ϵ))\nn = P i=1\n1[Xi&gt;ξp+ϵ] &gt; n(1 − p))\nn = P i=1\nVi −\n∑i=1\nE(Vi) &gt; nδ1),\nonde Vi = 1[Xi&gt;ξp+ϵ] e δ1 = F (ξp + ϵ) − p. Da mesma forma, P (ξbp &lt; ξp − ϵ) = P (p &gt; Fbn(ξp − ϵ))\nn = P i=1\nWi −\n∑i=1\nE(Wi) &gt; nδ2),\nonde Wi = 1[Xi&lt;ξp−ϵ] e δ2 = p − F (ξp − ϵ) − p. Portanto, utilizando o Lema de Hoeffding (Lema 2.20), temos P (ξbp &gt; ξp + ϵ) ≤ exp(−2nδ2) P (ξbp &lt; ξp − ϵ) ≤ exp(−2nδ2)· Colocando δϵ = min{δ1, δ2}, a prova está completa.\nDemonstramos que\nlim P (|ξbp − ξp| &gt; ϵ) ≤ lim 2 exp(−2nδ2) = 0,\no qual significa que ξp −→ ξp. Em outras palavras, sempre que ξp seja solução u´nica da desigualdade F (ξp ) ≤ p ≤ F (ξp), 0 &lt; p &lt; 1, o quantil amostral converge em probabilidade para o quantil populacional e isto sempre acontece nas distribuições contínuas. Um detalhe importante é que para demonstrarmos a convergência em probabilidade de ξp utilizamos o Lema de Hoeffding e ele depende da existência da esperança. O seguinte resultado fornece a distribuição assintótica da r-ésima estatística de ordem amostral de uma po- pulação com uma função de distribuição F , absolutamente contínua, e função de densidade f .\nDemonstração : Vamos demonstrar somente para o caso p = 1/2. Observemos que ξ1/2 é mediana u´nica dado que f (ξ1/2) &gt; 0. Primeiro, consideremos que n seja ímpar, por exemplo, n = 2m − 1, logo P [√n(X(m) − F −1(1/2)) ≤ t] = P (X(m) ≤ t/√n + F −1(1/2))·\nSeja Sn o nu´mero de X que excedem t/ n + F (1/2). Então\nPercebemos que\nt X(m) ≤ √n + F\n(1/2) se, e somente se, Sn ≤ m − 1 =\nn − 1 · 2\nSn ∼ Binomial(n, 1 − F (F −1(1/2) + t/√n))· Fazendo pn = 1 − F (F −1(1/2) + t/√n), temos que\nP [√n(X\n— F −1(1/2)) ≤ t] = P (Sn\n≤ n − 1 )\n( Sn − npn 1 (n − 1) − npn )\n= P\nUtilizando o Teorema de Berry-Esseen, temos que\n√npn(1 − pn) ≤ √npn(1 − p ) · n\n{ ( n − 1 ) ( 1 (n − 1) − npn )}\nlim P n→∞\nSn ≤ 2\n— Φ √np\n(1 − pn)\n= 0·\nEscrevendo\n1 (n − 1) − npn npn(1 − pn)\n=\n√n( 1 − pn) 1/2 √n( − 1 + F (t/√n + F −1(1/2)))\n= 2t\n1/2 F (t/√n + F −1(1/2)) − F (F −1(1/2))\n−→ 2tf\n(F −1(1/2))·\nEntão\n( 1 (n − 1) − npn )\n( ( −1 ))\nΦ npn ou\n(1 − pn)\n≈ Φ 2tf F\n(1/2)\n√n(X\n( 1 )) −D→ N (0,\n4f 2\n1 (F −1(1/2)\n)) ·\nQuando n é par, digamos n = 2m, ambos P (√n X(m) F −1(1/2) t) quanto P (√n X(m+1) F −1(1/2) t) convergem a Φ(2tf (F −1(1/2))).\nObserve que o quantil amostral de ordem p, assintótica\nξbp, como consequência do Teorema 2.23, tem por distribuição\nN (ξ , 1 p(1 − p)) , onde ξp é o correspondente quantil populacional e f é a função de densidade populacional. Por exemplo, suponha temos uma amostra aleatória da di√stribuição N (µ, σ2) de tamanho n. Seja ξb1/2 a mediana amostral obtida dessa b ( πσ2 )\nTambém devemos ter em consideração que para demonstrarmos o Teorema 2.23 utilizamos a Teorema de Berry- Esseen, o qual depende da existência dos primeiros dois momentos da variável aleatória. Com isso, caso X Cauchy(µ, σ), o Teorema 2.23 não se aplica.\n2.4 Gráficos descritivos Vejamos alguns conjuntos de dados disponíveis na linguagem de programação R (R Core Team, 2014), especifi- camente na libraria datasets, que nos permitiram mostrar a utilidade dos momentos amostrais para resumir as informações contidas nos dados. Para consultar estes conjuntos de dados basta digitar library(help = “datasets”) Alguns dos diversos exemplos disponíveis serão apresentados aqui. Exemplo 2.22 (Puromicina) Os dados sobre a velocidade de uma reação enzimática são obtidos por Treloar (1974) e disponíveis no arquivo de dados Puromycin. O nu´mero de contagens por minuto de produto radioativo a partir da reação foi medida como uma função da concentração do substrato em partes por milhão (ppm) e a partir destas contagens a taxa\ninicial (ou velocidade) da reação foi calculada (contagens/min/min). O experimento foi realizado uma vez com a enzima tratada com puromicina e depois com a enzima não tratada. A estrutura destes dados tem 23 linhas e 3 colunas, cada coluna contendo as informações das variáveis: conc: um vector numérico de concentrações de substrato (ppm); rate: um vector numérico de taxas de reação instantânea (contagens/min/min); state: um fator com níveis treated (tratada) ou untreated (não tratada). Para a leitura e observação dos nomes das varia´veis utilizamos os comandos a seguir: data(Puromycin) names(Puromycin) Uma maneira de obtermos estatísticas descritivas é utilizando as linhas de comando a seguir: summary(rate[state==’treated’]) Min. 1st Qu. Median Mean 3rd Qu. Max. 47.0 104.5 145.5 141.6 193.2 207.0 e summary(rate[state==’untreated’]) Min. 1st Qu. Median Mean 3rd Qu. Max. 51.0 85.0 115.0 110.7 137.5 160.0 para o caso da variável rate, as concentrações, obtidas as estatísticas descritivas segundo os níveis do fator state, se as concentrações foram ou não tratadas com puromicina. No caso das estatísticas descritivas acerca das concentrações de substrato, variável conc, temos: summary(conc[state==’treated’]) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.020 0.060 0.165 0.345 0.560 1.100 e summary(conc[state==’untreated’]) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.0200 0.0600 0.1100 0.2764 0.3900 1.1000 Os valores mínimos é máximos foram registrados sempre com os nomes de Min. e Max., respectivamente. O primeiro e terceiro quantis ou quantis de 25% e 75% respectivos são registrados com os nomes 1st Qu. e 3rd Qu. e, finalizando, o resumo de informações de estatísticas de posição temos os valores de medianas (Median) e médios (Mean). Exemplo 2.23 (Rock ) Medições em 48 amostras de rochas de um reservatório de petróleo estão disponíveis no arquivo de dados rock. Este conjunto de dados contem 48 linhas e 4 colunas numéricas, descritas a seguir: area área do espaço de poros, em pixels de 256 por 256; peri perímetro em pixels; shape perímetro/sqrt(area) perm permeabilidade em mili-Darcies. Doze amostras do nu´cleo de reservatórios de petróleo foram amostrados por 4 seções transversais. Cada amostra foi medida no nu´cleo para a permeabilidade e cada seção transversal tem uma área total de poros, perímetro total de poros e forma. A fonte destes dados é a BP Research e a análise das imagens foi de Ronit Katz, Oxford University. Na geologia, a permeabilidade é a medida da capacidade de um material (tipicamente uma rocha) para transmitir fluídos. E´ de grande importância na determinação das características de fluxo dos hidrocarbonetos em reservatórios de petróleo e gás e da água nos aquíferos. A unidade de permeabilidade é o Darcy ou, mais habitualmente, o mili- Darcy ou mD.\n2.4.1 Gráfico de Boxplot Em 1977, John Tukey (Tukey, 1977) publicou uma proposta que posteriormente foi reconhecida como sendo um eficiente método para mostrar cinco nu´mero que sumarizam qualquer conjunto de dados. O gráfico proposto é chamado de boxplot (também conhecido como box and whisker plot) e resume as seguintes medidas de posição estatísticas: mediana, quantis inferior e superior e os valores mínimos e máximos. Os quantis inferior e superior entendem-se serem os quantis de 25% e 75%, respectivamente. No caso do exemplo 2.22, deixamos a disposição os dados digitando attach(Puromycin) e com isso podemos mudar o nome dos níveis do fator da forma state=factor(state,labels=c(’Tratada’,’N~ao tratada’)) Então, com os comandos a seguir geramos o gráfico de boxplot, tanto para a varia´vel rate quanto para a variável conc, estas segundo os níveis do fator state. par(mar=c(5,4,3,1)) boxplot(rate ~ state, col = grey(c(0.4,1)), main=’Taxas de reaç~ao instant^anea’)\npara o caso do rate. Observemos que a primeira linha par(mar=c(5,4,3,1)) serve somente para dimensionar a janela gráfica. Para o caso da variável conc utilizamos comandos semelhantes. par(mar=c(5,4,3,1)) boxplot(conc ~ state, col = grey(c(0.4,1)), main=’Concentraç~oes de substrato’) O resultado deste trabalho pode ser observado na Figura 2.3. Interpretemos o gráfico de boxplot. A caixa (box) propriamente contém a metade 50% dos data. O limite superior da caixa indica o percentil 75% dos dados e o limite inferior da caixa indica o percentil 25%. A distancia entre esses dois quantis é conhecida como inter-quantil. A linha na caixa indica o valor de mediana dos dados. Se a linha mediana dentro da caixa não é equidistante dos extremos, diz-se então que os dados são assimétricos. O boxplot da variável rate (esquerda na Figura 2.3) é um exemplo de dados simétricos já a situação da variável conc (direita na Figura 2.3) é um caso clássico de assimetria dos dados. Os extremos do gráfico indicam os valores mínimo e máximo, a menos que valores outliers3 estejam presentes, nesse caso o gráfico de estende ao máximo de 1.5 vezes da distância inter-quantil. Os pontos fora do gráfico são então outliers ou suspeitos de serem outliers. Mais elegante seria utilizar a biblioteca de funções ggplot2, para isso, digitamos: library(ggplot2) Para gerar os gráficos de boxplot respectivos, fazemos: par(mar=c(5,4,3,1)) qplot(state, rate, geom=c(“boxplot”, “jitter”), main=“Taxas de reaç~ao instant^anea”, xlab=““, ylab=” “) e par(mar=c(5,4,3,1)) qplot(state, conc, geom=c(”boxplot”, “jitter”), main=“Concentraç~oes de substrato”, xlab=““, ylab=” “)\n3Em estatística, outlier, valor aberrante ou valor atípico, é uma observação que apresenta um grande afastamento das demais observações em uma amostra. A existência de outliers implica, tipicamente, em prejuízos a interpretação dos resultados dos testes estatísticos aplicados as amostras.\nTaxas de reação instantânea Concentrações de substrato\nTratada Não tratada Tratada Não tratada\nFigura 2.3: Gráfico de boxplot da variável rate à esquerda e da variável conc à direita, segundo os níveis do fator state, se a enzima foi tratada ou não com puromicida. Gráfico gerado utilizando a função R boxplot,\nTaxas de reação instantânea Concentrações de substrato\n200\n0.9\n150\n0.6\n100 0.3\n50\nTratada Não tratada\n0.0\nTratada Não tratada\nFigura 2.4: Gráfico de boxplot da variável rate à esquerda e da variável conc à direita, segundo os níveis do fator state, se a enzima foi tratada ou não com puromicida. Gráfico gerado utilizando a função R qplot, opção geom=c(”boxplot”, ”jitter”).\nobtendo-se assim os gráficos na Figuras 2.4. Além de melhor qualidade gráfica acrescentamos os pontos observados no boxplot, isso permite termos uma ideia também da dispersão dos dados. Vejamos as vantagens do boxplots. Mostra graficamente a posição central dos dados (mediana) e a tendência. Fornece algum indicativo de simetria ou assimetria dos dados. Ao contrário de muitas outras formas de mostrar os dados, o boxplots mostra os outliers. Utilizando o boxplot para cada varia´vel categórica no mesmo gráfico, pode-se facilmente comparar os dados. Esta é a situação no exemplo na Figura 2.3, podemos observar o comportamento das variáveis rate e conc segundo os níveis do fator state. Um detalhe do boxplot é que ele tende a enfatizar as caudas da distribuição, que são os pontos ao extremo nos dados. Também fornece detalhes da distribuição dos dados. Mostrar o histograma (Seção 2.4.2) em conjunto com o boxplot ajuda a entender a distribuição dos dados, constituindo estes dos gráficos ferramentas importantes na análise exploratória. Logicamente, o comportamento dos dados dentro da caixa (box), como podemos perceber nas figuras 2.3 e 2.4, permanece um mistério. Isso porque caso estejam os dados bem espalhados ou não, o gráfico boxplot continua mostrando uma caixa. Somente perceberemos algum comportamento diferente se o valor da mediana estiver mais próximo de um dos extremos desta caixa. Para tentar diminuir essa limitação foi sugerido uma melhoria, obtendo-se o chamada boxplot entalhado (notched boxplot). Com as linhas de comando a seguir se obtém os gráficos na Figura 2.5.\npar(mar=c(5,4,3,1)) boxplot(rate ~ state, col = grey(c(0.4,1)), notch=TRUE, main=’Taxas de reaç~ao instant^anea’)\ne par(mar=c(5,4,3,1)) boxplot(conc ~ state, col = grey(c(0.4,1)), notch=TRUE, main=’Concentraç~oes de substrato’)\nObserva-se que a u´nica diferença é a inclusão da opção notch=TRUE, permanecendo todas as outras instruções iguais. Mais elaborado é o chamado violin plot, mistura de boxplot com estimação de densidade, tema este tratado na Seção 4.3. Este gráfico, introduzido no artigo Hintze & Nelson (1998), sinergicamente combina o gráfico de boxplot e a estimação da densidade, também chamado de histograma suavizado, em uma u´nica tela que revela a estrutura encontrada nos dados. Com as linhas de comando a seguir se obtém os gráficos na Figura 2.6.\npar(mar=c(5,4,3,1)) qplot(state, rate, geom = c(“violin”, “jitter”), notch=TRUE, main=“Taxas de reaç~ao instant^anea”, xlab=““, ylab=” “)\ne par(mar=c(5,4,3,1)) qplot(state, conc, geom=c(“violin”, “jitter”), notch=TRUE, main=“Concentraç~oes de substrato”, xlab=““, ylab=” “)\nEste gráfico é similar ao boxplot excepto que mostra também a densidade de probabilidade dos dados. Pode incluir também um marcador para a média dos dados e uma caixa que indica a distância interquartil, como nos gráficos boxplot. O objetivo do gráfico violin plot é o mesmo do que o boxplot original porém, considera de alguma maneira o comportamento dos dados dentro da caixa (box). Assim, percebemos melhor a distribuição dos dados dentro do intervalo interquartil.\nTaxas de reação instantânea Concentrações de substrato\nTratada Não tratada Tratada Não tratada\nFigura 2.5: Gráfico de boxplot entalhado da variável rate à esquerda e da variável conc à direita, segundo os níveis do fator state, se a enzima foi tratada ou não com puromicida. Gráfico gerado utilizando a função R qplot, opção geom=c(”boxplot”, ”jitter”), notch=TRUE.\nTaxas de reação instantânea Concentrações de substrato\n200\n0.9\n150\n0.6\n100 0.3\n50\nTratada Não tratada\n0.0\nTratada Não tratada\nFigura 2.6: Gráfico de violin plot da variável rate à esquerda e da variável conc à direita, segundo os níveis do fator state, se a enzima foi tratada ou não com puromicida. Gráfico gerado utilizando a função R qplot, opção geom=c(”vioplot”, ”jitter”), notch=TRUE.\n2.4.2 Histograma Um histograma é uma representação gráfica da função de probabilidades ou da função de densidade de um conjunto de dados independentes e foi introduzido pela primeira vez por Karl Pearson4. A representação mais comum do histograma é um gráfico de barras verticais. A palavra histograma é de origem grega, derivada de duas: histos que pode significar testemunha no sentido de aquilo que se vê, como as barras verticais do histograma, e da também palavra grega gramma que significa desenhar, registrar ou escrever. Histograma Histograma com a curva norma\n−2 −1 0 1 2 Dados simulados\n−2 −1 0 1 2 Dados simulados\nFigura 2.7: Gráfico de histograma para dados simulados.\nPara construir um exemplo controlado do gráfico de histograma, simulamos uma amostra de tamanho 150 da distribuição normal padrão, com o comando x=rnorm(150) e, depois, construímos um gráfico colorido com as linhas de comando par(mar=c(5,4,2,1)) hist(x, breaks=12, col=“red”, xlab=“Dados simulados”, ylab=’Frequ^encia’, main=“Histograma”) box() Posteriormente, acrescentamos a este gráfico uma linha com a densidade normal par(mar=c(5,4,2,1)) h=hist(x, breaks=10, col=“red”, xlab=“Dados simulados”, ylab=’Frequ^encia’, main=“Histograma com a curva normal”) xfit=seq(min(x),max(x),length=40) yfit=dnorm(xfit,mean=mean(x),sd=sd(x)) yfit=yfitdiff(h$mids[1:2])length(x) lines(xfit, yfit, col=“blue”, lwd=2) box()\n4Pearson, K. (1895). Contributions to the Mathematical Theory of Evolution. II. Skew Variation in Homogeneous Material. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 186: 343-414.\nDesta forma geramos os gráficos na Figura 2.7. A ideia é mostrar que o histograma assemelha-se ao gráfico da densidade normal, a densidade dos dados.\nHistograma c2(6) Histograma c2(6)\n2 4 6 8 10 12 14 14 intervalos\n2 4 6 8 10 12 14 26 intervalos\nFigura 2.8: Histogramas da distribuição χ2 com 6 graus de liberdade. Nu´mero de intervalos 14 e 26, respectivamente.\nO histograma é um gráfico composto por retângulos justapostos em que a base de cada um deles corresponde ao intervalo de classe e a sua altura à respectiva frequência. A construção de histogramas tem caráter preliminar em qualquer estudo e é um importante indicador da distribuição de dados. Pode indicar se uma distribuição aproxima-se de uma densidade normal como pode indicar mistura de densidades, quando os dados apresentam várias modas. Os histogramas podem ser um mau método para determinar a forma de uma distribuição porque são fortemente influenciados pelo nu´mero de intervalos utilizados. Por exemplo, decidimos gerar 50 amostras da densidade χ2(6), da forma set.seed(5678) z=rchisq(50, df=6) Os gráficos de histogramas correspondentes com 14 e 26 intervalos são apresentados na Figura 2.8 e foram gerados com as linhas de comando\npar(mar=c(5,4,2,1)) hist(z, breaks=14, col=“blue”, main=expression(paste(’Histograma ’, chi^2,’(6)’)), ylab=’Frequ^encia’, xlab=’14 intervalos’) box()\ne\npar(mar=c(5,4,2,1)) hist(z, breaks=26, col=“blue”, main=expression(paste(’Histograma ’, chi^2,’(6)’)), ylab=’Frequ^encia’, xlab=’26 intervalos’) box()\nNa Figura 2.9 podemos observar os gráficos de histograma obtidos das variáveis descritas no Exemplo 2.23. A situação em (a) representa o caso em a distribuição dos dados de assemelha à distribuição normal, já a situação descrita no gráfico em (b) mostra-se uma mistura de densidades, percebemos a existência de duas modas. (a) Área do espaço de poros (b) Perímetro em pixels\n0 4000 8000 12000 pixels de 256 x 256\n0 1000 3000 5000\nFigura 2.9: Histogramas das varia´veis no Exemplo 2.23.\nOutras situações no mesmo exemplo, mas diferentes variáveis, são descritas nos gráficos na Figura 2.10. Nessa figura apresentamos dois gráficos, chamados de (c) e (d), nesta figura. Correspondem, como podemos observar, à distribuições assimétricas e descrevem os dados coletados nas variáveis shape e perm do arquivo de dados Rock, Exemplo 2.23. Os histogramas foram pensados somente para o caso de variáveis contínuas, porém é uma descrição discreta delas. Logicamente, também podemos utiliza-los em situações de varia´veis aleatórias discretas, nada impede isso. Estas figuras foram geradas utilizando a configuração padrão do comando hist, isto é, utilizamos uma maneira automática de determinar o nu´mero de intervalos, mais adiante dedicamos maior atenção a diferentes formas de calcular este nu´mero. Como pode ter sido observado, além de não ficar claro como determinar o nu´mero de intervalos nem como delimitar os intervalos, também não ficou claro o que queremos realmente observar com o gráfico desta função. Vejamos agora uma definição mais clara do histograma, esta definição nos permitirá obter propriedades impor- tantes.\n0.1 0.2 0.3 0.4 0.5\n0 200 600 1000 1400 mili−Darcies\nFigura 2.10: Histogramas das variáveis no Exemplo 2.23.\nFoi provado por Robertson (1967) que, dados os intervalos I1, I2, · · · , Ik, o histograma f é um estimador de máxima verossimilhança5 dentre os estimadores expressados como funções simples e semicontínuas superiormente, isto se o fecho de cada intervalos contiver duas ou mais observac¸ões. Os gráficos apresentados nas figuras 2.7, 2.9 e 2.10 são histogramas também segundo a proposta de Robertson (1967). Pode-se observar que este estimador tem duas limitações importantes: a dependência do comprimento do intervalo e o fato de o histograma não constituir uma função contínua. A primeira destas limitações foi amplamente estudada por Wegman (1975). Ele provou que os pontos extremos de cada intervalo Ik devem ser coincidentes com observações e que, se o nu´mero mínimo de observações em cada intervalo aumente, conforme aumenta o tamanho da amostra, o estimador f é consistente6. A segunda limitação importante do histograma, isto é, o fato de ele não constituir uma função contínua, incentivou diversos estudos na procura de estimadores contínuos da função de densidade. No Capítulo 3, a Seção 4.3 dedica-se a mostrar estimadores contínuos da função de densidade.\n5Os estimadores de máxima verossimilhanc¸a serão estudados na Seção 4.2 6Estimadores consistentes serão estudados na Seção 3.1.1\nCálculo automático do nu´mero de intervalos num histograma Uma questão importante é determinar de maneira automatizada o nu´mero de intervalos disjuntos que serão utili- zados para a construção do gráfico. Uma primeira forma de escolher o nu´mero de intervalos foi dada por Sturges (1926) e que constitui a forma padrão no R. Conhecida como fórmula de Sturges é dada por k = [log2(n) + 1], (2.41) isto significa que o nu´mero de intervalos é a parte inteira do logaritmo base 2 do nu´mero de observações mais 1. Outras expressões comumente utilizadas são a fórmula de Scott (Scott, 1979) h = 3.5s/√3 n, onde s é o desvio padrão e a fórmula de Freedman Diacconi (Freedman & Diaconis, 1981) h = 2IQR(x)/√3 n, onde IRQ é a diferença entre o terceiro e o primeiro quantil. Exemplo 2.24 Na libraria de funções R robustbase temos disponíveis dados do teor de cálcio e do pH em amostras de colo coletadas em diferentes comunidades da região de Condroz, na Bélgica. Podemos ler estes dados digitando as linhas de comando abaixo, primeiro para escolher a libraria de funções e depois para selecionar os dados. library(robustbase) data(condroz) Temos registadas duas variáveis: Ca que registra o tero de cálcio na amostra de solo e o pH, o pH corres- pondente. Construímos histogramas da variável Ca segundo a três formas de escolha do nu´mero de intervalos e os apresentamos na Figura 2.11. Os dados deste exemplo foram publicados em: Hubert, M. and Vandervieren, E. (2006). An Adjusted Boxplot for Skewed Distributions, Technical Report TR-06-11, KULeuven, Section of Statistics, Leuven.\nSturges\nScott\nFreedman−Diaconis\n0 1000 2000 3000 4000 Ca\n0 1000 2000 3000 4000 Ca\n0 1000 2000 3000 4000 Ca\nFigura 2.11: Diferentes histogramas da variável Ca no Exemplo 2.24.\n2.4.3 Gráficos para verificar normalidade Um primeiro gráfico chamado de qq-norm permite a comparação de duas distribuições de probabilidades traçando seus quantis uns contra os outros. Depois exploramos um gráfico mais recente, conhecido como worm plot (gráfico de minhoca), consistindo numa determinada coleção de de qq-norm.\nQQ-norm O gráfico quantil-quantil ou qq-plot, proposto por Wilk & Gnanadesikan (1968), é um dispositivo gráfico explo- ratório utilizado para verificar a validade de um pressuposto de distribuição para um conjunto de dados. Em geral, a ideia básica é a de calcular o valor teoricamente esperado para cada ponto de dados com base na distribuição em questão. Se os dados de fato seguirem a distribuição assumida os pontos deste gráfico formarão aproximadamente uma linha reta. Percebemos que podemos verificar com este gráfico qualquer densidade contínua, eventualmente pode ser uti- lizado também para funções de probabilidade. O qq-plot vai apresentar-se como uma linha reta se a densidade assumida estiver correta. Vejamos o caso particular de verificarmos se a densidade é normal, nesta situação o gráfico qq-plot será chamado de qq-norm. Primeiro consideraremos a situação da densidade normal padrão. Seja z1, z2, · · · , zn uma amostra aleatória de uma distribuição normal com média µ = 0 e desvio padrão σ = 1. As estatísticas de ordem amostrais são z(1) ≤ z(2) ≤ · · · ≤ z(n)· Estes valores desempenharão o papel dos quantis da amostra. Agora, quais devemos tomar como os quantis teóricas correspondentes? Se a função de distribuição cumulada da densidade normal padrão fosse denotada por Φ, usando a notação quantil, se ξq é o q-ésimo quantil de uma distribuição normal, então Φ(ξq) = q, ou seja, a probabilidade de uma amostra normal ser inferior a ξq é, de fato, apenas q. Considere o primeiro valor ordenado z(1). O que podemos esperar que o valor Φ(z(1)) seja? Intuitivamente, esperamos que essa seja a probabilidade de assumir um valor no intervalo (0, 1/n). Do mesmo modo, espera-se que Φ(z(2)) seja a probabilidade de assumir um valor no intervalo (1/n, 2/n). Continuando, esperamos que Φ(z(n)) seja a probabilidade de assumir um valor no intervalo (n 1)/n, 1). Assim, o quantil teórico desejamos seja definido pelo inverso da função de distribuição acumulada normal padrão. Em particular, o quantil teórico correspondente ao quantil empírico z(i) deve ser\npara i = 1, 2, · · · , n.\nξ = q i − 0, 5 , q n\nQQ−plot nomal\nQQ−plot nomal\nQQ−plot nomal\n−3 −2 −1 0 1 2 3 Quantis teóricos\n−3 −2 −1 0 1 2 3 Quantis teóricos\n−3 −2 −1 0 1 2 3 Quantis teóricos\nFigura 2.12: Diferentes qqplot para dados normais.\nNa Figura 2.12, a esquerda acima exibimos o qq-norm de uma pequena amostra normal de tamanho 5. Os restantes quadros na Figura 2.12 exibem as plotagens de qq-norm para amostras normais de tamanhos n = 100 e\nn = 1000, respectivamente. Como o tamanho da amostra aumenta, os pontos encontram-se mais perto da linha y = x. Estes gráficos (Figura 2.12) foram gerados utilizando as linhas de comando: set.seed(1278) x=rnorm(5) qqnorm(x, xlim=c(-3,3), ylim=c(-3,3), cex=0.6, pch=19, ylab=’Quantis amostrais’, xlab=’Quantis teóricos’, main=’QQ-plot nomal’) qqline(x,col=“red”) text(-1,2,’n=5’) para a situação de amostra de tamanho 5. A primeira linha de comando serve para fixar o gerador de nu´meros laetórios e, dessa forma, podermos simular sempre a mesma amostra e reproduzir o gráfico idêntico. Nas outras situações somente muda-se o tamanho da amostra que se quer gerar.\nQQ−plot nomal QQ−plot nomal\n−3 −2 −1 0 1 2 3 Quantis teóricos\n−3 −2 −1 0 1 2 3 Quantis teóricos\nFigura 2.13: Diferentes qqplot para dados não normais. Assim, os comandos para gerar o segundo e terceiro gráficos são: x=rnorm(100) qqnorm(x, xlim=c(-3,3), ylim=c(-3,3), cex=0.6, pch=19, ylab=’Quantis amostrais’, xlab=’Quantis teóricos’, main=’QQ-plot nomal’) qqline(x,col=“red”) text(-1,2,’n=100’)\nx=rnorm(1000) qqnorm(x, xlim=c(-3,3), ylim=c(-3,3), cex=0.6, pch=19, ylab=’Quantis amostrais’, xlab=’Quantis teóricos’, main=’QQ-plot nomal’) qqline(x,col=“red”) text(-1,2,’n=1000’) Caso os dados não forem padronizados bastar aplicar a transformação (X − µ)/σ, onde X representa os dados originais e µb e σb representam os estimadores dos parâmetros µ e σ, respectivamente.\nEstes gráficos podem indicar afastamentos da normalidade por isso apresentamos duas situações de dados não simétricos e com cuadas pesadas. Na Figura 2.13, mostramos o que acontece se os dados forem da distribuição t-Student(8) e da distribuição χ2(5), sempre de tamanho n = 1000. Observe, em particular, que os dados a partir da distribuição t-Student seguem a curva normal bem de perto até os u´ltimos pontos em cada extremo. Na outra situação o afastamento da distribuição normal é evidente. Foi mencionado que o qq-norm é uma situação particular do qq-plot devido a este u´ltimo permitir comparar os quantis amostrais com os quantis distribucionais. Com isto queremos dizer que o qq-plot serve para verificar se os dados forem t-Student ou χ2(5), por exemplo. Na Figura 2.14 apresentamos a aparência dos gráficos qq-plot caso queira-se verificar se as amostras seguem distribuição t-Student(8) ou χ2(5), respectivamente.\nQQ plot para t−Student(8)\n−4 −2 0 2 4 t−Student(8)\nQQ plot para c2(5)\n0 5 10 15 20 c2(5)\nFigura 2.14: Diferentes qqplot para dados não normais. Os gráficos na Figura 2.14 foram gerados pelas linhas de comandos qqplot(qt(ppoints(1000), df = 8), x, cex=0.6, pch=19, main = “QQ plot para t-Student(8)”, xlab=“t-Student(8)”) qqline(x, distribution = function(p) qt(p, df = 8), prob = c(0.1, 0.6), col = 2) no caso t-Student(8) e qqplot(qchisq(ppoints(1000), df = 5), x, cex=0.6, pch=19, main = expression(“QQ plot para” ~~ {chi^2}(5)), xlab=expression({chi^2}(5))) qqline(x, distribution = function(p) qchisq(p, df = 5), prob = c(0.1, 0.6), col = 2) para o caso χ2(5).\nWorn plot O worm-plot é uma série de parcelas de gráficos qq-plot retificados. Constitui uma ferramenta de diagnóstico para visualização de quão bem um modelo estatístico se ajusta aos dados, para encontrar locais em que o ajuste pode ser melhorado e para comparar o ajuste de diferentes modelos. Na Figura 2.15 mostramos este gráfico para duas situações: a esquerda os dados são normais e a direita os dados são t-Student com 8 graus de liberdade. Nesta situação aparece bem a qualidade da observação com esta\n−4 −2 0 2 4 Unit normal quantile\n−4 −2 0 2 4 Unit normal quantile\nFigura 2.15: Diferentes worm-plot para dados normais.\nfigura. Se os dados forem normais o curva worm-plot ou gráfico de minhoca deve aparentar um verme achatado, os pontos próximos a curva vermelha e com poucas oscilações. Quando aplicamos este gráfico ao caso t-Student percebemos uma oscilação grande no verme e com pontos fugindo da banda de confiança. Isso comprova que os dados não seguem como referência a distribuição normal.\n−4 −2 0 2 4 Unit normal quantile\n−4 −2 0 2 4 Unit normal quantile\nFigura 2.16: Diferentes worm-plot para dados não normais. As linhas a seguir mostram os comandos necessários para gerar os gráficos na Figura 2.15. Utilizamos a libraria de comandos R gamlss (Rigby & Stasinopoulos, 2005).\nlibrary(gamlss) x=rnorm(1000) wp(gamlss(x~1), cex=0.6) x=rt(1000, df=8) wp(gamlss(x~1), cex=0.6)\nNa Figura 2.16, a esquerda temos o caso de dados com distribuição χ2(5) e a direita dados com distribuição Cauchy padrão. Nestas situações fica claro que os dados não são normais. Oa gráficos na figura foram gerados pelas linhas de comando a seguir. x=rchisq(1000, df=5) wp(gamlss(x~1), cex=0.6) x=rcauchy(1000) wp(gamlss(x~1), cex=0.6)\n2.5. EXERC´ICIOS 101 2.5 Exercícios Exercícios da Seção 2.1 1. Seja X ∼ Bernoulli( 1 ) e considere todas as possíveis amostras aleatórias de tamanho n = 3. Calcule Xn e S2 cada uma das\n2 n oito amostras. Encontre a função de probabilidade de Xn e S2. 2. Um dado é lançado. Seja X o valor da face superior que aparece e X1, X2 duas observações independentes de X. Encontre a função de probabilidade de Xn. 3. Seja X1, · · · , Xn uma amostra aleatória de alguma população. Mostre que (n − 1)Sn\nmax |Xi Xn| &lt; 1≤i≤n\nonde Sn é a raiz quadrada positiva da variância amostral S2.\n√n ,\nExercícios da Seção 2.2 1. Seja (X(1), X(2), · · · , X(n)) o conjunto das estatísticas de ordem de n variáveis aleatórias independentes X1, X2, · · · , Xn com função de densidade comum\nf (x) =\nβe−xβ, se x 0 · 0, caso contrário\nf (x) =\nσ 0, se x ≤\nMostre que X(1), X(2) − X(1), X(3) − X(2), · · · , X(n) − X(n−1) são independentes. 6. Sejam X1, X2, · · · , Xn variáveis aleatórias independentes e igualmente distribuídas com função de distribuição acumulada comum\nF (t) =\ntα, se 0 &lt; t &lt; 1  1, se t ≥ 1\npara α &gt; 0. Mostre que X(i)/X(n), i = 1, 2, · · · , n − 1 e X(n) são independentes. 7. Sejam X1 e X2 duas variáveis aleatórias discretas independentes com função de probabilidade comum P (X = x) = (1 − )x−1, x = 1, 2, · · · ; 0 &lt; &lt; 1· Mostre que X(1) e X(2) − X(1) são independentes. 8. Sejam X1, · · · , Xn duas variáveis aleatórias independentes com função de densidade comum f . Encontre a função de densidade de X(1) e de X(n).\nEncontre E(Mn) em cada uma das seguintes situações: a) Xk tem como função de distribuição comum F (x) = 1 − e−xβ, se x ≥ 0. b) Xk tem como função de distribuição comum F (x) = x, se 0 &lt; x &lt; 1.\nExercícios da Seção 2.3 1. Demonstre o Corolário 2.17. 2. Demonstre o Corolário 2.18.",
    "crumbs": [
      "Módulo I",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estatísticas de ordem</span>"
    ]
  },
  {
    "objectID": "Modulo I/Aula 02/aula02.html#momentos-amostrais",
    "href": "Modulo I/Aula 02/aula02.html#momentos-amostrais",
    "title": "2  Estatísticas de ordem",
    "section": "",
    "text": "n!  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1    ∫ +c   x   \n\n\n\n\n\n\n1    ∫ +c\n\nc   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n= 1 etXi · n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 + ∑ ∑\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nn(n − 1)µ2] = ( n − 1 ) (m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 3 ∑ ∑\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(n − 1) (n − 2 + 3 µ 2 —\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n= \n\n\n\n\n\n\n\n\n\n= \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ne−h(1 − µ )]2 ·\nµi\nNa segunda derivada, escolhendo u = µ + e−h(1 − µ ) 0 &lt; u &lt; 1. Portanto, f ′′(h) ≤ 1 . Pela série de Taylor\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n√n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n− F\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerímetro/sqrt(Área) (d) Permeabilidade\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMostre que X(s) e X(r) − X(s) são independentes para quaisquer r &gt; s.\nEncontre a função de densidade de X(r+1) − X(r).\nSeja Z1 = nX(1), Z2 = (n − 1)(X(2) − X(1)), Z3 = (n − 2)(X(3) − X(2)), …, Zn = ((X(n) − X(n−1))). Prove que (Z1, Z2, · · · , Zn) e (X1, X2, · · · , Xn) são identicamente distribuídas.\n\n\nProvar o Teorema 2.1\nSejam X1, X2, · · · , Xn variáveis aleatórias com distribuição geométrica de parâmetros p1, p2, · · · , pn, respectivamente. Prove que Nn = min(X1, X2, · · · , Xn) têm também distribuição geométrica de parâmetro n p = 1 − (1 − pi)· i=1\nAs X1, · · · , Xn variáveis aleatórias independentes e identicamente distribuídas tem por função de probabilidade BN (1; p) se, e somente se, Nn = min(X1, · · · , Xn) tem distribuição geométrica de parâmetro 1 − (1 − p)n.\nSejam X1, X2, · · · , Xn variáveis aleatórias independentes e igualmente distribuídas com função de densidade comum\n\n\n\n\n\n\n\n\nSejam X(1), X(2), · · · , X(n) as estatísticas de ordem de n variáveis aleatórias independentes e igualmente distribuídas X1, X2, · · · , Xn com função de densidade comum f (x) = 1 se 0 &lt; x &lt; 1 · 0, caso contrário Prove que Y1 = X(1)/X(2), Y2 = X(2)/X(3), · · · , Yn−1 = X(n−1)/X(n) e Yn = X(n) são independentes. Encontre a função de densidade conjunta de Y1, Y2, · · · , Yn.\nSejam X1.X2, · · · , Xn variáveis aleatórias independentes identicamente distribuídas não negativas contínuas. Prove que se E|X| &lt; ∞, então E|X(r)| &lt; ∞. Definamos Mn = X(n) = max(X1, X2, · · · , Xn). Mostre que ∫ ∞\n\n\n\nProvar que, qualquer seja a amostra aleatória X1.X2, · · · , Xn sempre cumpre-se que X(1) ≤ X ≤ X(n).\nDemonstrar o Teorema 2.5.\nDemonstrar o Teorema 2.9.\n\n\n\nSeja X1, · · · , Xn uma amostra aleatória Poisson(). Encontre Var(S2) e compare-a com Var(X). Observe que E(X) = = E(S2).\nSeja X1, X2, · · · , Xn uma amostra aleatória da função de distribuição F e seja F ∗(x) a função de distribuição amostral. Encontre Cov[F ∗(x), F ∗(y)] para nu´meros reais fixos x, y. n n\nSeja F ∗ a função de distribuição empírica de uma amostra aleatória com função de distribuição teórica F . Prove que { ∗ ϵ } 1\nSejam X1, X2, · · · , Xn n observacões independentes da variável aleatória X. Encontre a distribuição amostral de X, a média amostral, se:\n\n\nX ∼ P ();\nX ∼ Cauchy(1, 0);\nX ∼ χ2(m).\n\n\nSeja X1, · · · , Xn uma amostra aleatória Poisson(). Encontre Var(S2) e compare-a com Var(X). Observe que E(X) = = E(S2).\nDemonstre o Teorema 2.23. [Dica: para quaisquer reais µ e σ &gt; 0, encontre a função de densidade de (U(r) − µ)/σ e mostre que as variáveis padronizadas de U(r), (U(r) − µ)/σ, são assintoticamente N (0, 1) sob as condições do teorema.]\nProvar que o momentos amostral central b1 é sempre zero.",
    "crumbs": [
      "Módulo I",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estatísticas de ordem</span>"
    ]
  }
]