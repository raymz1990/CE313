<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-03-01">

<title>CE313 - Estatística Não-Paramétrica - 2&nbsp; Estatísticas de Ordem</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../ModuloI/Aula03/index.html" rel="next">
<link href="../../ModuloI/Aula01/index.html" rel="prev">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../custom.css">
<meta property="og:title" content="CE313 - Estatística Não-Paramétrica - 2&nbsp; Estatísticas de Ordem">
<meta property="og:description" content="Material da disciplina CE313 - ESTATISTICA NÃO PARAMÉTRICA do curso de Estatística da Universidade Federal do Paraná (UFPR).">
<meta property="og:image" content="non-parametric.pngs">
<meta property="og:site_name" content="CE313 - Estatística Não-Paramétrica">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../ModuloI/index.html">Módulo I</a></li><li class="breadcrumb-item"><a href="../../ModuloI/Aula02/index.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Estatísticas de Ordem</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">CE313 - Estatística Não-Paramétrica</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://raymz1990.github.io/CE313/tree/main/book/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-git"></i></a>
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="../../CE313---Estatística-Não-Paramétrica.pdf">
              <i class="bi bi-bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="../../CE313---Estatística-Não-Paramétrica.docx">
              <i class="bi bi-bi-file-word pe-1"></i>
            Download Docx
            </a>
          </li>
      </ul>
    </div>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-1" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-1">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=|url|">
              <i class="bi bi-bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
      </ul>
    </div>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Apresentação</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../ModuloI/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Módulo I</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../ModuloI/Aula01/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Estatística Não Paramétrica</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../ModuloI/Aula02/index.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Estatísticas de Ordem</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../ModuloI/Aula03/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Estatística Não Paramétrica 2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../ModuloI/Aula04/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">index.html</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#amostras-aleatórias" id="toc-amostras-aleatórias" class="nav-link active" data-scroll-target="#amostras-aleatórias"><span class="header-section-number">2.1</span> Amostras aleatórias</a></li>
  <li><a href="#estatísticas-de-ordem" id="toc-estatísticas-de-ordem" class="nav-link" data-scroll-target="#estatísticas-de-ordem"><span class="header-section-number">2.2</span> Estatísticas de ordem</a>
  <ul class="collapse">
  <li><a href="#propriedades-das-estatísticas-de-ordem" id="toc-propriedades-das-estatísticas-de-ordem" class="nav-link" data-scroll-target="#propriedades-das-estatísticas-de-ordem"><span class="header-section-number">2.2.1</span> Propriedades das estatísticas de ordem</a></li>
  </ul></li>
  <li><a href="#pg8" id="toc-pg8" class="nav-link" data-scroll-target="#pg8"><span class="header-section-number">2.3</span> pg8</a>
  <ul class="collapse">
  <li><a href="#quantis" id="toc-quantis" class="nav-link" data-scroll-target="#quantis"><span class="header-section-number">2.3.1</span> 2.2.2 Quantis</a></li>
  </ul></li>
  <li><a href="#momentos-amostrais" id="toc-momentos-amostrais" class="nav-link" data-scroll-target="#momentos-amostrais"><span class="header-section-number">2.4</span> Momentos amostrais</a></li>
  <li><a href="#gráficos-descritivos" id="toc-gráficos-descritivos" class="nav-link" data-scroll-target="#gráficos-descritivos"><span class="header-section-number">2.5</span> Gráficos descritivos</a>
  <ul class="collapse">
  <li><a href="#gráfico-de-boxplot" id="toc-gráfico-de-boxplot" class="nav-link" data-scroll-target="#gráfico-de-boxplot"><span class="header-section-number">2.5.1</span> 2.4.1 Gráfico de Boxplot</a></li>
  <li><a href="#histograma" id="toc-histograma" class="nav-link" data-scroll-target="#histograma"><span class="header-section-number">2.5.2</span> 2.4.2 Histograma</a></li>
  <li><a href="#gráficos-para-verificar-normalidade" id="toc-gráficos-para-verificar-normalidade" class="nav-link" data-scroll-target="#gráficos-para-verificar-normalidade"><span class="header-section-number">2.5.3</span> 2.4.3 Gráficos para verificar normalidade</a></li>
  </ul></li>
  <li><a href="#exercícios" id="toc-exercícios" class="nav-link" data-scroll-target="#exercícios"><span class="header-section-number">2.6</span> Exercícios</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://raymz1990.github.io/CE313/edit/main/book/ModuloI/Aula02/index.qmd" class="toc-action"><i class="bi bi-git"></i>Edit this page</a></li><li><a href="https://raymz1990.github.io/CE313/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../ModuloI/index.html">Módulo I</a></li><li class="breadcrumb-item"><a href="../../ModuloI/Aula02/index.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Estatísticas de Ordem</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Estatísticas de Ordem</span></h1>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">March 1, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>Quando estudadas as idéias e técnicas da teoria das probabilidades fundamentais, criamos um modelo matemático de um ensaio aleatório, associando-o com um espaço de amostragem no qual os acontecimentos aleatórios correspondem a um conjunto de uma certa <span class="math inline">\(\sigma\)</span>-álgebra. A noção de probabilidade definida nesta <span class="math inline">\(\sigma\)</span>-álgebra corresponde à noção de incerteza no resultado em qualquer realização do experimento aleatório.</p>
<p>Vamos começar o estudo de alguns problemas de estatística matemática. Suponha que buscamos informações sobre algumas características numéricas de um conjunto de elementos, chamado de uma população. Por razões de custo, de tempo ou simplesmente para não destruir todos os elementos amostrados podemos não querer ou não poder estudar cada elemento da população. Nosso objetivo é tirar conclusões sobre as características desconhecidas da população com base em informações sobre algumas características de uma amostra adequadamente selecionada.</p>
<p>Formalmente, seja <span class="math inline">\(X\)</span> uma variável aleatória que descreve a população sob investigação e seja <span class="math inline">\(F\)</span> a função de distribuição de <span class="math inline">\(X\)</span>. Há duas possibilidades: ou <span class="math inline">\(X\)</span> tem função de distribuição <span class="math inline">\(F(\cdot ; \theta)\)</span> com uma forma funcional conhecida exceto, talvez, para o parâmetro <span class="math inline">\(\theta\)</span>, o qual pode ser um vetor ou <span class="math inline">\(X\)</span> tem uma função de distribuição <span class="math inline">\(F\)</span> sobre a qual não sabemos nada, exceto talvez que <span class="math inline">\(F\)</span> seja, digamos, absolutamente contínua. No primeiro caso, seja <span class="math inline">\(\Theta\)</span> o conjunto dos possíveis valores do parâmetro desconhecido <span class="math inline">\(\theta\)</span>. Seguidamente, o trabalho de um estatístico é decidir com base em uma amostra selecionada adequadamente que membro ou membros da família <span class="math inline">\(\{F (\cdot ; \theta) : \theta \in \Theta\}\)</span> pode representar a função de distribuição de <span class="math inline">\(X\)</span>. Problemas desse tipo são chamados de problemas de inferência estatística paramétrica e o espaço estatístico é dado por <span class="math inline">\((\mathbb{R}, {F (·; \theta) : \theta \ \Theta}, \beta (\mathbb{R}))\)</span>, sendo <span class="math inline">\(\mathbb{R}\)</span> o conjunto dos reais na reta, <span class="math inline">\({F (\cdot; \theta) : \theta \in  \Theta}\)</span> a família de distribuições de <span class="math inline">\(X\)</span> e <span class="math inline">\(\beta (\mathbb{R})\)</span> a <span class="math inline">\(\sigma\)</span>-álgebra dos Borelianos na reta. O caso em que nada se sabe sobre a forma funcional da função de distribuição <span class="math inline">\(F\)</span> de <span class="math inline">\(X\)</span> é claramente muito mais difícil. Problemas de inferência deste tipo de são o domínio de estudo da estatística não paramétrica. Neste livro abordamos os problemas da estatística paramétrica.</p>
<section id="amostras-aleatórias" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="amostras-aleatórias"><span class="header-section-number">2.1</span> Amostras aleatórias</h2>
<p>Considere-se um experimento estatístico que culmina em desfechos <span class="math inline">\(x\)</span>, que são os valores assumidos por uma variável aleatória <span class="math inline">\(X\)</span>. Seja <span class="math inline">\(F\)</span> a função de distribuição de <span class="math inline">\(X\)</span>. Na prática, <span class="math inline">\(F\)</span> não será completamente conhecida, isto é, um ou mais parâmetros associados com <span class="math inline">\(F\)</span> serão desconhecidos. O trabalho de um estatístico é estimar esses parâmetros desconhecidos ou testar a validade de certas afirmações sobre eles. Ele pode, por exemplo, obter <span class="math inline">\(n\)</span> observações independentes de <span class="math inline">\(X\)</span>. Isso significa que ele observa <span class="math inline">\(n\)</span> valores <span class="math inline">\(x1, x2, \cdots , x_n\)</span> assumidos da variável aleatória <span class="math inline">\(X\)</span>. Cada <span class="math inline">\(x_i\)</span> pode ser considerado como o valor assumido pela variável aleatória <span class="math inline">\(X_i, i = 1, 2, \cdots , n\)</span> onde <span class="math inline">\(X_1, X_2, \cdots, X_n\)</span> são variáveis aleatórias independentes com distribuição comum <span class="math inline">\(F\)</span> . Os valores observados <span class="math inline">\((x_1, x_2, \cdots, x_n)\)</span> são então valores assumidos por <span class="math inline">\((X_1, X_2, \cdots, X_n)\)</span>. O conjunto <span class="math inline">\((X_1, X_2, \cdots , X_n)\)</span> é, então, uma amostra de tamanho <span class="math inline">\(n\)</span> da distribuição da população <span class="math inline">\(F\)</span> . O conjunto de <span class="math inline">\(n\)</span> valores <span class="math inline">\(x_1, x_2, \cdots , x_n\)</span> é chamado de uma realização ou estimativa da amostra. Note-se que os possíveis valores do vector aleatório <span class="math inline">\((X_1, X_2, \cdots , X_n)\)</span> podem ser olhados como pontos em <span class="math inline">\(\mathbb{R}^n\)</span>, os quais podem ser chamados de elementos do espaço amostral. Na prática podemos não observar <span class="math inline">\(x_1, x_2, \cdots, x_n\)</span> mas alguma função <span class="math inline">\(g(x_1, x_2, \cdots , x_n)\)</span>. Então <span class="math inline">\(g(x_1, x_2, \cdots , x_n)\)</span> serão considerados os valores assumidos pela variável aleatória <span class="math inline">\(g(X)\)</span>.</p>
<p>Vamos agora formalizar esses conceitos.</p>
<blockquote class="blockquote">
<p><strong>Definição</strong>. Seja <span class="math inline">\(X\)</span> uma variável aleatória com função de distribuição <span class="math inline">\(F\)</span> e <span class="math inline">\(X_1, \cdots, X_n\)</span> variáveis aleatórias independentes com distribuição comum <span class="math inline">\(F\)</span>. Chamaremos a coleção <span class="math inline">\(X_1, \cdots, X_n\)</span> de uma amostra aletória de tamanho <span class="math inline">\(n\)</span> de <span class="math inline">\(F\)</span> ou simplesmente como <span class="math inline">\(n\)</span> observações independentes de <span class="math inline">\(X\)</span>.</p>
</blockquote>
<p>Se <span class="math inline">\(X_1, X_2, \cdots , X_n\)</span> é uma amostra aleatória de <span class="math inline">\(F (\cdot; \theta)\)</span>, a função de distribuição conjunta é dada por:</p>
<p><span class="math display">\[F(x_1, \cdots, x_n; \theta) = \prod_{i=1}^{n} F(x_i; \theta)\]</span></p>
<blockquote class="blockquote">
<p><strong>Definição</strong>. Sejam <span class="math inline">\(X_1, X_2, \cdots, X_n\)</span> <span class="math inline">\(n\)</span> observações independentes da variável aleatória <span class="math inline">\(X\)</span> e seja <span class="math inline">\(g: \mathbb{R}^n \to \mathbb{R}\)</span> uma função real derivável. Então a variável aleatória <span class="math inline">\(g(X_1, X_2, \cdots, X_n)\)</span> é chamada de estatística, desde que não dependa de parâmetros desconhecidos.</p>
</blockquote>
<p>Segundo esta definição cada uma das variáveis na amostra isoladamente é uma estatística assim como funções destas que, eventualmente, podem não fornecer informações úteis. Duas das estatísticas mais comumente utilizadas são mostradas no exemplo a seguir.</p>
<blockquote class="blockquote">
<p><strong>Definição</strong>. Seja <span class="math inline">\(X_1, X_2, \cdots, X_n\)</span> uma amostra aleatória da função de distribuição <span class="math inline">\(F\)</span>. A estatística <span class="math display">\[\overline{X}_n = \sum_{i=1}^{n} \displaystyle \frac{Xi}{n},\]</span> é chamada de média amostral e a estatística <span class="math display">\[S_{n}^{2} = \sum_{i=1}^{n} \displaystyle \frac{(X_i - \overline{X}_n)^2}{n-1}\]</span> é chama de variância amostral.</p>
</blockquote>
<p>Deve-se lembrar que as estatísticas amostrais apresentadas neste exemplo <span class="math inline">\(\overline{X}_n, S_{n}^{2}\)</span> e outras que irão definir-se posteriormente são variáveis aleatórias, com todas as consequências que isso implica, enquanto os parâmetros populacionais <span class="math inline">\(\mu, \sigma^2\)</span> e assim por diante são constantes fixas, que podem ser desconhecidas.</p>
<p><strong>Exemplo</strong>. Seja <span class="math inline">\(X \sim Bernoulli(p)\)</span>, onde <span class="math inline">\(p\)</span> é desconhecido. A função de distribuição de <span class="math inline">\(X\)</span>, mostrada na Figura 2.1, é dada por</p>
<p><span class="math display">\[F(x) = p\delta (x − 1) + (1 − p)\delta(x),    x \in \mathbb{R},\]</span> onde a função <span class="math inline">\(\delta(\cdot)\)</span> foi definida em (1.2) como <span class="math inline">\(\delta(x) = \begin{cases} 1, \ x \ \geq \ 0, \\ 0, \ x \ &lt; 0 \end{cases}\)</span>, chamada de função delta.</p>
<p>Suponha que cinco observações independentes de <span class="math inline">\(X\)</span> sejam 0, 1, 1, 1, 0. Então 0, 1, 1, 1, 0 é uma realização da amostra <span class="math inline">\(X_1, X_2, \cdots , X_5\)</span>. A estimativa da média amostral é</p>
<p><span class="math display">\[\overline{x}_5 = \displaystyle \frac{0 + 1 + 1 + 1 + 0}{5} = 0, 6\]</span> o qual é o valor assumido pela variável aleatória <span class="math inline">\(\overline{X}_n\)</span>. A estimativa da variância amostral é <span class="math display">\[S_{5}^{2} = \sum_{i=1}{5} \displaystyle \frac{(x_i - \overline{x}_5)}{5 - 1}= \displaystyle \frac{2 \ \times (0,6)^2 \ + \ 3 \ \times (0,4)^2}{4}= 0,3\]</span></p>
<p>sendo este o valor assumido pela variável aleatória <span class="math inline">\(S_{5}^{2}\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Figura 2.1.png" class="img-fluid figure-img"></p>
<figcaption>Figura 2.1: Representação da função de distribuição Bernoulli para três valores do parâmetro <span class="math inline">\(p\)</span> = 0.3, 0.5 e 0.8. Observe que nesta curva a reta no intervalo (0, 1) depende de 1 - <span class="math inline">\(p\)</span>, isso porque a função <span class="math inline">\(\delta\)</span> é sempre zero para <span class="math inline">\(x\)</span> - 1 nesse intervalo.</figcaption>
</figure>
</div>
<p><strong>Exemplo</strong>. Seja <span class="math inline">\(X \sim N (\mu, \sigma^2)\)</span>, <span class="math inline">\(\mu\)</span> conhecida, <span class="math inline">\(\sigma^2\)</span> desconhecido e <span class="math inline">\(X_1, \cdots , X_n\)</span> uma amostra aleatória dessa distribuição. De acordo com nossa definição, a função <span class="math inline">\(\sum_{i=1}^{n} X_{i} / \sigma^2\)</span> não é uma estatística. Suponha que cinco observações de <span class="math inline">\(X\)</span> -0,864; 0,561; 2.355; 0,582 e -0,774. Então, a estimativa da média amostral é 0.372 e a estimativa da variância amostral é 1.648.</p>
</section>
<section id="estatísticas-de-ordem" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="estatísticas-de-ordem"><span class="header-section-number">2.2</span> Estatísticas de ordem</h2>
<p>Seja <span class="math inline">\((X_1, X_2, \cdots , X_n)\)</span> um vetor aleatório n-dimensional e <span class="math inline">\((x_1, x_2, \cdots , x_n)\)</span> uma <span class="math inline">\(n\)</span>-tupla assumida por <span class="math inline">\((X_1, X_2, \cdots , X_n)\)</span>. Vamos organizar <span class="math inline">\(x_1, x_2, \cdots , x_n\)</span> em ordem crescente de magnitude, para que <span class="math display">\[x_{(1)}\le x_{(2)} \le \cdots ≤ x_{(n)},\]</span> onde <span class="math inline">\(x_{(1)}\)</span> = min<span class="math inline">\((x_1, x_2, \cdots , x_n)\)</span>, <span class="math inline">\(x_{(2)}\)</span> é o segundo menor valor em <span class="math inline">\(x_1, \cdots , x_n\)</span> e assim por diante, <span class="math inline">\(x_{(n)}\)</span> = max<span class="math inline">\((x_1, \cdots , x_n)\)</span>. Se quaisquer dois <span class="math inline">\(x_i\)</span>, <span class="math inline">\(x_j\)</span> forem iguais, a ordem não importa.</p>
<blockquote class="blockquote">
<p><strong>Definição</strong>. A função <span class="math inline">\(X_{(k)}\)</span> de <span class="math inline">\((X_1, \cdots ,X_n)\)</span> que assume o valor <span class="math inline">\(x_{(k)}\)</span> em cada possível sequência <span class="math inline">\((x_1, x_2, \cdots , x_n)\)</span> de valores assumidos por <span class="math inline">\((X_1,X_2, \cdots , X_n)\)</span> é conhecida como a <span class="math inline">\(k\)</span>-ésima estatística de ordem ou a estatística de ordem <span class="math inline">\(k\)</span>. O conjunto <span class="math inline">\((X_{(1)},X_{(2)}, \cdots , X_{(n)})\)</span> é chamado de estatísticas de ordem para <span class="math inline">\((X_1, X_2, \cdots , X_n)\)</span>.</p>
</blockquote>
<p><strong>Exemplo</strong>. Consideremos <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, <span class="math inline">\(X_3\)</span> três variáveis aleatórias discretas de maneira que, <span class="math inline">\(X_1\)</span> e <span class="math inline">\(X_3\)</span> sejam tais que assumam somente valores 0, 1 e que <span class="math inline">\(X_2\)</span> assuma valores 1, 2, 3. O vetor aleatório <span class="math inline">\((X_1\)</span>, <span class="math inline">\(X_2\)</span>, <span class="math inline">\(X_3)\)</span> assume os valores: (0, 1, 0), (0, 2, 0), (0, 3, 0), (0, 1, 1), (0, 2, 1), (0, 3, 1), (1, 1, 0), (1, 2, 0), (1, 3, 0), (1, 1, 1), (1, 2, 1) e (1, 3, 1). Então <span class="math inline">\(X_{(1)}\)</span> assume somente valores 0 ou 1; <span class="math inline">\(X_{(2)}\)</span> assume somente valores 0 ou 1 e <span class="math inline">\(X_{(3)}\)</span> assume somente valores 1, 2 ou 3.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Teorema 2.1
</div>
</div>
<div class="callout-body-container callout-body">
<p>Seja <span class="math inline">\((X_{(1)}, X_{(2)}, \cdots , X_{(n)})\)</span> um vetor aleatório de dimensão <span class="math inline">\(n\)</span> e seja <span class="math inline">\(X_{(k)}, 1 \le k \le n\)</span>, a <span class="math inline">\(k\)</span>-ésima estatística de ordem. Então <span class="math inline">\(X_{(k)}\)</span> é também uma variável aleatória.</p>
</div>
</div>
<p><strong>Exercício</strong>. Na apresentação dos resultados a seguir assumiremos que <span class="math inline">\(X_1, X_2, \cdots , X_n\)</span> são variáveis aleatórias independentes e igualmente distribuídas contínuas com função de densidade <span class="math inline">\(f\)</span> . Seja <span class="math inline">\(\{ X_{(1)}, X_{(2)}, \cdots , X_{(n)} \}\)</span> o conjunto das estatística de ordem para <span class="math inline">\(X_1, X_2, \cdots , X_n\)</span>. Dado que todas as <span class="math inline">\(X_i\)</span> são contínuas segue que, com probabilidade 1 <span class="math display">\[X_{(1)} \le X_{(2)} \le \cdots \le X_{(n)}·\]</span></p>
<section id="propriedades-das-estatísticas-de-ordem" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="propriedades-das-estatísticas-de-ordem"><span class="header-section-number">2.2.1</span> Propriedades das estatísticas de ordem</h3>
<p>Começaremos o estudo das propriedades encontrando a função de densidade conjunta de <span class="math inline">\((X_{(1)}, X_{(2)}, \cdots , X_{(n)})\)</span>.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Teorema 2.2
</div>
</div>
<div class="callout-body-container callout-body">
<p>Sejam <span class="math inline">\(X_1, \cdots, X_n\)</span> variáveis aleatórias contínuas independentes, igualmente distríbuidas com densidade <span class="math inline">\(f\)</span>. A função de desindade conjunta de <span class="math inline">\((X_{(1)}, \cdots, X_{(n)})\)</span> é dada por <span class="math display">\[
f(x_{(1)}, \ldots, x_{(n)}) =
\begin{cases}
n! \prod\limits_{i=1}^{n} f(x_{(i)}), &amp; \text{se } x_{(1)} &lt; x_{(2)} &lt; \cdots &lt; x_{(n)} \\
0, &amp; \text{caso contrário}
\end{cases}.
\]</span></p>
</div>
</div>
<p><strong>Demonstração</strong>: A transformação de <span class="math inline">\((X_1, \cdots , X_n)\)</span> a <span class="math inline">\((X_{(1)}, \cdots , X_{(n)})\)</span> não é biunívoca. De fato, existem um total de <span class="math inline">\(n!\)</span> possíveis arranjos de <span class="math inline">\(x_1, \cdots , x_n\)</span> em ordem crescente de magnitude. Assim, existem <span class="math inline">\(n!\)</span> inversas para a transformação.</p>
<p>Por exemplo, uma das <span class="math inline">\(n!\)</span> permutações pode ser <span class="math display">\[
x_4 &lt; x_1 &lt; x_{n−1} &lt; x_3 &lt; \cdots &lt; x_n &lt; x_2·
\]</span></p>
<p>A inversa correspondente é <span class="math display">\[
x_4 = x_{(1)}, x_1 = x_{(2)}, \ x_{n−1} = x_{(3)}, \ x_3 = x_{(4)} \cdots x_n = x_{(n−1)}, \ x_2 = x_{(n)}·
\]</span> O determinante Jacobiano desta transformação é a matriz <span class="math inline">\(n \times n\)</span> identidade com as colunas reorganizadas, isto devido a que cada <span class="math inline">\(x_{(i)}\)</span> é igual a uma, e somente uma, das <span class="math inline">\(x_1, x_2, \cdots , x_n\)</span>. Portanto <span class="math inline">\(J = \pm 1\)</span> e</p>
<p><span class="math display">\[
f (x_{(2)}, \ x_{(n)}, \ x_{(4)}, \ x_{(1)}, \ \cdots , \ x_{(3)}, \ x_{(n−1)}) \ \mid J \mid =
\prod\limits_{i=1}^{n} \ f(x_{(i)}),
\]</span></p>
<p>quando <span class="math inline">\(x_{(1)} &lt; x_{(2)} &lt; \cdots &lt; x_{(n)}\)</span>. A mesma expressão é válida para cada um dos <span class="math inline">\(n!\)</span> arranjos. Segue então que</p>
<p><span class="math display">\[
\begin{align*}
f(x_{(1)}, \ldots, x_{(n)}) &amp;= \sum \prod\limits_{i=1}^{n} f(x_{(i)}) \\
&amp;\ \ \ \ \ \ \text{Todas as } n! \text{ permutações} \\
&amp; = \begin{cases}
n! \prod\limits_{i=1}^{n} f(x_{(i)}), &amp; \text{se } x_{(1)} &lt; x_{(2)} &lt; \cdots &lt; x_{(n)} \\
0, &amp; \text{caso contrário}
\end{cases}.
\end{align*}
\]</span></p>
<p><strong>Exemplo</strong>. Sejam <span class="math inline">\(X_1, \cdots , X_n\)</span> variáveis aleatórias independentes com função de densidade comum <span class="math display">\[
f(x) =
\begin{cases}
1, &amp; \text{se } 0 &lt; x &lt; 1 \\
0, &amp; \text{caso contrário}
\end{cases}.
\]</span> Então a função de densidade conjunta de <span class="math inline">\(X_{(1)}, X_{(2)}, \cdots , X_{(n)}\)</span> é <span class="math display">\[
f(x_{(1)}, \ \cdots, \ x_{(n)}) =
\begin{cases}
n!, &amp; \text{se } 0 &lt; x_{(1)} &lt; x_{(2)} &lt; \cdots &lt; x_{(n)} \\
0, &amp; \text{caso contrário}
\end{cases}.
\]</span></p>
<p>Estamos confiados que como resultado do Teorema 2.2 temos funções de densidade. Vejamos neste exemplo se isso é realmente acontece. Consideremos, para simplificar, o caso <span class="math inline">\(n\)</span> = 3 e verifiquemos se a integral da função de densidade é 1. Então</p>
<p><span class="math display">\[
\begin{align*}
\int \int\limits_\mathbb{R} \int f(x_{(1)},x_{(2)},x_{(3)}) \ dx_{(1)}dx_{(2)}dx_{(3)})
&amp; = 6 \int_{0}^1 \left[\int_{x_{(1)}}^1 \left(\int_{x_{(2)}}^1 dx_{(3)} \right) dx_{(2)} \right] dx_{(1)} \\
&amp; = 6 \int_{0}^1 \left[\int_{x_{(1)}}^1 \left(1 - x_{(2)} \right) dx_{(2)} \right] dx_{(1)} \\
&amp; = 6 \int_{0}^1 \left[\frac{1}{2} - x_{(1}) + \frac{x^2_{(1)}}{2} \right] dx_{(1)} = 1. \\
\end{align*}
\]</span></p>
<p>Um detalhe interessante é que esta e outras propriedades demonstradas aqui somente são válidas quando as variáveis aleatórias são contínuas. Isso não significa que estatísticas de ordem não possam ser definidas no caso discreto. O que estamos dizendo é que estas propriedades somente podem ser demonstradas no caso contínuo.</p>
<p><strong>Exemplo</strong>. Consideremos a situação em que temos somente três variáveis aleatórias independentes <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span> e <span class="math inline">\(X_3\)</span> com distribuição geométrica de parâmetro <span class="math inline">\(p\)</span>, isto é,</p>
<p><span class="math display">\[
P (X = x; p) = (1 − p)p^x, \ \ \ \ \    x = 0, 1, 2, \cdots
\]</span> Encontremos <span class="math inline">\(P (X_{(1)} &lt; X_{(2)} &lt; X_{(3)})\)</span>. Nesta situação a probabilidade requerida pode ser escrita como: <span class="math display">\[
\begin{align*}
P (X_{(1)} &lt; X_{(2)} &lt; X_{(3)})  =  1 − P (X_1 = X_2 \neq X_3) − P (X_1 = X_3 \neq X_2) \\
−P (X_2 = X_3 \neq X_1) − P (X_1 = X_2 = X_3)
\end{align*}
\]</span> a qual pode ser escrita como <span class="math display">\[
\begin{align*}
P (X_{(1)} &lt; X_{(2)} &lt; X_{(3)})
&amp; = 1 − 3P (X_1 = X_2 \neq X_3) − P (X_1 = X_2 = X_3) \\
&amp; = 1 − 3 [P (X_1 = X_2) − P (X_1 = X_2 = X_3)] − P (X_1 = X_2 = X_3) \\
&amp; = 1 − 3P (X_1 = X_2) + 2P (X_1 = X_2 = X_3)· \\
\end{align*}
\]</span></p>
<p>Não é difícil perceber que <span class="math display">\[
P(X_1 = X_2) = \frac{(1-p)^2}{1-p^2},
\]</span></p>
<p>e que <span class="math display">\[
P(X_1 = X_2 = X_3) = \frac{(1-p)^3}{1-p^3},
\]</span></p>
<p>do qual obtemos que <span class="math display">\[
P(X_{(1)} = X_{(2)} = X_{(3)}) = \frac{6p^3}{(1-p)(1+p+p^2)}.
\]</span> As propriedades das estatísticas de ordem que serão demonstradas valerão somente caso as variáveis sejam contínuas. Isto deve-se a que, caso as variáveis sejam discretas, a probabilidade</p>
<p><span class="math display">\[
P (X_{(1)} = X_{(2)} = \cdots = X_{(n)}) \neq 0,
\]</span></p>
<p>como vai ser mostrado no seguinte exemplo. Acontece que o fato da probabilidade das estatística de ordem poderem coincidir, com probabilidade diferente de zero, altera a estrutura da demonstração e não nos permite obtermos estes resultados para o caso discreto.</p>
<p><strong>Exemplo</strong>. Sejam <span class="math inline">\(X_1, X_2, \cdots , X_n\)</span> variáveis aleatórias independentes assumindo somente 0 e 1 com probabilidade 1/2. Observemos que <span class="math display">\[
\begin{align*}
P(X_{(1)} = X_{(2)} = \cdots = X_{(n)})
&amp; = \prod\limits_{k=1}^{n} P(X_{(k)} = 0) + \prod\limits_{k=1}^{n} P(X_{(k)} = 1)  \\
&amp; = \prod\limits_{k=1}^{n} P(X_{(k)} = 0) + \prod\limits_{k=1}^{n} P(X_{k} = 1) = \frac{1}{2^{n-1}} . \\
\end{align*}
\]</span></p>
<p>Estudemos agora o comportamento marginal, ou seja, nos interessa agora encontrar a função de distribuição marginal de cada estatística de ordem.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Teorema 2.3
</div>
</div>
<div class="callout-body-container callout-body">
<p>Sejam <span class="math inline">\(X_1, \cdots, X_n\)</span> variáveis aleatórias contínuas independentes e igualmente distribuídas e <span class="math inline">\((X_{(1)}, \cdots, X_{(n)})\)</span> as estatísticas de ordem. A função de densidade marginal de <span class="math inline">\(X_{(r)}\)</span> é dada por <span class="math display">\[
f_r(x_{(r)}) = \frac{n!}{(r-1)!(n-r)!}[F(x_{(r)})]^{r-1}[1-F(x_{(r)})]^{n-r}f(x_{(r)}),
\]</span> onde <span class="math inline">\(F\)</span> é a função de distribuição comum de <span class="math inline">\(X_1, \cdots, X_n\)</span>.</p>
</div>
</div>
<p>Demonstração : Partimos da expressão da função de densidade conjunta das estatísticas de ordem obtida no Teorema 2.2. Então,</p>
<p>$$ <span class="math display">\[\begin{align*}
f_r(x_{(r)})
&amp; = n!f(x_{(r)})  
\int_{-\infty}^{x_{(r)}} \int_{-\infty}^{x_{(r-1)}} \cdots
\int_{-\infty}^{x_{(2)}} \int_{x_{(r)}}^{+\infty} \int_{x_{(r+1)}}^{+\infty} \cdots
\int_{x_{(n-1)}}^{+\infty}
\prod\limits_{i \neq r}^{n} f(t_i) \
\text{d}t_n \cdots\text{d}t_{r+1} \ \text{d}t_{1} \cdots \text{d}t_{r-1}  \\

&amp; = n!f(x_{(r)})
\frac{[1-F(x_{(r)})]^{n-r}}{(n-r)!}
\int_{-\infty}^{x_{(r)}} \cdots
\int_{x_{(-\infty)}}^{x_{(2)}}
\prod\limits_{i =1}^{r-1} f(t_i) \ \text{d}t_i \\

&amp; = n!f(x_{(r)})
\frac{[1-F(x_{(r)})]^{n-r}}{(n-r)!}
\frac{[F(x_{(r)})]^{r-1}}
{(r-1)!}.

\end{align*}\]</span>\end{align*} $$</p>
<p>Como utilidade deste teorema podemos mencionar o fato de agora podermos encontrar os momentos das estatística de ordem. Faremos isso como consequência do seguinte exemplo.</p>
<strong>Exemplo</strong>. Sejam <span class="math inline">\(X_1, X_2, \cdots , X_n\)</span> variáveis aleatórias independentes <span class="math inline">\(U\)</span> (0, 1). Então $$ f_r(x_{(r)}) =
<span class="math display">\[\begin{cases}
\frac{n!}{(r-1)!(n-r)!} x_{(r)}^{r-1}(1-x_{(r)})^{n-r},
&amp; \text{se } 0 &lt; x_{(r)} &lt; 1 \\

&amp; \ \ \ \ \ \ \ \ \ \  (1 \leq r \leq n) \\
0, &amp; \text{caso contrário}
\end{cases}\]</span>
<p>. $$</p>
<p>Observemos que, na situação do exemplo acima,</p>
<p><span class="math display">\[
X_{(r)} \sim \beta(r, n − r + 1),
\]</span></p>
<p>logo, valem os resultados da distribuição Beta e, por exemplo,</p>
<p><span class="math display">\[
E(X_{(r)}) = r/(n + 1)·
\]</span></p>
<p>Para uma densidade qualquer e somente quatro variáveis aleatórias a forma da densidade marginal, de uma qualquer estatística de ordem, é mostrada no seguinte exemplo.</p>
<p><strong>Exemplo</strong>. Sejam <span class="math inline">\(X_1, X_2, X_3 ,X_4\)</span> variáveis aleatórias independentes com densidade comum <span class="math inline">\(f\)</span>. A função de densidade conjunta das estatísticas de ordem <span class="math inline">\(X_{(1)}, X_{(2)}, X_{(3)}, X_{(4)}\)</span> é</p>
$$ f(x_{(1)}, x_{(2)}, x_{(3)}, x_{(4)}) =
<span class="math display">\[\begin{cases}
4!f (x_{(1)})f (x_{(2)})f (x_{(3)})f (x_{(4)}),
&amp; \text{se }    x_{(1)} &lt; x_{(2)} &lt; x_{(3)} &lt; x_{(4)} \\

0,  
&amp; \text{caso contrário}
\end{cases}\]</span>
<p>. $$</p>
<p>Vamos calcular a função de densidade marginal de <span class="math inline">\(X_{(2)}\)</span>. Temos que, se <span class="math inline">\(x_{(1)} &lt; x_{(2)} &lt; x_{(3)} &lt; x_{(4)}\)</span></p>
<p>$$ <span class="math display">\[\begin{align*}
f2(x_{(2)})
&amp; =  4! \int \int \int \int f(t_1) f(x_{(2)}) f(t_3) f(t_4)
\ \text{d}t_1 \ \text{d}t_3 \ \text{d}t_4 \\

&amp; =  4! f(x_{(2)})
\int_{-\infty}^{x_{(2)}}
\int_{2}^{+\infty}
\left[\int_{t_3}^{+\infty} f(t_4) \ \text{d}t_4\right]
f(t_3) f(t_1) \ \text{d}t_3 \ \text{d}t_1  \\

&amp; =  4! f(x_{(2)})
\int_{-\infty}^{x_{(2)}}
\Biggl\{
\int_{x_{(2)}}^{+\infty}[1 - F(t_3)]f(t_3) \text{d}t_3
\Biggl\}
f{(t_1)} \text{d}t_1\\

&amp; =  4! f(x_{(2)})
\int_{-\infty}^{x_{(2)}}
\frac{[1-F(x_{(2)})]^2}{2}
f(t_1) \ \text{d}t_1 =
4! f(x_{(2)})
\frac{[1-F(x_{(2)})]^2}{2}
F(x_{(2)})

·
\end{align*}\]</span> $$</p>
<p>Evidentemente, a expressão acima coincide com o resultado apresentado no Teorema 2.3.</p>
</section>
</section>
<section id="pg8" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="pg8"><span class="header-section-number">2.3</span> pg8</h2>
<p>Demonstração :</p>
<p>frs(x(r), x(s)) =</p>
<p>∫ x(r)</p>
<p>∫ t2</p>
<p>∫ x(s)</p>
<p>∫ x(s) ∫ +∞</p>
<p>∫ +∞</p>
<p>−∞ −∞</p>
<p>x(r)</p>
<p>ts−2</p>
<p>x(s)</p>
<p>tn−1</p>
<p>n!f (t1) <span class="math inline">\(F\)</span> (tn) dtn dts+1 dts−1 dtr+1 dt1 dtr−1</p>
<p>= n!</p>
<p>∫ x(r)</p>
<p>∫ t2</p>
<p>∫ x(s)</p>
<p>∫ x(s) [1 − <span class="math inline">\(F\)</span> (x(s))]n−s</p>
<p>−∞ −∞</p>
<p>x(r)</p>
<p>ts−2</p>
<p>(n − s)!</p>
<p>×f (t1)f (t2) <span class="math inline">\(F\)</span> (x(s)) dts−1 dtr+1 dt1 dtr−1</p>
<p>= n!</p>
<p>[1 − <span class="math inline">\(F\)</span> (x(s))]n−s (n − s)! f (x(s))</p>
<p>x(r)</p>
<p>−∞</p>
<p>t2 f (t1) <span class="math inline">\(F\)</span> (x(r))× −∞</p>
<p>[F (x(s)) − <span class="math inline">\(F\)</span> (x(r))]s−r−1 × (s − r − 1)! dt1 dtr−1</p>
<pre><code>n!  </code></pre>
<p>= (n − s)!(s − r − 1)![1 − <span class="math inline">\(F\)</span> (x</p>
<ol start="19" type="a">
<li></li>
</ol>
<p>)]n−s×</p>
<p>[F (x(r))]r−1</p>
<p>caso x(r) &lt; x(s).</p>
<p>×[F (x(s)) − <span class="math inline">\(F\)</span> (x(r))]s−r−1f (x(s))f (x(r))</p>
<p>, (r − 1)!</p>
<p>De modo semelhante, podemos mostrar que a função de densidade conjunta de X(k1), , X(km) se 1 ≤ k1 &lt;</p>
<p>k2 &lt; &lt; km ≤ n, 1 ≤ m ≤ n, é dada por fk1k2···km (x(k1), x(k2), , x(km)) =</p>
<p>(k1</p>
<p>— 1)!(k2</p>
<p>— k1</p>
<p>n! × — 1)! × (n − km)!</p>
<p>×Fk1−1(x(k ))f (x(k ))[F (x(k )) − <span class="math inline">\(F\)</span> (x(k ))]k2−k1−1f (x(k )) × × ×[F (x(k )) − <span class="math inline">\(F\)</span> (x(k ))]km−1−km−2−1f (x(k ))[1 − <span class="math inline">\(F\)</span> (x(k ))]n−km <span class="math inline">\(F\)</span> (x(k )), caso x(k1) &lt; x(k2) &lt; &lt; x(km) e zero noutras situações.</p>
<p><strong>Exemplo 2.9</strong> (Continuação do Exemplo 2.7) Sabemos que as variáveis aleatórias <span class="math inline">\(X_1, X_2, \cdots , X_n\)</span> são independentes e tem como função de densidade comum f (x) = 1, se 0 &lt; x &lt; 1 · 0, caso contrário Então, a função de densidade conjunta de X(r) e X(s) é dada por</p>
<p>frs</p>
<p>(x(r)</p>
<p>, x(s)) =</p>
<p> n!</p>
<p>xr−1(x(r) − x(s))s−r−1(1 − x(s))n−s (r − 1)!(s − r − 1)!(n − s)!</p>
<p>, se x</p>
<ol start="18" type="a">
<li></li>
</ol>
<p>&lt; x(s) ·</p>
<p></p>
<p>onde 1 ≤ r &lt; s ≤ n.</p>
<p>0, caso contrário</p>
<p>Uma situação mais complexa é trabalharmos com funções de estatísticas de ordem. Não temos um resultado simples para o caso de qualquer funções destas estatísticas. Mas, no exemplo a seguir, podemos encontrar um resultado interessante para o comportamento da diferença de estatísticas de ordem.</p>
<p><strong>Exemplo 2.10</strong> Sejam X_{(1)}, X_{(2)}, X_{(3)} as estatísticas de ordem das variáveis aleatórias independentes e igualmente distribuídas X1, X2, X3 com função de densidade comum { βe−xβ, se x ≥ 0 sendo β &gt; 0. Sejam Y1 = X_{(3)} − X_{(2)} e Y2 = X_{(2)}. Mostraremos que Y1 e Y2 são independentes. Para isso primeiro observemos que a função de densidade conjunta de X_{(2)} e X_{(3)} é dada por</p>
<p>f23(x, y) =</p>
<p>1!0!0!</p>
<p>· 0, caso contrário</p>
<p>A função de densidade conjunta de (Y1, Y2) é então f (y1, y2) = 3!β2(1 − e−y2β )e−y2βe−(y1+y2)β = [3!βe−2y2β (1 e−y2β )][βe−y1β ], se 0 &lt; y &lt; + , 0 &lt; y &lt; + = · 0, caso contrário Do qual segue que Y1 e Y2 são independentes. Duas estatísticas de ordem importantes são o máximo e mínimo. Nesses casos é possível encontrar, de maneira</p>
<p>analítica, expressões para a função de distribuição. Vejamos no teorema a seguir as expressões da função de distribuição das estatísticas de ordem X_{(1)} e X_{(n)}.</p>
<p>Demonstração : Exercício.</p>
<p>Acerca da função de distribuição de qualquer estatística de ordem temos o resultado a seguir.</p>
<p>Demonstração : O evento {X(k) ≤ x} ocorre se, e somente se, pelo menos k dos <span class="math inline">\(X_1, X_2, \cdots , X_n\)</span> são menores ou iguais a x, por isso o somatório começa em k.</p>
<p>Nos dois teoremas seguintes relacionamos a distribuição condicional de estatísticas de ordem, condicionadas em outra estatística de ordem, com a distribuição de estatísticas de ordem de uma população cuja distribuição é uma forma truncada da função de distribuição da população original <span class="math inline">\(F\)</span> .</p>
<p>Demonstração : A densidade condicional de X(j) dado que X_{(i)} = xi calcula-se dividindo a densidade conjunta de X_{(i)} e X(j), dada em (2.7), pela densidade marginal de X_{(i)}, esta obtida no Teorema 2.4. Temos então que, quando</p>
<p>i &lt; j ≤ n e xi ≤ xj &lt; ∞,</p>
<p>f (xj|X_{(i)}</p>
<p>= x ) = fij(xi, xj) i fi(xi) (n − i)! [ <span class="math inline">\(F\)</span> (xj) − <span class="math inline">\(F\)</span> (xi)]j−i−1</p>
<p>[ 1 − <span class="math inline">\(F\)</span> (xj)]n−j <span class="math inline">\(F\)</span> (xj)</p>
<p>(j − i − 1)!(n − j)! 1 − <span class="math inline">\(F\)</span> (xi)</p>
<p>1 − <span class="math inline">\(F\)</span> (xi) 1 − <span class="math inline">\(F\)</span> (xi)</p>
<p>O resultado segue observando que <span class="math inline">\(F\)</span> (xj ) − <span class="math inline">\(F\)</span> (xi) e <span class="math inline">\(F\)</span> (xj ) são, respectivamente, as funções de distribuição e de 1 − <span class="math inline">\(F\)</span> (xi) 1 − <span class="math inline">\(F\)</span> (xi) densidade truncando à esquerda em xi a distribuição <span class="math inline">\(F\)</span> .</p>
<p>Na demonstração do teorema anterior utiliza-se o conceito de distribuição truncada, o que é isso? define-se a seguir este conceito e incluem-se exemplos explicativos.</p>
<p>Caso a variável aleatória X seja discreta com função de probabilidade P , a distribuição truncada de X é dada por</p>
<p>P (X = x|X ∈ A) =</p>
<p>P (X = x, X ∈ A) P (X ∈ A)</p>
<p>=  </p>
<p>P (X = x) P (X = a), se x ∈ A a∈A 0, se x ∈/ A</p>
<p>Na situação X do tipo contínua, com função de densidade <span class="math inline">\(F\)</span> , temos que</p>
<p>P (X ≤ x|X ∈ A) =</p>
<p>P (X ≤ x, X ∈ A) = P (X ∈ A)</p>
<p>∫(−∞,x]∩A</p>
<p>f (y) dy</p>
<p>· (2.8)</p>
<p>Concluindo então que, a função de densidade da distribuição truncada é dada por</p>
<p>h(x) =</p>
<p> ∫</p>
<p>f (x) f (y) dy</p>
<p>, caso x ∈ A,</p>
<p>· (2.9)</p>
<p> 0 A</p>
<p>caso x ∈/ A</p>
<p>Exemplo 2.11 Suponhamos X uma variável aleatória com distribuição normal padrão e A = ( , 0]. Então, P (X A) = 1/2, dado que X é simétrica e contínua. Para a densidade truncada temos que { 2f (x), caso − ∞ &lt; x ≤ 0,</p>
<p>O truncamento é especialmente importante nos casos em que a distribuição <span class="math inline">\(F\)</span> em questão não tem média finita. Se X é uma variável aleatória, truncamos X em algum c &gt; 0, onde c é finito, substituindo X por Xc = X caso |X| c e zero caso |X| &gt; c.&nbsp;Então Xc é X truncada em c e todos os momentos de Xc existem e são finitos. Na verdade, sempre podemos selecionar c suficientemente grande para que P (X ̸= Xc) = P (|X| &gt; c),</p>
<p>seja arbitrariamente pequena. A distribuição de Xc é então dada por</p>
<p>P (Xc ≤ x) = P (X ≤ x| |X| ≤ c) = no caso contínuo com função de densidade <span class="math inline">\(F\)</span> e é dada por</p>
<p>f (y) dy (−∞,x]∩[−c,+c] , P (|X| ≤ c)</p>
<p>P (Xc = x) =</p>
<p> </p>
<p>P (X = x)</p>
<p>P (X = a) a∈[−c,+c]</p>
<p>, se x ∈ [−c, +c] ,</p>
<p>0, se x ∈/ [−c, +c] no caso discreto. Observemos que, para algum α &gt; 0, E(|Xc|)α ≤ cα·</p>
<p><strong>Exemplo 2.12</strong> Caso X Cauchy(0, 1), sabemos que E(X) não existe. Seja c &gt; 0 um número finito, truncando X em c definimos</p>
<p>Então</p>
<p>Xc =</p>
<p>X, caso |X| c, · 0, caso |X| &gt; c</p>
<p>1 ∫ +c 1</p>
<p>2 −1</p>
<p>Sendo que a função de densidade truncada é dada por</p>
<p> 1 1</p>
<p>1 , caso x ∈ [−c, +c],</p>
<p>h(x) =</p>
<p>Desta expressão obtemos que</p>
<p>2 1 + x2 tan−1(c) ·  0, caso x ∈/ [−c, +c]</p>
<pre><code>1    ∫ +c   x   </code></pre>
<p>e também que</p>
<p>E(Xc) =</p>
<p>2 tan−1(c)</p>
<p>−c 1 + x2</p>
<p>dx = 0,</p>
<p>E(Xc)2 =</p>
<pre><code>1    ∫ +c</code></pre>
<p>x2 dx =</p>
<pre><code>c   </code></pre>
<p>— 1·</p>
<p>2 tan−1(c)</p>
<p>−c 1 + x2</p>
<p>tan−1(c)</p>
<p>Por último, temos o seguinte resultado estabelecendo novamente relação entre estatísticas de ordem e distri- buições truncadas.</p>
<p>Demonstração : A densidade condicional de X_{(i)} dado que X(j) = xj calcula-se dividindo a densidade conjunta de X_{(i)} e X(j), dada em (2.7), pela densidade marginal de X(j), esta obtida no Teorema 2.4. Temos então que, quando i &lt; j ≤ n e xi ≤ xj &lt; ∞, (j − i)! [ <span class="math inline">\(F\)</span> (xi) ]i−1 [ <span class="math inline">\(F\)</span> (xj) − <span class="math inline">\(F\)</span> (xi)]j−i−1 <span class="math inline">\(F\)</span> (xi) O resultado segue observando que <span class="math inline">\(F\)</span> (xi)/F (xj) e <span class="math inline">\(F\)</span> (xi)/F (xj) são, respectivamente, as funções de distribuição e de densidade truncando à direita em xj a distribuição <span class="math inline">\(F\)</span> .</p>
<section id="quantis" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="quantis"><span class="header-section-number">2.3.1</span> 2.2.2 Quantis</h3>
<p>Lembremos que a função de distribuição <span class="math inline">\(F\)</span> é contínua à direita e que o número de descontinuidades é, no máximo, enumerável. Estas são propriedades importantes que farão toda diferença na definição dos quantis amostrais, por isso, demonstraremos as propriedades mencionadas da função de distribuição. A prova de que <span class="math inline">\(F\)</span> é contínua à direita advém do seguinte fato F (x + hn) − <span class="math inline">\(F\)</span> (x) = P (x &lt; X ≤ x + hn), onde {hn} é uma sequência de números reais estritamente positivos tais que limn→∞ hn = 0. Segue, da propriedade de continuidade da função de probabilidade,1 que lim [F (x + hn) F (x)] = 0, n→∞ e, portanto, <span class="math inline">\(F\)</span> é contínua à direita. Definamos por D o conjunto dos pontos de descontinuidade de <span class="math inline">\(F\)</span> e seja D = {x ∈ D : P (X = x) ≥ 1 } , onde n é um inteiro positivo. Dado que <span class="math inline">\(F\)</span> ( ) F ( ) = 1, o número de elementos em Dn não pode exceder n.&nbsp;Logicamente ∞ D = Dn n=1 e, então, o conjunto D é enumerável. Demonstrando-se assim a segunda propriedade importante mencionada da função de distribuição. Definimos a seguir o conceito de quantil teórico e depois mostramos a forma de cálculo.</p>
<p>1A função de distribuição é contínua, devido a que P lim n→∞</p>
<p>An = lim n→∞</p>
<p>P (An),</p>
<p>se o limite limn→∞ An existir.</p>
<p>A função <span class="math inline">\(F\)</span> −1(t), 0 &lt; t &lt; 1 foi definida em (1.29) e é chamada de função inversa de <span class="math inline">\(F\)</span> . O seguinte teorema fornece-nos propriedades úteis. Fica claro que as propriedades apresentadas no seguinte teorema nos permitirão o cálculo dos quantis e é por isso que dedicamos atenção a este conceito.</p>
<p>Demonstração : Exercício.</p>
<p><strong>Exemplo 2.13</strong></p>
<p>Seja X Exponencial(). Sabemos que a função de distribuição neste caso é <span class="math inline">\(F\)</span> (x) = 1 e−x/. Resulta que a expressão de qualquer um dos quantis é possível de ser encontrada de maneira exata via</p>
<p>obtendo-se que</p>
<p>F (ξp) = p 1 − e−ξp/= p 1 − p = e−ξp/,</p>
<p>ξp = −ln(1 − p)</p>
<p>é a expressão teórica do p-ésimo quantil. Devemos mencionar que a expressão dos quantis está bem definida, no sentido de que o resultado é sempre positivo. Isto é importante porque devemos lembrar que a distribuição exponencial está definida somente para valores positivos, então o quantil teórico deve ser positivo, já que é um dos possíveis valores da variável. Observemos que caso <span class="math inline">\(F\)</span> seja contínua e estritamente crescente, <span class="math inline">\(F\)</span> −1 é definida como F −1(y) = x quando y = <span class="math inline">\(F\)</span> (x)· Ainda podemos observar que, se x0 é um ponto de descontinuidade de <span class="math inline">\(F\)</span> e supondo que F (x−) &lt; y &lt; <span class="math inline">\(F\)</span> (x0) = <span class="math inline">\(F\)</span> (x+) 0 0</p>
<p>vemos que, embora não exista x tal que y = <span class="math inline">\(F\)</span> (x), <span class="math inline">\(F\)</span> −1(y) é definido como igual a x0. A situação na qual <span class="math inline">\(F\)</span> não é estritamente crescente, por exemplo, caso da variável aleatória ser discreta, podemos escrever</p>
<p>F (x) =</p>
<p>= y, caso a ≤ x ≤ b ·  &gt; y, caso x &gt; b</p>
<p>Então, qualquer valor a x b poderia ser escolhido como x = <span class="math inline">\(F\)</span> −1(y). A convenção é que, neste caso, definimos F −1(y) = a. Em particular ξ1/2 = <span class="math inline">\(F\)</span> −1(1/2), (2.10) é chamada de mediana de <span class="math inline">\(F\)</span> . Observemos que ξp satisfaz a desigualdade F (ξp− ) ≤ p ≤ <span class="math inline">\(F\)</span> (ξp)· Exemplo 2.14 (Continuação do Exemplo 2.13) Caso p = 1/2, a mediana amostral será ξ1/2 = −ln(1/2) = 0.6931472.</p>
</section>
</section>
<section id="momentos-amostrais" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="momentos-amostrais"><span class="header-section-number">2.4</span> Momentos amostrais</h2>
<p>Nesta seção vamos estudar algumas estatísticas amostrais comumente utilizadas e suas distribuições.</p>
<p>Observemos que nFn(x) é o número de Xk (1 ≤ k ≤ n) menores ou iguais a x. Se X_{(1)}, X_{(2)}, , X_{(n)} são as estatísticas de ordem de <span class="math inline">\(X_1, X_2, \cdots , X_n\)</span> então claramente</p>
<p>Fn(x) =  </p>
<p>k , se X n</p>
<ol start="11" type="a">
<li></li>
</ol>
<p>≤ x &lt; X</p>
<p>(k+1)</p>
<p>, (k = 1, 2, , n − 1)</p>
<p>· (2.11)</p>
<p> 1, se x ≥ X_{(n)}</p>
<p>com esperança e variância</p>
<p>Var[Fb</p>
<p>E[Fn(x)] = <span class="math inline">\(F\)</span> (x) (2.13)</p>
<p>F (x)[1 − <span class="math inline">\(F\)</span> (x)]</p>
<p>Demonstração : Dado que δ(x − Xi), i = 1, 2, , n são variáveis aleatórias independentes igualmente distribuídas cada uma com função de probabilidade P [δ(x − Xi) = 1] = P (x − Xi ≥ 0) = <span class="math inline">\(F\)</span> (x) e P [δ(x − Xi) = 0] = 1 − <span class="math inline">\(F\)</span> (x), sua soma nF ∗(x) é uma variável aleatória com distribuição Binomial(n, p), onde p = <span class="math inline">\(F\)</span> (x). As relações (2.12), (2.13) e (2.14) seguem-se imediatamente.</p>
<p>Demonstração :</p>
<p>E´ uma consequência da Lei dos Grandes Números .</p>
<p>Corolário 2.12</p>
<p>onde Z ∼ N (0, 1).</p>
<p>√n[F (x) F (x)] √F (x)[1 − <span class="math inline">\(F\)</span> (x)] −→ Z quando n → ∞,</p>
<p>Demonstração :</p>
<p>E´ consequência do Teorema do Limite Central.</p>
<p><strong>Exemplo 2.15</strong></p>
<p>Vamos apresentar o conceito de função distribuição empírica no caso de termos uma amostra aleatória da distribuição N (0, 1). A lista de comandos na linguagem de programação R está disponível abaixo. O primeiro comando destina-se a fixar o gerador de amostras e, assim, em qualquer momento podemos obter a mesma amostra aleatória. Na Figura 2.2 mostramos a forma da distribuição empírica, de três formas diferentes, para uma amostra de tamanho 12. A representação da função de distribuição empírica é realizada permitindo escolher qual utilizar segundo o agrado.</p>
<p>lwd = 2</p>
<p>−1.5 −1.0 −0.5 0.0 0.5</p>
<p>x</p>
<p>−1.5 −1.0 −0.5 0.0 0.5</p>
<p>x</p>
<p>−1.5 −1.0 −0.5 0.0 0.5</p>
<p>x</p>
<p>Figura 2.2: Representação da função de distribuição amostral ou empírica, de três formas diferentes, para uma amostra normal padrão de tamanho 12.</p>
<p>A linhas de comando a seguir permitiram-nos gerar os gráficos na Figura 2.2: construímos : set.seed(5739); x=rnorm(12); Fn=ecdf(x) par(mar=c(5,4,3,1), cex=0.9) plot(Fn, main=““) plot(Fn, verticals = TRUE, do.points = FALSE, main=”“) plot(Fn , lwd = 2, main=”“); mtext(”lwd = 2”, adj = 1) xx=unique(sort(c(seq(-3, 2, length = 201), knots(Fn12)))) lines(xx, Fn(xx), col = “blue”) abline(v = knots(Fn), lty = 2, col = “gray70”) Observemos que a convergência da distribuição empírica, segundo o Teorema 2.10, é para cada valor de x. E´ possível fazer uma demonstração da convergência em probabilidade simultaneamente para todos os x, ou seja, da convergência uniforme.</p>
<p>Demonstração : Seja ϵ &gt; 0. Escolhemos um inteiro k &gt; 1/ϵ e números −∞ = x0 &lt; x1 ≤ x2 ≤ ≤ xk−1 &lt; xk = ∞, tais que <span class="math inline">\(F\)</span> (x−) ≤ j/k ≤ <span class="math inline">\(F\)</span> (xj), para j = 1, , k − 1. Observe que se xj−1 &lt; xj, então</p>
<p>Pela Lei dos Grandes Números</p>
<p>F (x−) − <span class="math inline">\(F\)</span> (xj−1) ≤ ϵ·</p>
<p>q.c. Fn(xj) −→ <span class="math inline">\(F\)</span> (xj) e — q.c. −</p>
<p>para j = 1, , k − 1. Consequentemente,</p>
<p>Fbn(xj ) −→ <span class="math inline">\(F\)</span> (xj ),</p>
<p>∗ − q.c. ∆n = max{|Fbn(xj) − <span class="math inline">\(F\)</span> (xj)|, |Fn (xj ) − <span class="math inline">\(F\)</span> (xj)|, j = 1, , k − 1} −→ 0·</p>
<p>Seja x arbitrário e encontremos j tal que xj−1 &lt; x ≤ xj. Então, Fbn(x) − <span class="math inline">\(F\)</span> (x) ≤ Fbn(x−) − <span class="math inline">\(F\)</span> (xj−1) ≤ Fbn(x−) − <span class="math inline">\(F\)</span> (x−) + ϵ,</p>
<p>e</p>
<p>Isto implica que</p>
<p>Fbn(x) − <span class="math inline">\(F\)</span> (x) ≥ Fbn(xj−1) − <span class="math inline">\(F\)</span> (x−) ≥ Fbn(xj−1) − <span class="math inline">\(F\)</span> (xj−1) − ϵ·</p>
<p>q.c. sup |Fn(x) F (x)| ∆n + ϵ ϵ· x Como isso vale para todo ϵ &gt; 0, o teorema segue.</p>
<p>Agora, dado que <span class="math inline">\(F\)</span> ∗(x) tem pontos de salto em Xi, i = 1, 2, , n é claro que existem todos os momentos de <span class="math inline">\(F\)</span> ∗(x). Vamos considerar alguns valores típicos da função de distribuição <span class="math inline">\(F\)</span> , chamados de estatísticas amostrais. Escrevamos a = 1 ∑ Xk, (2.15)</p>
<p>para os momentos de ordem k ao redor do 0 (zero). Aqui ak, serão chamados de momentos amostrais de ordem k. Com esta notação</p>
<p>O momento amostral central é definido por</p>
<p>n a1 = Xi n i=1</p>
<p>= X·</p>
<p>b = 1 ∑(X − a )k = 1 ∑(X</p>
<p>— X) · (2.16)</p>
<p>Logicamente,</p>
<p>k n i 1 i=1</p>
<p>n i i=1</p>
<p>b = 0 e b</p>
<p>= (n − 1 ) S2·</p>
<p>Como mencionado anteriormente, não chamamos b2 a variaˆncia amostral. S2</p>
<p>será chamada como a variaˆncia</p>
<p>amostral por razões que se tornarão claras posteriormente. Temos que b2 = a2 − a2· Para a função geradora de momentos de Fn podemos afirmar que n</p>
<p>MFbn</p>
<ol start="20" type="a">
<li>= 1 etXi · n</li>
</ol>
<p>i=1</p>
<p>Definições similares são realizadas para momentos amostrais de distribuições multivariadas. Por exemplo, se (X1, Y2), (X2, Y2), , (Xn, Yn) é uma amostra de uma distribuição bivariada, podemos escrever n n X = 1 ∑ X , Y = 1 ∑ Y</p>
<p>para as duas médias amostrais e para os momentos de segunda ordem centrais escrevemos</p>
<p>b20</p>
<p>n = (Xi n i=1</p>
<p>— X) , b02</p>
<p>n = (Yi n i=1</p>
<p>— Y ) ,</p>
<p>Mais uma vez, escrevemos</p>
<p>b11</p>
<p>n</p>
<p>n = (Xi n i=1</p>
<p>— X)(Yi</p>
<p>— Y )·</p>
<p>n</p>
<p>S2 = 1 ∑(X</p>
<p>— X)2, S2 = 1 ∑(Y</p>
<p>— Y ) , (2.17)</p>
<p>para as duas variaˆncias amostrais e para a covariância amostral utilizamos n</p>
<p>S11</p>
<p>= 1 (X n − 1 i=1</p>
<p>— X)(Yi</p>
<p>— Y )· (2.18)</p>
<p>Em particular, o coeficiente de correlação amostral é definido por b11 S11</p>
<p>R = 20</p>
<p>b02</p>
<p>= · S1S2</p>
<p>Pode ser demonstrado que |R| ≤ 1 e que os valores extremos ±1 ocorrem somente quando todos os pontos amostrais (X1, Y2), (X2, Y2), , (Xn, Yn) estão alinhados. Correspondendo a uma amostra <span class="math inline">\(X_1, X_2, \cdots , X_n\)</span> de observações em <span class="math inline">\(F\)</span> , p-ésimo quantil amostral é definido como o p-ésimo quantil da função de distribuição amostral, ou seja, como <span class="math inline">\(F\)</span> −1. Os quatis amostrais são definidos de maneira similar. Então, se 0 &lt; p &lt; 1, o quantil amostral de ordem p,</p>
<p>r = [np] se n é um número par [np] + 1 se n é um número ímpar</p>
<p>· (2.19)</p>
<p>Como usual, [x] denota o maior inteiro x. Observe que, se [n] for par, podemos escolher qualquer valor entre X([np]) e X([np]+1) como o p-ésimo quantil amostral. Então, se p = 1 e n par podemos escolher qualquer valor entre X(n/2) e X(n/2+1), os dois valores do meio, como a mediana amostral. Habitualmente é escolhido o ponto médio. Assim, a mediana amostral é definida como</p>
<p>ξb1/2 = </p>
<p>X((n+1)/2) se n é ímpar X(n/2) + X((n/2)+1) se n é par</p>
<p>· (2.20)</p>
<p>Observe que</p>
<p>2 [n + 1] = (n + 1 )</p>
<p>2 2 se n é ímpar. Consideraremos agora os momentos de características amostrais. Nos seguintes desenvolvimentos denotaremos E(Xk) = mk e E[(X µ)k] = µk como os momentos populacionais e os momentos populacionais centrais de k-ésima ordem, respectivamente. Nas situações onde utilizamos mk ou µk assumiremos que estes existem. Também, σ2 representará a variância populacional.</p>
<p>Demonstração : Para provar (2.23) observemos que</p>
<p>(∑n</p>
<p>Xi = ∑</p>
<p>X3 + 3 ∑ ∑</p>
<p>X2Xk + ∑ ∑ ∑ XiXjXk,</p>
<p>i=1</p>
<p>i=1</p>
<p>i=1 j=1 j̸=k</p>
<p>i=1 j=1 k=1 i̸=j, i̸=k j̸=k</p>
<p>desta expressão obtemos o resultado em (2.23). Similarmente</p>
<p>(∑n )4</p>
<p>( n )  n n n</p>
<p></p>
<p>∑ ∑ ∑</p>
<p>Xi =</p>
<p>∑ Xi ∑ X3 + 3 ∑ ∑ X2Xj +</p>
<p>i j k</p>
<p>i=1</p>
<p>i=1</p>
<p>n</p>
<p> i=1</p>
<p>i i=1 j=1 i̸=j n n</p>
<p>i=1 j=1 k=1 i̸=j, i̸=k j̸=k</p>
<p>= ∑ X4 + 4 ∑ ∑ XiX3 + 3 ∑ ∑ X2X2</p>
<p>i i=1</p>
<p>i=1 j=1 i̸=j</p>
<p>j i j i=1 j=1 i̸=j</p>
<p>n n n n n n n = +6 ∑ ∑ ∑ X2XjXk + ∑ ∑ ∑ ∑ XiXjXkXl·</p>
<p>i=1 j=1 k=1 i̸=j, i̸=k j̸=k</p>
<p>i=1 j=1 k=1 l=1 i̸=j, i̸=k, i̸=l j̸=k, j̸=l k̸=l</p>
<p>Um detalhe importante é que os momentos centrais podem ser calculados a partir dos momentos, por exemplo, µ2 = E[(X − µ)2] = m2 − µ2, µ3 = E[(X − µ)3] = m3 − 3µm2 + 2µ3</p>
<p>e assim por diante. Sabemos agora como calcular os momentos, até quarta ordem, de X. Vejamos a seguir como calcular os momentos centrais.</p>
<p>Demonstração : Temos que µ (X) = E(X − µ)3 = E {∑n</p>
<p>(X − µ)3} = ∑n</p>
<p>E(X</p>
<p>3 µ3 — µ) = ·</p>
<p>3</p>
<p>No caso do quarto momento central</p>
<p>n3 i=1 i</p>
<p>1</p>
<p>n3</p>
<p>{∑n</p>
<p>i=1 i n2</p>
<p>}</p>
<p>da qual obtemos que</p>
<p>µ (X) = E(X µ)4 = E n4</p>
<p>i=1</p>
<p>(Xi − µ)4 ,</p>
<p>1 µ4(X) =</p>
<p>E(Xi − µ)4 +</p>
<p>_{(4)} 1 + ∑ ∑</p>
<p>E{(Xi − µ)2(Xj − µ)2}·</p>
<p>n4 i=1</p>
<p>2 n4</p>
<p>i=1 j=1 i̸=j</p>
<p>Desenvolvendo adequadamente chegamos ao resultado em (2.26).</p>
<p><strong>Exemplo 2.16</strong></p>
<p>, Xn uma amostra aleatória da distribuição Gamma(α, β). Sabemos da Seção 1.2 que E(X) = αβ, Var(X) = αβ2</p>
<p>mk = βk(α + k − 1)(α + k − 2) α, k ≥ 1· αβ2 E(X) = αβ, Var(X) = n 1 1 µ (X) = µ = (6α3β3 + 3α2β3 + 2αβ3)· 3 n2 3 n</p>
<p>Até o momento estudamos como calcular os momentos da média amostral. Mais complexo é obter expressões para os momentos da variância amostral S2. O teorema a seguir dedica-se ao objetivo de encontrarmos expressões, até segunda ordem, dos momentos amostrais centrais. Como consequência deste resultado obtemos os momentos da variaˆncia amostral.</p>
<p>Demonstração : Temos que</p>
<p>n E(b2) = E n i=1</p>
<p>X2 n2</p>
<p>n 2 Xi i=1</p>
<p>= m2 −</p>
<p>1  n E </p>
<p>X2 + ∑</p>
<p>∑ X2Xj</p>
<p>Agora</p>
<p>= m2</p>
<p>1 — n2 [nm2</p>
<ul>
<li>n(n − 1)µ2] = ( n − 1 ) (m</li>
</ul>
<p>— µ )·</p>
<p>n2b2 =</p>
<p>n</p>
<p>i=1</p>
<p>2</p>
<p>(Xi − µ)2 − n(X − µ)2 ·</p>
<p>Escrevendo Yi = Xi − µ, vemos que E(Yi) = 0, Var(Yi) = σ2 e E(Y 4) = µ4. Temos então que</p>
<p>n2 E(b2) = E</p>
<p>n i=1 n</p>
<p>2</p>
<p>Y 2 − nY 2</p>
<p>n n</p>
<p> n n n </p>
<p>= E ∑ Y 4 + ∑ ∑ Y 2Y 2 − 2 ∑ ∑ Y 2Y 2 + ∑ Y 4</p>
<ul>
<li>1 3 ∑ ∑</li>
</ul>
<p>Y 2Y 2 + ∑</p>
<p>Y 4 ·</p>
<p>Segue então que</p>
<p>n i j i=1 j=1 i̸=j</p>
<p>i=1</p>
<p>i </p>
<p>2 1 n2 E(b2) = nµ + n(n − 1)σ2 − [n(n − 1)σ4 + nµ ] + [3n(n − 1)σ4 + nµ ]</p>
<p>= (n − 2 + 1 ) µ + (n − 2 + 3 ) (n − 1)µ2 · (µ</p>
<p>= σ2)</p>
<p>Portanto</p>
<p>n 4 n 2 2</p>
<p>Var(b2) = E(b2) − [ E(b2)]2</p>
<p>= (n − 2 +</p>
<p>1 ) µ4</p>
<ul>
<li>(n − 1) (n − 2 + 3 µ 2 —</li>
</ul>
<p>( n − 1 )2</p>
<p>= (n − 2 +</p>
<p>1 ) µ4</p>
<p>µ2 + (n − 1)(3 − n) ,</p>
<p>como afirmado. As relações (2.29) e (2.30) podem ser provadas de forma semelhante.</p>
<p>Este é justamente o motivo pelo qual chamamos S2 e não b2 de variância amostral.</p>
<p><strong>Exemplo 2.17</strong> (Continuação do Exemplo 2.16)</p>
<p>inteir Nesta situação, σ2 = αβ2, µ2 = σ2 e µ4 = m4 − 4m3µ + 6m2µ2 − 3µ4. Obtemos que E(S2) = αβ2 e Var)(S2) = µ4 + 3 − n α2β4· n n(n − 1)</p>
<p>O seguinte resultado fornece uma justificativa para a nossa definição de covariaˆncia amostral.</p>
<p>Demonstração : Do Corolário 2.17 sabemos que E(S2) = σ2 e E(S2) = σ2. Para provar que E(S11) = ρσ1σ2 1 1 2 2 observemos que Xi é independente de Xj, (i ̸= j) e de Yj, (i ̸= j). Temos que</p>
<p>Agora</p>
<p>(n − 1) E(S11) = E</p>
<p>E{(Xi − X)(Yi − Y )} =</p>
<p>n i=1</p>
<p>(Xi − X)(Yi − Y )] ·</p>
<p>( ∑n Yj</p>
<p>∑n Yj</p>
<p>∑n Xj ∑n</p>
<p>Yj )</p>
<p>e segue que</p>
<p>1 = E(XY ) − n [ E(XY ) + (n − 1) E(X) E(Y )] 1 − n [ E(XY ) + (n − 1) E(X) E(Y )] 1 − n2 [n E(XY ) + n(n − 1) E(X) E(Y )] = n − 1 [ E(XY ) E(X) E(Y )] n</p>
<p>(n − 1) E(S11) = n ( ) [ E(XY ) − E(X) E(Y )], n − 1</p>
<p>isto é</p>
<p>E(S11) = E(XY ) − E(X) E(Y ) = Cov(X, Y ) = ρσ1σ2·</p>
<p>A seguir, voltamos nossa atenção para as distribuições das características da amostra. Existem várias possi- bilidades. Se for necessária a distribuição exata o método de transformação de variáveis pode ser utilizado. As vezes, a técnica da função geradora de momentos pode ser aplicada. Assim, se <span class="math inline">\(X_1, X_2, \cdots , X_n\)</span> é uma amostra aleatória de uma população com distribuição para a qual existe a função geradora de momentos, a função geradora de momentos da média amostral X é dada por n M (t) = E(etXi/n) = [MX(t/n)]n , (2.32) i=1 onde MX é a função geradora de momentos da distribuição populacional. Se MX (t) tiver alguma forma conhecida seria possível escrever a função de probabilidade ou de densidade de X. Embora este método tem a desvantagem óbvia que se aplica apenas à distribuições para as quais existem todos os momentos, veremos sua efetividade na situação importante de amostras da distribuição normal.</p>
<p><strong>Exemplo 2.18</strong></p>
<p>Seja <span class="math inline">\(X_1, X_2, \cdots , X_n\)</span> uma amostra aleatória de tamanho n da distribuição Gama(α, 1). Nesta situação podemos encontrar a função de densidade de X. Temos que</p>
<p>MX (t) = [MX</p>
<p>(t/n)]n = 1 , t (1 − t/n)αn n</p>
<p>&lt; 1,</p>
<p>da qual obtemos que X ∼ Gama(nα, 1/n).</p>
<p><strong>Exemplo 2.19</strong></p>
<p>Seja <span class="math inline">\(X_1, X_2, \cdots , X_n\)</span> uma amostra aleatória da distribuição Uniforme no intervalo (0, 1). Considere a média geométrica</p>
<p>Yn =</p>
<p>n</p>
<p>i=1</p>
<p>1/n Xi ·</p>
<p>Sabemos que log(Yn) = (1/n) ∑n log(Xi) e, desta forma, log(Yn) é a média amostral de log(X1), , log(Xn). A função de densidade comum de log(X1), , log(Xn) é</p>
<p>ex, se x &lt; 0 f (x) = , 0, caso contrário</p>
<p>que é a distribuição exponencial negativa com parâmetro β = 1. Vemos que a função geradora de momentos de log(Yn) é dada por</p>
<p>Mlog(Yn)</p>
<p>n (t) = E(et log(Xi)/n) = , (1 t/n)n i=1</p>
<p>e a função de densidade de log(Yn) é dada por</p>
<p>flog(Yn)</p>
<ol start="25" type="a">
<li>= </li>
</ol>
<p>nn Γ_{(n)}[−y]</p>
<p>n−1</p>
<p>eny</p>
<p>, se − ∞ &lt; y &lt; 0 ·</p>
<p> 0, caso contrário</p>
<p>Segue então que Yn tem por função de densidade</p>
<p>fYn</p>
<ol start="25" type="a">
<li>= </li>
</ol>
<p>nn y Γ_{(n)}</p>
<p>n−1</p>
<p>[− log(y)]</p>
<p>n−1</p>
<p>, se 0 &lt; y &lt; 1 ·</p>
<p></p>
<p>Voltemos ao quantil amostral de ordem p,</p>
<p>0, caso contrário</p>
<p>ξbp, o qual sabemos é ou X([np]) ou X([np]+1) dependendo se [np] é</p>
<p>um número par ou ímpar, como definido em (2.19). Simplificando, vamos discutir as propriedades de X([np]), onde p ∈ (0, 1) e n é grande. Isso, por sua vez, nos informará sobre as propriedades de ξp. Primeiro observemos que, se U1, U2, , Un é uma amostra aleatória da distribuição U (0, 1) então, pelo Teorema 2.3, temos que</p>
<p>do qual obtemos que</p>
<p>U([np]) ∼ Beta([np], n − [np] + 1),</p>
<p>[np]</p>
<p>E(U([np])) =</p>
<p>n + 1</p>
<p>n−→→∞ p,</p>
<p>Cov(U , U</p>
<p>) = n <a href="n − [np2] + 1">np1</a></p>
<p>−→ p (1 − p )·</p>
<p>Utilizando este resultado e a desigualdade de Chebychev, demonstramos que U −P→ p· (2.33)</p>
<p>Isso gera a questão</p>
<p>ξbp −→ ξp?</p>
<p>qualquer seja a distribuição da amostra aleatória X1, , Xn. Para respondermos a pergunta acima vamos utilizar o Lema de Hoeffding, ou seja, para respondermos se o quantil amostral de ordem p converge em probabilidade para o quantil teórico correspondente, utilizaremos o seguinte resultado devido a Hoeffding (1963).</p>
<p>Demonstração : Dado que as variáveis aleatórias são limitadas ao intervalo (0, 1), sabemos que ehX ≤ (1 − X) + Xeh, isto deve-se a que a função exponencial ehX é convexa e, portanto, seu gráfico é limitado por cima no intervalo 0 ≤ X ≤ 1 pela linha que conecta as ordenadas X = 0 e X = 1. Então E(ehX ) ≤ (1 − E(X)) + E(X)eh· (2.35)</p>
<p>Seja Sn = ∑n</p>
<p>Xi. Sabemos que</p>
<p>P (Sn − E(Sn) ≥ nt) = E(1[Sn− E(Sn)−nt≥0]),</p>
<p>também sabemos que</p>
<p>1[Sn− E(Sn)−nt≥0] ≤ exp (h(Sn − E(Sn) − nt)),</p>
<p>qualquer seja h uma constante positiva arbitrária. Então P Sn − E(Sn) ≥ nt ≤ E eh(Sn− E(Sn)−nt) (2.36) e como estamos assumindo que as variáveis são independentes, podemos escrever</p>
<p>( ( ))</p>
<p>∏ ( ( ))</p>
<p>Escrevendo µi = E(Xi) temos, pela expressão em (2.35) que E(eh(Xi−µi)) ≤ e−hµi ((1 − µi) + µieh) = ef(h), (2.38) onde <span class="math inline">\(F\)</span> (h) = −hµi + ln(1 − µi + µieh). As primeiras duas derivadas são:</p>
<p>′ µi</p>
<p>′′ µie−h(1 − µi)</p>
<p>f (h) = −µi + e−h(1 − µ ) + µ</p>
<p>e f (h) = [µi</p>
<ul>
<li><p>e−h(1 − µ )]2 ·</p>
<p>µi<br>
Na segunda derivada, escolhendo u = µ + e−h(1 − µ ) 0 &lt; u &lt; 1. Portanto, <span class="math inline">\(F\)</span> ′′(h) ≤ 1 . Pela série de Taylor</p></li>
</ul>
<p>vemos que este quociente é da forma u(1 − u), sendo</p>
<p>Então, pela expressão em (2.38)</p>
<p>f (h) ≤</p>
<p>f (0) + <span class="math inline">\(F\)</span> ′(0)h +</p>
<p>1 h2 = 8</p>
<p>1 h2· 8</p>
<p>Substituindo em (2.36) temos que</p>
<p>E(eh(Xi−µi)) ≤ e 1 h2 ·</p>
<p>P (Sn</p>
<p>— E(Sn</p>
<p>) ≥ nt) ≤ e−nht+ 1 nh2 ,</p>
<p>e o mínimo no expoente é atingido quando h = 4t. Então, o mínimo do limite superior da probabilidade é exp(−2nt2).</p>
<p>Devemos lembrar que esta não é a única maneira de termos uma taxa de convergência para Teorema do Limite Central. Por exemplo, se Y1, Y2, , Yn forem variáveis aleatórias independentes e identicamente distribuídas, utilizando o Teorema de Berry-Esseen2, temos que</p>
<p>( ∑n ∑</p>
<p>) ( √ Var(Y1))</p>
<p>C E|Y1</p>
<p>— E(Y1)|</p>
<p>P i=1</p>
<p>Xi −</p>
<p>i=1</p>
<p>E(Xi) ≥ nt ≤ Φ t n</p>
<ul>
<li>√n</li>
</ul>
<p>Var3/2(Y ) ·</p>
<p>2</p>
<p>Demonstração : Berry (1941); Esseen (1942).</p>
<p>Pode-se consultar o livro de Feller (1971) para uma demonstração moderna.</p>
<p><strong>Exemplo 2.20</strong></p>
<p>Caso a amostra aleatória seja Bernoulli(µ), temos que n Xk ∼ Binomial(n, µ)· i=1 Então, segundo a desigualdade de Hoeffding</p>
<p>P (X − µ ≥ t) ≤ exp(−2nt2)· Uma vantagem da desigualdade no Lema de Hoeffding é que não assume-se conhecimento da variância e, em geral, o limite da probabilidade é mais acurado do que outras desigualdades. Caso as variáveis aleatórias sejam limitadas como a ≤ Xi ≤ b, com a &lt; b, o limite superior da desigualdade (2.34) seria exp − 2nt2/(b − a)2 .</p>
<p><strong>Exemplo 2.21</strong></p>
<p>Sejam X1, , Xn variáveis aleatórias com distribuição U ( 1, 1). Nesta situação E(X) = 0, a = 1 e b = 1. A desigualdade de Hoeffding assume a forma P (X ≥ t) ≤ exp ( − nt2/2)·</p>
<p>Demonstração : Para ϵ &gt; 0 qualquer, podemos escrever P (|ξp − ξp| &gt; ϵ) = P (ξp &gt; ξp + ϵ) + P (ξp &lt; ξp − ϵ)· Pelo Teorema 2.9, podemos escrever P (ξbp &gt; ξp + ϵ) = P (p &gt; Fbn(ξp + ϵ))</p>
<p>n = P i=1</p>
<p>1[Xi&gt;ξp+ϵ] &gt; n(1 − p))</p>
<p>n = P i=1</p>
<p>Vi −</p>
<p>∑i=1</p>
<p>E(Vi) &gt; nδ1),</p>
<p>onde Vi = 1[Xi&gt;ξp+ϵ] e δ1 = <span class="math inline">\(F\)</span> (ξp + ϵ) − p.&nbsp;Da mesma forma, P (ξbp &lt; ξp − ϵ) = P (p &gt; Fbn(ξp − ϵ))</p>
<p>n = P i=1</p>
<p>Wi −</p>
<p>∑i=1</p>
<p>E(Wi) &gt; nδ2),</p>
<p>onde Wi = 1[Xi&lt;ξp−ϵ] e δ2 = p − <span class="math inline">\(F\)</span> (ξp − ϵ) − p.&nbsp;Portanto, utilizando o Lema de Hoeffding (Lema 2.20), temos P (ξbp &gt; ξp + ϵ) ≤ exp(−2nδ2) P (ξbp &lt; ξp − ϵ) ≤ exp(−2nδ2)· Colocando δϵ = min{δ1, δ2}, a prova está completa.</p>
<p>Demonstramos que</p>
<p>lim P (|ξbp − ξp| &gt; ϵ) ≤ lim 2 exp(−2nδ2) = 0,</p>
<p>o qual significa que ξp −→ ξp. Em outras palavras, sempre que ξp seja solução única da desigualdade <span class="math inline">\(F\)</span> (ξp ) ≤ p ≤ <span class="math inline">\(F\)</span> (ξp), 0 &lt; p &lt; 1, o quantil amostral converge em probabilidade para o quantil populacional e isto sempre acontece nas distribuições contínuas. Um detalhe importante é que para demonstrarmos a convergência em probabilidade de ξp utilizamos o Lema de Hoeffding e ele depende da existência da esperança. O seguinte resultado fornece a distribuição assintótica da r-ésima estatística de ordem amostral de uma po- pulação com uma função de distribuição <span class="math inline">\(F\)</span> , absolutamente contínua, e função de densidade <span class="math inline">\(F\)</span> .</p>
<p>Demonstração : Vamos demonstrar somente para o caso p = 1/2. Observemos que ξ1/2 é mediana única dado que f (ξ1/2) &gt; 0. Primeiro, consideremos que n seja ímpar, por exemplo, n = 2m − 1, logo P [√n(X(m) − <span class="math inline">\(F\)</span> −1(1/2)) ≤ t] = P (X(m) ≤ t/√n + <span class="math inline">\(F\)</span> −1(1/2))·</p>
<p>Seja Sn o número de X que excedem t/ n + F (1/2). Então</p>
<p>Percebemos que</p>
<p>t X(m) ≤ √n + F</p>
<p>(1/2) se, e somente se, Sn ≤ m − 1 =</p>
<p>n − 1 · 2</p>
<p>Sn ∼ Binomial(n, 1 − <span class="math inline">\(F\)</span> (F −1(1/2) + t/√n))· Fazendo pn = 1 − <span class="math inline">\(F\)</span> (F −1(1/2) + t/√n), temos que</p>
<p>P [√n(X</p>
<ol start="13" type="a">
<li></li>
</ol>
<p>— F −1(1/2)) ≤ t] = P (Sn</p>
<p>≤ n − 1 )</p>
<p>( Sn − npn 1 (n − 1) − npn )</p>
<p>= P</p>
<p>Utilizando o Teorema de Berry-Esseen, temos que</p>
<p>√npn(1 − pn) ≤ √npn(1 − p ) · n</p>
<p>{ ( n − 1 ) ( 1 (n − 1) − npn )}</p>
<p>lim P n→∞</p>
<p>Sn ≤ 2</p>
<p>— Φ √np</p>
<p>(1 − pn)</p>
<p>= 0·</p>
<p>Escrevendo</p>
<p>1 (n − 1) − npn npn(1 − pn)</p>
<p>=</p>
<p>√n( 1 − pn) 1/2 √n( − 1 + <span class="math inline">\(F\)</span> (t/√n + <span class="math inline">\(F\)</span> −1(1/2)))</p>
<p>= 2t</p>
<p>1/2 F (t/√n + <span class="math inline">\(F\)</span> −1(1/2)) − <span class="math inline">\(F\)</span> (F −1(1/2))</p>
<p>−→ 2tf</p>
<p>(F −1(1/2))·</p>
<p>Então</p>
<p>( 1 (n − 1) − npn )</p>
<p>( ( −1 ))</p>
<p>Φ npn ou</p>
<p>(1 − pn)</p>
<p>≈ Φ 2tf F</p>
<p>(1/2)</p>
<p>√n(X</p>
<ol start="13" type="a">
<li>− F</li>
</ol>
<p>( 1 )) −D→ N (0,</p>
<p>4f 2</p>
<p>1 (F −1(1/2)</p>
<p>)) ·</p>
<p>Quando n é par, digamos n = 2m, ambos P (√n X(m) F −1(1/2) t) quanto P (√n X(m+1) F −1(1/2) t) convergem a Φ(2tf (F −1(1/2))).</p>
<p>Observe que o quantil amostral de ordem p, assintótica</p>
<p>ξbp, como consequência do Teorema 2.23, tem por distribuição</p>
<p>N (ξ , 1 p(1 − p)) , onde ξp é o correspondente quantil populacional e <span class="math inline">\(F\)</span> é a função de densidade populacional. Por exemplo, suponha temos uma amostra aleatória da di√stribuição N (µ, σ2) de tamanho n.&nbsp;Seja ξb1/2 a mediana amostral obtida dessa b ( πσ2 )</p>
<p>Também devemos ter em consideração que para demonstrarmos o Teorema 2.23 utilizamos a Teorema de Berry- Esseen, o qual depende da existência dos primeiros dois momentos da variável aleatória. Com isso, caso X Cauchy(µ, σ), o Teorema 2.23 não se aplica.</p>
</section>
<section id="gráficos-descritivos" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="gráficos-descritivos"><span class="header-section-number">2.5</span> Gráficos descritivos</h2>
<p>Vejamos alguns conjuntos de dados disponíveis na linguagem de programação R (R Core Team, 2014), especifi- camente na libraria datasets, que nos permitiram mostrar a utilidade dos momentos amostrais para resumir as informações contidas nos dados. Para consultar estes conjuntos de dados basta digitar library(help = “datasets”) Alguns dos diversos exemplos disponíveis serão apresentados aqui.</p>
<p><strong>Exemplo 2.22</strong> (Puromicina)</p>
<p>Os dados sobre a velocidade de uma reação enzimática são obtidos por Treloar (1974) e disponíveis no arquivo de dados Puromycin. O número de contagens por minuto de produto radioativo a partir da reação foi medida como uma função da concentração do substrato em partes por milhão (ppm) e a partir destas contagens a taxa</p>
<p>inicial (ou velocidade) da reação foi calculada (contagens/min/min). O experimento foi realizado uma vez com a enzima tratada com puromicina e depois com a enzima não tratada. A estrutura destes dados tem 23 linhas e 3 colunas, cada coluna contendo as informações das variáveis: conc: um vector numérico de concentrações de substrato (ppm); rate: um vector numérico de taxas de reação instantânea (contagens/min/min); state: um fator com níveis treated (tratada) ou untreated (não tratada). Para a leitura e observação dos nomes das variáveis utilizamos os comandos a seguir: data(Puromycin) names(Puromycin) Uma maneira de obtermos estatísticas descritivas é utilizando as linhas de comando a seguir: summary(rate[state==’treated’]) Min. 1st Qu. Median Mean 3rd Qu. Max. 47.0 104.5 145.5 141.6 193.2 207.0 e summary(rate[state==’untreated’]) Min. 1st Qu. Median Mean 3rd Qu. Max. 51.0 85.0 115.0 110.7 137.5 160.0 para o caso da variável rate, as concentrações, obtidas as estatísticas descritivas segundo os níveis do fator state, se as concentrações foram ou não tratadas com puromicina. No caso das estatísticas descritivas acerca das concentrações de substrato, variável conc, temos: summary(conc[state==’treated’]) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.020 0.060 0.165 0.345 0.560 1.100 e summary(conc[state==’untreated’]) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.0200 0.0600 0.1100 0.2764 0.3900 1.1000 Os valores mínimos é máximos foram registrados sempre com os nomes de Min. e Max., respectivamente. O primeiro e terceiro quantis ou quantis de 25% e 75% respectivos são registrados com os nomes 1st Qu. e 3rd Qu. e, finalizando, o resumo de informações de estatísticas de posição temos os valores de medianas (Median) e médios (Mean).</p>
<p><strong>Exemplo 2.23</strong> (Rock )</p>
<p>Medições em 48 amostras de rochas de um reservatório de petróleo estão disponíveis no arquivo de dados rock. Este conjunto de dados contem 48 linhas e 4 colunas numéricas, descritas a seguir: area área do espaço de poros, em pixels de 256 por 256; peri perímetro em pixels; shape perímetro/sqrt(area) perm permeabilidade em mili-Darcies. Doze amostras do núcleo de reservatórios de petróleo foram amostrados por 4 seções transversais. Cada amostra foi medida no núcleo para a permeabilidade e cada seção transversal tem uma área total de poros, perímetro total de poros e forma. A fonte destes dados é a BP Research e a análise das imagens foi de Ronit Katz, Oxford University. Na geologia, a permeabilidade é a medida da capacidade de um material (tipicamente uma rocha) para transmitir fluídos. E´ de grande importância na determinação das características de fluxo dos hidrocarbonetos em reservatórios de petróleo e gás e da água nos aquíferos. A unidade de permeabilidade é o Darcy ou, mais habitualmente, o mili- Darcy ou mD.</p>
<section id="gráfico-de-boxplot" class="level3" data-number="2.5.1">
<h3 data-number="2.5.1" class="anchored" data-anchor-id="gráfico-de-boxplot"><span class="header-section-number">2.5.1</span> 2.4.1 Gráfico de Boxplot</h3>
<p>Em 1977, John Tukey (Tukey, 1977) publicou uma proposta que posteriormente foi reconhecida como sendo um eficiente método para mostrar cinco número que sumarizam qualquer conjunto de dados. O gráfico proposto é chamado de boxplot (também conhecido como box and whisker plot) e resume as seguintes medidas de posição estatísticas: mediana, quantis inferior e superior e os valores mínimos e máximos. Os quantis inferior e superior entendem-se serem os quantis de 25% e 75%, respectivamente. No caso do exemplo 2.22, deixamos a disposição os dados digitando attach(Puromycin) e com isso podemos mudar o nome dos níveis do fator da forma state=factor(state,labels=c(’Tratada’,’N~ao tratada’)) Então, com os comandos a seguir geramos o gráfico de boxplot, tanto para a variável rate quanto para a variável conc, estas segundo os níveis do fator state. par(mar=c(5,4,3,1)) boxplot(rate ~ state, col = grey(c(0.4,1)), main=’Taxas de reaç~ao instant^anea’)</p>
<p>para o caso do rate. Observemos que a primeira linha par(mar=c(5,4,3,1)) serve somente para dimensionar a janela gráfica. Para o caso da variável conc utilizamos comandos semelhantes. par(mar=c(5,4,3,1)) boxplot(conc ~ state, col = grey(c(0.4,1)), main=’Concentraç~oes de substrato’) O resultado deste trabalho pode ser observado na Figura 2.3. Interpretemos o gráfico de boxplot. A caixa (box) propriamente contém a metade 50% dos data. O limite superior da caixa indica o percentil 75% dos dados e o limite inferior da caixa indica o percentil 25%. A distancia entre esses dois quantis é conhecida como inter-quantil. A linha na caixa indica o valor de mediana dos dados. Se a linha mediana dentro da caixa não é equidistante dos extremos, diz-se então que os dados são assimétricos. O boxplot da variável rate (esquerda na Figura 2.3) é um exemplo de dados simétricos já a situação da variável conc (direita na Figura 2.3) é um caso clássico de assimetria dos dados. Os extremos do gráfico indicam os valores mínimo e máximo, a menos que valores outliers3 estejam presentes, nesse caso o gráfico de estende ao máximo de 1.5 vezes da distância inter-quantil. Os pontos fora do gráfico são então outliers ou suspeitos de serem outliers. Mais elegante seria utilizar a biblioteca de funções ggplot2, para isso, digitamos: library(ggplot2) Para gerar os gráficos de boxplot respectivos, fazemos: par(mar=c(5,4,3,1)) qplot(state, rate, geom=c(“boxplot”, “jitter”), main=“Taxas de reaç~ao instant^anea”, xlab=““, ylab=” “) e par(mar=c(5,4,3,1)) qplot(state, conc, geom=c(”boxplot”, “jitter”), main=“Concentraç~oes de substrato”, xlab=““, ylab=” “)</p>
<p>3Em estatística, outlier, valor aberrante ou valor atípico, é uma observação que apresenta um grande afastamento das demais observações em uma amostra. A existência de outliers implica, tipicamente, em prejuízos a interpretação dos resultados dos testes estatísticos aplicados as amostras.</p>
<p>Taxas de reação instantânea Concentrações de substrato</p>
<p>Tratada Não tratada Tratada Não tratada</p>
<p>Figura 2.3: Gráfico de boxplot da variável rate à esquerda e da variável conc à direita, segundo os níveis do fator state, se a enzima foi tratada ou não com puromicida. Gráfico gerado utilizando a função R boxplot,</p>
<p>Taxas de reação instantânea Concentrações de substrato</p>
<p>200</p>
<p>0.9</p>
<p>150</p>
<p>0.6</p>
<p>100 0.3</p>
<p>50</p>
<p>Tratada Não tratada</p>
<p>0.0</p>
<p>Tratada Não tratada</p>
<p>Figura 2.4: Gráfico de boxplot da variável rate à esquerda e da variável conc à direita, segundo os níveis do fator state, se a enzima foi tratada ou não com puromicida. Gráfico gerado utilizando a função R qplot, opção geom=c(”boxplot”, ”jitter”).</p>
<p>obtendo-se assim os gráficos na Figuras 2.4. Além de melhor qualidade gráfica acrescentamos os pontos observados no boxplot, isso permite termos uma ideia também da dispersão dos dados. Vejamos as vantagens do boxplots. Mostra graficamente a posição central dos dados (mediana) e a tendência. Fornece algum indicativo de simetria ou assimetria dos dados. Ao contrário de muitas outras formas de mostrar os dados, o boxplots mostra os outliers. Utilizando o boxplot para cada variável categórica no mesmo gráfico, pode-se facilmente comparar os dados. Esta é a situação no exemplo na Figura 2.3, podemos observar o comportamento das variáveis rate e conc segundo os níveis do fator state. Um detalhe do boxplot é que ele tende a enfatizar as caudas da distribuição, que são os pontos ao extremo nos dados. Também fornece detalhes da distribuição dos dados. Mostrar o histograma (Seção 2.4.2) em conjunto com o boxplot ajuda a entender a distribuição dos dados, constituindo estes dos gráficos ferramentas importantes na análise exploratória. Logicamente, o comportamento dos dados dentro da caixa (box), como podemos perceber nas figuras 2.3 e 2.4, permanece um mistério. Isso porque caso estejam os dados bem espalhados ou não, o gráfico boxplot continua mostrando uma caixa. Somente perceberemos algum comportamento diferente se o valor da mediana estiver mais próximo de um dos extremos desta caixa. Para tentar diminuir essa limitação foi sugerido uma melhoria, obtendo-se o chamada boxplot entalhado (notched boxplot). Com as linhas de comando a seguir se obtém os gráficos na Figura 2.5.</p>
<p>par(mar=c(5,4,3,1)) boxplot(rate ~ state, col = grey(c(0.4,1)), notch=TRUE, main=’Taxas de reaç~ao instant^anea’)</p>
<p>e par(mar=c(5,4,3,1)) boxplot(conc ~ state, col = grey(c(0.4,1)), notch=TRUE, main=’Concentraç~oes de substrato’)</p>
<p>Observa-se que a única diferença é a inclusão da opção notch=TRUE, permanecendo todas as outras instruções iguais. Mais elaborado é o chamado violin plot, mistura de boxplot com estimação de densidade, tema este tratado na Seção 4.3. Este gráfico, introduzido no artigo Hintze &amp; Nelson (1998), sinergicamente combina o gráfico de boxplot e a estimação da densidade, também chamado de histograma suavizado, em uma única tela que revela a estrutura encontrada nos dados. Com as linhas de comando a seguir se obtém os gráficos na Figura 2.6.</p>
<p>par(mar=c(5,4,3,1)) qplot(state, rate, geom = c(“violin”, “jitter”), notch=TRUE, main=“Taxas de reaç~ao instant^anea”, xlab=““, ylab=” “)</p>
<p>e par(mar=c(5,4,3,1)) qplot(state, conc, geom=c(“violin”, “jitter”), notch=TRUE, main=“Concentraç~oes de substrato”, xlab=““, ylab=” “)</p>
<p>Este gráfico é similar ao boxplot excepto que mostra também a densidade de probabilidade dos dados. Pode incluir também um marcador para a média dos dados e uma caixa que indica a distância interquartil, como nos gráficos boxplot. O objetivo do gráfico violin plot é o mesmo do que o boxplot original porém, considera de alguma maneira o comportamento dos dados dentro da caixa (box). Assim, percebemos melhor a distribuição dos dados dentro do intervalo interquartil.</p>
<p>Taxas de reação instantânea Concentrações de substrato</p>
<p>Tratada Não tratada Tratada Não tratada</p>
<p>Figura 2.5: Gráfico de boxplot entalhado da variável rate à esquerda e da variável conc à direita, segundo os níveis do fator state, se a enzima foi tratada ou não com puromicida. Gráfico gerado utilizando a função R qplot, opção geom=c(”boxplot”, ”jitter”), notch=TRUE.</p>
<p>Taxas de reação instantânea Concentrações de substrato</p>
<p>200</p>
<p>0.9</p>
<p>150</p>
<p>0.6</p>
<p>100 0.3</p>
<p>50</p>
<p>Tratada Não tratada</p>
<p>0.0</p>
<p>Tratada Não tratada</p>
<p>Figura 2.6: Gráfico de violin plot da variável rate à esquerda e da variável conc à direita, segundo os níveis do fator state, se a enzima foi tratada ou não com puromicida. Gráfico gerado utilizando a função R qplot, opção geom=c(”vioplot”, ”jitter”), notch=TRUE.</p>
</section>
<section id="histograma" class="level3" data-number="2.5.2">
<h3 data-number="2.5.2" class="anchored" data-anchor-id="histograma"><span class="header-section-number">2.5.2</span> 2.4.2 Histograma</h3>
<p>Um histograma é uma representação gráfica da função de probabilidades ou da função de densidade de um conjunto de dados independentes e foi introduzido pela primeira vez por Karl Pearson4. A representação mais comum do histograma é um gráfico de barras verticais. A palavra histograma é de origem grega, derivada de duas: histos que pode significar testemunha no sentido de aquilo que se vê, como as barras verticais do histograma, e da também palavra grega gramma que significa desenhar, registrar ou escrever. Histograma Histograma com a curva norma</p>
<p>−2 −1 0 1 2 Dados simulados</p>
<p>−2 −1 0 1 2 Dados simulados</p>
<p>Figura 2.7: Gráfico de histograma para dados simulados.</p>
<p>Para construir um exemplo controlado do gráfico de histograma, simulamos uma amostra de tamanho 150 da distribuição normal padrão, com o comando x=rnorm(150) e, depois, construímos um gráfico colorido com as linhas de comando par(mar=c(5,4,2,1)) hist(x, breaks=12, col=“red”, xlab=“Dados simulados”, ylab=’Frequ^encia’, main=“Histograma”) box() Posteriormente, acrescentamos a este gráfico uma linha com a densidade normal par(mar=c(5,4,2,1)) h=hist(x, breaks=10, col=“red”, xlab=“Dados simulados”, ylab=’Frequ^encia’, main=“Histograma com a curva normal”) xfit=seq(min(x),max(x),length=40) yfit=dnorm(xfit,mean=mean(x),sd=sd(x)) yfit=yfit<em>diff(h$mids[1:2])</em>length(x) lines(xfit, yfit, col=“blue”, lwd=2) box()</p>
<p>4Pearson, K. (1895). Contributions to the Mathematical Theory of Evolution. II. Skew Variation in Homogeneous Material. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 186: 343-414.</p>
<p>Desta forma geramos os gráficos na Figura 2.7. A ideia é mostrar que o histograma assemelha-se ao gráfico da densidade normal, a densidade dos dados.</p>
<p>Histograma c2(6) Histograma c2(6)</p>
<p>2 4 6 8 10 12 14 14 intervalos</p>
<p>2 4 6 8 10 12 14 26 intervalos</p>
<p>Figura 2.8: Histogramas da distribuição χ2 com 6 graus de liberdade. Número de intervalos 14 e 26, respectivamente.</p>
<p>O histograma é um gráfico composto por retângulos justapostos em que a base de cada um deles corresponde ao intervalo de classe e a sua altura à respectiva frequência. A construção de histogramas tem caráter preliminar em qualquer estudo e é um importante indicador da distribuição de dados. Pode indicar se uma distribuição aproxima-se de uma densidade normal como pode indicar mistura de densidades, quando os dados apresentam várias modas. Os histogramas podem ser um mau método para determinar a forma de uma distribuição porque são fortemente influenciados pelo número de intervalos utilizados. Por exemplo, decidimos gerar 50 amostras da densidade χ2(6), da forma set.seed(5678) z=rchisq(50, df=6) Os gráficos de histogramas correspondentes com 14 e 26 intervalos são apresentados na Figura 2.8 e foram gerados com as linhas de comando</p>
<p>par(mar=c(5,4,2,1)) hist(z, breaks=14, col=“blue”, main=expression(paste(’Histograma ’, chi^2,’(6)’)), ylab=’Frequ^encia’, xlab=’14 intervalos’) box()</p>
<p>e</p>
<p>par(mar=c(5,4,2,1)) hist(z, breaks=26, col=“blue”, main=expression(paste(’Histograma ’, chi^2,’(6)’)), ylab=’Frequ^encia’, xlab=’26 intervalos’) box()</p>
<p>Na Figura 2.9 podemos observar os gráficos de histograma obtidos das variáveis descritas no Exemplo 2.23. A situação em (a) representa o caso em a distribuição dos dados de assemelha à distribuição normal, já a situação descrita no gráfico em (b) mostra-se uma mistura de densidades, percebemos a existência de duas modas. (a) Área do espaço de poros (b) Perímetro em pixels</p>
<p>0 4000 8000 12000 pixels de 256 x 256</p>
<p>0 1000 3000 5000</p>
<p>Figura 2.9: Histogramas das variáveis no Exemplo 2.23.</p>
<p>Outras situações no mesmo exemplo, mas diferentes variáveis, são descritas nos gráficos na Figura 2.10. Nessa figura apresentamos dois gráficos, chamados de (c) e (d), nesta figura. Correspondem, como podemos observar, à distribuições assimétricas e descrevem os dados coletados nas variáveis shape e perm do arquivo de dados Rock, Exemplo 2.23. Os histogramas foram pensados somente para o caso de variáveis contínuas, porém é uma descrição discreta delas. Logicamente, também podemos utiliza-los em situações de variáveis aleatórias discretas, nada impede isso. Estas figuras foram geradas utilizando a configuração padrão do comando hist, isto é, utilizamos uma maneira automática de determinar o número de intervalos, mais adiante dedicamos maior atenção a diferentes formas de calcular este número. Como pode ter sido observado, além de não ficar claro como determinar o número de intervalos nem como delimitar os intervalos, também não ficou claro o que queremos realmente observar com o gráfico desta função. Vejamos agora uma definição mais clara do histograma, esta definição nos permitirá obter propriedades impor- tantes.</p>
<ol start="3" type="a">
<li>Perímetro/sqrt(Área) (d) Permeabilidade</li>
</ol>
<p>0.1 0.2 0.3 0.4 0.5</p>
<p>0 200 600 1000 1400 mili−Darcies</p>
<p>Figura 2.10: Histogramas das variáveis no Exemplo 2.23.</p>
<p>Foi provado por Robertson (1967) que, dados os intervalos I1, I2, , Ik, o histograma <span class="math inline">\(F\)</span> é um estimador de máxima verossimilhança5 dentre os estimadores expressados como funções simples e semicontínuas superiormente, isto se o fecho de cada intervalos contiver duas ou mais observac¸ões. Os gráficos apresentados nas figuras 2.7, 2.9 e 2.10 são histogramas também segundo a proposta de Robertson (1967). Pode-se observar que este estimador tem duas limitações importantes: a dependência do comprimento do intervalo e o fato de o histograma não constituir uma função contínua. A primeira destas limitações foi amplamente estudada por Wegman (1975). Ele provou que os pontos extremos de cada intervalo Ik devem ser coincidentes com observações e que, se o número mínimo de observações em cada intervalo aumente, conforme aumenta o tamanho da amostra, o estimador <span class="math inline">\(F\)</span> é consistente6. A segunda limitação importante do histograma, isto é, o fato de ele não constituir uma função contínua, incentivou diversos estudos na procura de estimadores contínuos da função de densidade. No Capítulo 3, a Seção 4.3 dedica-se a mostrar estimadores contínuos da função de densidade.</p>
<p>5Os estimadores de máxima verossimilhanc¸a serão estudados na Seção 4.2 6Estimadores consistentes serão estudados na Seção 3.1.1</p>
<p>Cálculo automático do número de intervalos num histograma Uma questão importante é determinar de maneira automatizada o número de intervalos disjuntos que serão utili- zados para a construção do gráfico. Uma primeira forma de escolher o número de intervalos foi dada por Sturges (1926) e que constitui a forma padrão no R. Conhecida como fórmula de Sturges é dada por k = [log2_{(n)} + 1], (2.41) isto significa que o número de intervalos é a parte inteira do logaritmo base 2 do número de observações mais 1. Outras expressões comumente utilizadas são a fórmula de Scott (Scott, 1979) h = 3.5s/√3 n, onde s é o desvio padrão e a fórmula de Freedman Diacconi (Freedman &amp; Diaconis, 1981) h = 2IQR(x)/√3 n, onde IRQ é a diferença entre o terceiro e o primeiro quantil.</p>
<p><strong>Exemplo 2.24</strong></p>
<p>Na libraria de funções R robustbase temos disponíveis dados do teor de cálcio e do pH em amostras de colo coletadas em diferentes comunidades da região de Condroz, na Bélgica. Podemos ler estes dados digitando as linhas de comando abaixo, primeiro para escolher a libraria de funções e depois para selecionar os dados. library(robustbase) data(condroz) Temos registadas duas variáveis: Ca que registra o tero de cálcio na amostra de solo e o pH, o pH corres- pondente. Construímos histogramas da variável Ca segundo a três formas de escolha do número de intervalos e os apresentamos na Figura 2.11. Os dados deste exemplo foram publicados em: Hubert, M. and Vandervieren, E. (2006). An Adjusted Boxplot for Skewed Distributions, Technical Report TR-06-11, KULeuven, Section of Statistics, Leuven.</p>
<p>Sturges</p>
<p>Scott</p>
<p>Freedman−Diaconis</p>
<p>0 1000 2000 3000 4000 Ca</p>
<p>0 1000 2000 3000 4000 Ca</p>
<p>0 1000 2000 3000 4000 Ca</p>
<p>Figura 2.11: Diferentes histogramas da variável Ca no Exemplo 2.24.</p>
</section>
<section id="gráficos-para-verificar-normalidade" class="level3" data-number="2.5.3">
<h3 data-number="2.5.3" class="anchored" data-anchor-id="gráficos-para-verificar-normalidade"><span class="header-section-number">2.5.3</span> 2.4.3 Gráficos para verificar normalidade</h3>
<p>Um primeiro gráfico chamado de qq-norm permite a comparação de duas distribuições de probabilidades traçando seus quantis uns contra os outros. Depois exploramos um gráfico mais recente, conhecido como worm plot (gráfico de minhoca), consistindo numa determinada coleção de de qq-norm.</p>
<p>QQ-norm O gráfico quantil-quantil ou qq-plot, proposto por Wilk &amp; Gnanadesikan (1968), é um dispositivo gráfico explo- ratório utilizado para verificar a validade de um pressuposto de distribuição para um conjunto de dados. Em geral, a ideia básica é a de calcular o valor teoricamente esperado para cada ponto de dados com base na distribuição em questão. Se os dados de fato seguirem a distribuição assumida os pontos deste gráfico formarão aproximadamente uma linha reta. Percebemos que podemos verificar com este gráfico qualquer densidade contínua, eventualmente pode ser uti- lizado também para funções de probabilidade. O qq-plot vai apresentar-se como uma linha reta se a densidade assumida estiver correta. Vejamos o caso particular de verificarmos se a densidade é normal, nesta situação o gráfico qq-plot será chamado de qq-norm. Primeiro consideraremos a situação da densidade normal padrão. Seja z1, z2, , zn uma amostra aleatória de uma distribuição normal com média µ = 0 e desvio padrão σ = 1. As estatísticas de ordem amostrais são z_{(1)} ≤ z_{(2)} ≤ ≤ z_{(n)}· Estes valores desempenharão o papel dos quantis da amostra. Agora, quais devemos tomar como os quantis teóricas correspondentes? Se a função de distribuição cumulada da densidade normal padrão fosse denotada por Φ, usando a notação quantil, se ξq é o q-ésimo quantil de uma distribuição normal, então Φ(ξq) = q, ou seja, a probabilidade de uma amostra normal ser inferior a ξq é, de fato, apenas q. Considere o primeiro valor ordenado z_{(1)}. O que podemos esperar que o valor Φ(z_{(1)}) seja? Intuitivamente, esperamos que essa seja a probabilidade de assumir um valor no intervalo (0, 1/n). Do mesmo modo, espera-se que Φ(z_{(2)}) seja a probabilidade de assumir um valor no intervalo (1/n, 2/n). Continuando, esperamos que Φ(z_{(n)}) seja a probabilidade de assumir um valor no intervalo (n 1)/n, 1). Assim, o quantil teórico desejamos seja definido pelo inverso da função de distribuição acumulada normal padrão. Em particular, o quantil teórico correspondente ao quantil empírico z_{(i)} deve ser</p>
<p>para i = 1, 2, , n.</p>
<p>ξ = q i − 0, 5 , q n</p>
<p>QQ−plot nomal</p>
<p>QQ−plot nomal</p>
<p>QQ−plot nomal</p>
<p>−3 −2 −1 0 1 2 3 Quantis teóricos</p>
<p>−3 −2 −1 0 1 2 3 Quantis teóricos</p>
<p>−3 −2 −1 0 1 2 3 Quantis teóricos</p>
<p>Figura 2.12: Diferentes qqplot para dados normais.</p>
<p>Na Figura 2.12, a esquerda acima exibimos o qq-norm de uma pequena amostra normal de tamanho 5. Os restantes quadros na Figura 2.12 exibem as plotagens de qq-norm para amostras normais de tamanhos n = 100 e</p>
<p>n = 1000, respectivamente. Como o tamanho da amostra aumenta, os pontos encontram-se mais perto da linha y = x. Estes gráficos (Figura 2.12) foram gerados utilizando as linhas de comando: set.seed(1278) x=rnorm(5) qqnorm(x, xlim=c(-3,3), ylim=c(-3,3), cex=0.6, pch=19, ylab=’Quantis amostrais’, xlab=’Quantis teóricos’, main=’QQ-plot nomal’) qqline(x,col=“red”) text(-1,2,’n=5’) para a situação de amostra de tamanho 5. A primeira linha de comando serve para fixar o gerador de números laetórios e, dessa forma, podermos simular sempre a mesma amostra e reproduzir o gráfico idêntico. Nas outras situações somente muda-se o tamanho da amostra que se quer gerar.</p>
<p>QQ−plot nomal QQ−plot nomal</p>
<p>−3 −2 −1 0 1 2 3 Quantis teóricos</p>
<p>−3 −2 −1 0 1 2 3 Quantis teóricos</p>
<p>Figura 2.13: Diferentes qqplot para dados não normais. Assim, os comandos para gerar o segundo e terceiro gráficos são: x=rnorm(100) qqnorm(x, xlim=c(-3,3), ylim=c(-3,3), cex=0.6, pch=19, ylab=’Quantis amostrais’, xlab=’Quantis teóricos’, main=’QQ-plot nomal’) qqline(x,col=“red”) text(-1,2,’n=100’)</p>
<p>x=rnorm(1000) qqnorm(x, xlim=c(-3,3), ylim=c(-3,3), cex=0.6, pch=19, ylab=’Quantis amostrais’, xlab=’Quantis teóricos’, main=’QQ-plot nomal’) qqline(x,col=“red”) text(-1,2,’n=1000’) Caso os dados não forem padronizados bastar aplicar a transformação (X − µ)/σ, onde X representa os dados originais e µb e σb representam os estimadores dos parâmetros µ e σ, respectivamente.</p>
<p>Estes gráficos podem indicar afastamentos da normalidade por isso apresentamos duas situações de dados não simétricos e com cuadas pesadas. Na Figura 2.13, mostramos o que acontece se os dados forem da distribuição t-Student(8) e da distribuição χ2(5), sempre de tamanho n = 1000. Observe, em particular, que os dados a partir da distribuição t-Student seguem a curva normal bem de perto até os últimos pontos em cada extremo. Na outra situação o afastamento da distribuição normal é evidente. Foi mencionado que o qq-norm é uma situação particular do qq-plot devido a este último permitir comparar os quantis amostrais com os quantis distribucionais. Com isto queremos dizer que o qq-plot serve para verificar se os dados forem t-Student ou χ2(5), por exemplo. Na Figura 2.14 apresentamos a aparência dos gráficos qq-plot caso queira-se verificar se as amostras seguem distribuição t-Student(8) ou χ2(5), respectivamente.</p>
<p>QQ plot para t−Student(8)</p>
<p>−4 −2 0 2 4 t−Student(8)</p>
<p>QQ plot para c2(5)</p>
<p>0 5 10 15 20 c2(5)</p>
<p>Figura 2.14: Diferentes qqplot para dados não normais. Os gráficos na Figura 2.14 foram gerados pelas linhas de comandos qqplot(qt(ppoints(1000), df = 8), x, cex=0.6, pch=19, main = “QQ plot para t-Student(8)”, xlab=“t-Student(8)”) qqline(x, distribution = function(p) qt(p, df = 8), prob = c(0.1, 0.6), col = 2) no caso t-Student(8) e qqplot(qchisq(ppoints(1000), df = 5), x, cex=0.6, pch=19, main = expression(“QQ plot para” ~~ {chi^2}(5)), xlab=expression({chi^2}(5))) qqline(x, distribution = function(p) qchisq(p, df = 5), prob = c(0.1, 0.6), col = 2) para o caso χ2(5).</p>
<p>Worn plot O worm-plot é uma série de parcelas de gráficos qq-plot retificados. Constitui uma ferramenta de diagnóstico para visualização de quão bem um modelo estatístico se ajusta aos dados, para encontrar locais em que o ajuste pode ser melhorado e para comparar o ajuste de diferentes modelos. Na Figura 2.15 mostramos este gráfico para duas situações: a esquerda os dados são normais e a direita os dados são t-Student com 8 graus de liberdade. Nesta situação aparece bem a qualidade da observação com esta</p>
<p>−4 −2 0 2 4 Unit normal quantile</p>
<p>−4 −2 0 2 4 Unit normal quantile</p>
<p>Figura 2.15: Diferentes worm-plot para dados normais.</p>
<p>figura. Se os dados forem normais o curva worm-plot ou gráfico de minhoca deve aparentar um verme achatado, os pontos próximos a curva vermelha e com poucas oscilações. Quando aplicamos este gráfico ao caso t-Student percebemos uma oscilação grande no verme e com pontos fugindo da banda de confiança. Isso comprova que os dados não seguem como referência a distribuição normal.</p>
<p>−4 −2 0 2 4 Unit normal quantile</p>
<p>−4 −2 0 2 4 Unit normal quantile</p>
<p>Figura 2.16: Diferentes worm-plot para dados não normais. As linhas a seguir mostram os comandos necessários para gerar os gráficos na Figura 2.15. Utilizamos a libraria de comandos R gamlss (Rigby &amp; Stasinopoulos, 2005).</p>
<p>library(gamlss) x=rnorm(1000) wp(gamlss(x~1), cex=0.6) x=rt(1000, df=8) wp(gamlss(x~1), cex=0.6)</p>
<p>Na Figura 2.16, a esquerda temos o caso de dados com distribuição χ2(5) e a direita dados com distribuição Cauchy padrão. Nestas situações fica claro que os dados não são normais. Oa gráficos na figura foram gerados pelas linhas de comando a seguir. x=rchisq(1000, df=5) wp(gamlss(x~1), cex=0.6) x=rcauchy(1000) wp(gamlss(x~1), cex=0.6)</p>
</section>
</section>
<section id="exercícios" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="exercícios"><span class="header-section-number">2.6</span> Exercícios</h2>
<p>Exercícios da Seção 2.1 1. Seja X ∼ Bernoulli( 1 ) e considere todas as possíveis amostras aleatórias de tamanho n = 3. Calcule Xn e S2 cada uma das</p>
<p>2 n oito amostras. Encontre a função de probabilidade de Xn e S2. 2. Um dado é lançado. Seja X o valor da face superior que aparece e X1, X2 duas observações independentes de X. Encontre a função de probabilidade de Xn. 3. Seja X1, , Xn uma amostra aleatória de alguma população. Mostre que (n − 1)Sn</p>
<p>max |Xi Xn| &lt; 1≤i≤n</p>
<p>onde Sn é a raiz quadrada positiva da variância amostral S2.</p>
<p>√n ,</p>
<p>Exercícios da Seção 2.2 1. Seja (X_{(1)}, X_{(2)}, , X_{(n)}) o conjunto das estatísticas de ordem de n variáveis aleatórias independentes <span class="math inline">\(X_1, X_2, \cdots , X_n\)</span> com função de densidade comum</p>
<p>f (x) =</p>
<p>βe−xβ, se x 0 · 0, caso contrário</p>
<ol type="a">
<li>Mostre que X(s) e X(r) − X(s) são independentes para quaisquer r &gt; s.</li>
<li>Encontre a função de densidade de X(r+1) − X(r).</li>
<li>Seja Z1 = nX_{(1)}, Z2 = (n − 1)(X_{(2)} − X_{(1)}), Z3 = (n − 2)(X_{(3)} − X_{(2)}), …, Zn = ((X_{(n)} − X(n−1))). Prove que (Z1, Z2, , Zn) e <span class="math inline">\((X_1, X_2, \cdots , X_n)\)</span> são identicamente distribuídas.</li>
</ol>
<ol start="2" type="1">
<li>Provar o Teorema 2.1</li>
<li>Sejam <span class="math inline">\(X_1, X_2, \cdots , X_n\)</span> variáveis aleatórias com distribuição geométrica de parâmetros p1, p2, , pn, respectivamente. Prove que Nn = min<span class="math inline">\((X_1, X_2, \cdots , X_n)\)</span> têm também distribuição geométrica de parâmetro n p = 1 − (1 − pi)· i=1</li>
<li>As X1, , Xn variáveis aleatórias independentes e identicamente distribuídas tem por função de probabilidade BN (1; p) se, e somente se, Nn = min(X1, , Xn) tem distribuição geométrica de parâmetro 1 − (1 − p)n.</li>
<li>Sejam <span class="math inline">\(X_1, X_2, \cdots , X_n\)</span> variáveis aleatórias independentes e igualmente distribuídas com função de densidade comum</li>
</ol>
<p>f (x) =</p>
<p>σ 0, se x ≤ </p>
<p>Mostre que X_{(1)}, X_{(2)} − X_{(1)}, X_{(3)} − X_{(2)}, , X_{(n)} − X(n−1) são independentes. 6. Sejam <span class="math inline">\(X_1, X_2, \cdots , X_n\)</span> variáveis aleatórias independentes e igualmente distribuídas com função de distribuição acumulada comum</p>
<p>F (t) =</p>
<p>tα, se 0 &lt; t &lt; 1  1, se t ≥ 1</p>
<p>para α &gt; 0. Mostre que X_{(i)}/X_{(n)}, i = 1, 2, , n − 1 e X_{(n)} são independentes. 7. Sejam X1 e X2 duas variáveis aleatórias discretas independentes com função de probabilidade comum P (X = x) = (1 − )x−1, x = 1, 2, ; 0 &lt; &lt; 1· Mostre que X_{(1)} e X_{(2)} − X_{(1)} são independentes. 8. Sejam X1, , Xn duas variáveis aleatórias independentes com função de densidade comum <span class="math inline">\(F\)</span> . Encontre a função de densidade de X_{(1)} e de X_{(n)}.</p>
<ol start="9" type="1">
<li>Sejam X_{(1)}, X_{(2)}, , X_{(n)} as estatísticas de ordem de n variáveis aleatórias independentes e igualmente distribuídas <span class="math inline">\(X_1, X_2, \cdots , X_n\)</span> com função de densidade comum f (x) = 1 se 0 &lt; x &lt; 1 · 0, caso contrário Prove que Y1 = X_{(1)}/X_{(2)}, Y2 = X_{(2)}/X_{(3)}, , Yn−1 = X(n−1)/X_{(n)} e Yn = X_{(n)} são independentes. Encontre a função de densidade conjunta de Y1, Y2, , Yn.</li>
<li>Sejam X1.X2, , Xn variáveis aleatórias independentes identicamente distribuídas não negativas contínuas. Prove que se E|X| &lt; ∞, então E|X(r)| &lt; ∞. Definamos Mn = X_{(n)} = max<span class="math inline">\((X_1, X_2, \cdots , X_n)\)</span>. Mostre que ∫ ∞</li>
</ol>
<p>Encontre E(Mn) em cada uma das seguintes situações: a) Xk tem como função de distribuição comum <span class="math inline">\(F\)</span> (x) = 1 − e−xβ, se x ≥ 0. b) Xk tem como função de distribuição comum <span class="math inline">\(F\)</span> (x) = x, se 0 &lt; x &lt; 1.</p>
<ol start="11" type="1">
<li>Provar que, qualquer seja a amostra aleatória X1.X2, , Xn sempre cumpre-se que X_{(1)} ≤ X ≤ X_{(n)}.</li>
<li>Demonstrar o Teorema 2.5.</li>
<li>Demonstrar o Teorema 2.9.</li>
</ol>
<p>Exercícios da Seção 2.3 1. Demonstre o Corolário 2.17. 2. Demonstre o Corolário 2.18.</p>
<ol start="3" type="1">
<li><p>Seja X1, , Xn uma amostra aleatória Poisson(). Encontre Var(S2) e compare-a com Var(X). Observe que E(X) = = E(S2).</p></li>
<li><p>Seja <span class="math inline">\(X_1, X_2, \cdots , X_n\)</span> uma amostra aleatória da função de distribuição <span class="math inline">\(F\)</span> e seja <span class="math inline">\(F\)</span> ∗(x) a função de distribuição amostral. Encontre Cov[F ∗(x), <span class="math inline">\(F\)</span> ∗(y)] para números reais fixos x, y. n n</p></li>
<li><p>Seja <span class="math inline">\(F\)</span> ∗ a função de distribuição empírica de uma amostra aleatória com função de distribuição teórica <span class="math inline">\(F\)</span> . Prove que { ∗ ϵ } 1</p></li>
<li><p>Sejam <span class="math inline">\(X_1, X_2, \cdots , X_n\)</span> n observacões independentes da variável aleatória X. Encontre a distribuição amostral de X, a média amostral, se:</p></li>
</ol>
<ol type="a">
<li>X ∼ P ();</li>
<li>X ∼ Cauchy(1, 0);</li>
<li>X ∼ χ2(m).</li>
</ol>
<ol start="7" type="1">
<li>Seja X1, , Xn uma amostra aleatória Poisson(). Encontre Var(S2) e compare-a com Var(X). Observe que E(X) = = E(S2).</li>
<li>Demonstre o Teorema 2.23. [Dica: para quaisquer reais µ e σ &gt; 0, encontre a função de densidade de (U(r) − µ)/σ e mostre que as variáveis padronizadas de U(r), (U(r) − µ)/σ, são assintoticamente N (0, 1) sob as condições do teorema.]</li>
<li>Provar que o momentos amostral central b1 é sempre zero.</li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../ModuloI/Aula01/index.html" class="pagination-link  aria-label=" &lt;span="" não="" paramétrica&lt;="" span&gt;"="">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Estatística Não Paramétrica</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../ModuloI/Aula03/index.html" class="pagination-link" aria-label="<span class='chapter-number'>3</span>&nbsp; <span class='chapter-title'>Estatística Não Paramétrica 2</span>">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Estatística Não Paramétrica 2</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Estatística 2023-2026</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://raymz1990.github.io/CE313/edit/main/book/ModuloI/Aula02/index.qmd" class="toc-action"><i class="bi bi-git"></i>Edit this page</a></li><li><a href="https://raymz1990.github.io/CE313/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>