{
  "hash": "aa4c9bb9d42fe708c669aec0afd7789a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Estimação de Densidades\"\ndate: 2024-03-13\ncode-fold: true\n---\n\n\nDe certa forma, problemas de estimação não-paramétrica são extensões de problemas de estimação paramétrica, mas a natureza do primeiro é bem diferente do último. Considere, por exemplo, a situação de observações independentes identicamente distribuídas, digamos $X_1,X_2,\\cdots,X_n$.\n\nEm um problema paramétrico, assumimos que a distribuição de $X_i$ é $F(\\cdot;\\theta)$, a qual é totalmente especificada até o vetor de parâmetros $\\theta$; então o problema é essencialmente a estimaçã de $\\theta$. Em um problema não-paramétrico, a distribuição é totalmente desconhecida com,\ntalvez, algumas restriçõs em propriedades gerais e, portanto, é denotada por $F$.\n\nAqui consideramos estimadores de $F$ em termos de função de densidade $f$. A função de densidade tem a vantagem de fornecer uma representação visualmente mais informativa do que a função de distribuição. Por exemplo, o histograma geralmente dá uma ideia aproximada da forma da distribuição. Este último ficou como o único estimador de densidade não paramétrico até 1950. Por essa razão, nossa discussão começará com os histogramas.\n\nEmbora o histograma seja usado extensivamente, não é tão frequente que seja necessária uma definição matemática. Uma maneira de defini-lo é\natravés da função de densidade empírica.\n\n## Função de densidade empírica.\n\n>\n**Definição**.\nSeja $f$ a derivada de $F$, por isso, pode-se escrever como \n$$\nf(x) = \\lim_{h\\to 0} \\dfrac{F(x+h)-F(x-h)}{2h}\\cdot\n$$\nDizemos então que $\\widehat{f}$, defnido por \n$$\n\\widehat{f}_n(x)=\\dfrac{\\widehat{F}_n(x+h)-\\widehat{F}_n(x-h)}{2h},\n$$\né o histograma, sendo que $\\widehat{F}$_n é a função de distribuição empírica.\n\nUm histograma é uma representação gráﬁca da função de probabilidades ou da função de densidade de um conjunto de dados independentes e foi\nintroduzido pela primeira vez por Karl Pearson no artigo Pearson, K. (1895). Contributions to the Mathematical Theory of Evolution. II. Skew\nVariation in Homogeneous Material. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 186: 343-414.\n\nA representação mais comum do histograma é um gráﬁco de barras verticais. A palavra histograma é de origem grega, derivada de duas: histos que pode signiﬁcar testemunha no sentido de aquilo que se vê, como as barras verticais do histograma e da também palavra grega gramma que signiﬁca desenhar, registrar ou escrever.\n\nPara construir um exemplo controlado do gráﬁco de histograma, simulamos uma amostra de tamanho 150 da distribuição normal padrão, com o comando **rnorm** e depois construímosum gráfico colorido, abaixo a esquerda. Posteriormente, acrescentamos a este gráﬁco uma linha com a densidade normal, abaixo a direita.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(8476)\nexemplo = rnorm(150)\npar(mar=c(5,4,2,1), mfrow=c(1,2))\nhist(exemplo, breaks=12, col=\"red\", xlab=\"Dados simulados\", ylab=\"Frequencia\", main=\"Histograma\")\nbox()\ngrid()\nh=hist(exemplo, breaks=10, col=\"red\", xlab=\"Dados simulados\", ylab=\"Frequencia\", \n       main=\"Histograma com a curva normal\")\nxfit=seq(min(exemplo),max(exemplo),length=40)\nyfit=dnorm(xfit,mean=mean(exemplo),sd=sd(exemplo))\nyfit=yfit*diff(h$mids[1:2])*length(exemplo)\nlines(xfit, yfit, col=\"blue\", lwd=2)\nbox()\ngrid()\n```\n\n::: {.cell-output-display}\n![Gráﬁco de histograma para dados simulados.](index_files/figure-html/fig-1-1.png){#fig-1 width=672}\n:::\n:::\n\n\nA ideia é mostrar que o histograma assemelha-se ao gráﬁco da densidade normal, a densidade dos dados.\n\n## Histograma, informalmente\n\n>\n**Definição**.\nMatematicamente o histograma é uma função $m_i$ que conta o número de observações que pertencem a vários intervalos disjuntos, entanto que o gráﬁco do histograma ou simplesmente histograma é uma mera representação desta função. Assim, se $n$ representa o total de observações e $k$ o número de intervalos disjuntos, o histograma satisfaz que \n$$\nn=\\sum_{i=1}^k m_i\\cdot\n$$\n\nO histograma é um gráﬁco composto por retângulos justapostos em que a base de cada um deles corresponde ao intervalo de classe e a sua altura à respectiva frequência. A construção de histogramas tem caráter preliminar em qualquer estudo e é um importante indicador da distribuição de dados. Pode indicar se uma distribuição aproxima-se de uma densidade normal como pode indicar mistura de densidades, quando os dados apresentam várias modas.\n\nOs histogramas podem ser um mau método para determinar a forma de uma distribuição porque são fortemente inﬂuenciados pelo número de intervalos utilizados. Por exemplo, decidimos gerar 50 amostras da densidade $\\chi^2(6)$ e mostramos os gráﬁcos de histogramas correspondentes com 14 e 26 intervalos na Figura 2.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(5678)\nz=rchisq(50, df=6)\npar(mar=c(5,4,2,1),mfrow=c(1,2))\nhist(z, breaks=14, col=\"blue\", main=expression(paste(\"Histograma \",chi^2,\"(6)\")), \n     ylab=\"Frequencia\", xlab=\"14 intervalos\")\nbox()\ngrid()\nhist(z, breaks=26, col=\"blue\", main=expression(paste(\"Histograma \",chi^2,\"(6)\")), \n     ylab=\"Frequencia\", xlab=\"26 intervalos\")\nbox()\ngrid()\n```\n\n::: {.cell-output-display}\n![Histogramas da distribuição chi² com 6 graus de liberdade. Número de intervalos 14 e 26, respectivamente.](index_files/figure-html/fig-2-1.png){#fig-2 width=672}\n:::\n:::\n\n\nVejamos agora uma deﬁnição mais clara do histograma.\n\n## Histograma\n\n>\n**Definição**: \nSejam $I_1,\\cdots,I_k$ intervalos disjuntos do suporte da função de probabilidade ou de densidade da variável aleatória $X$. O histograma é deﬁnido por \n$$\n\\widehat{f}_n(x) = \\dfrac{m_i}{n|I_i|}, \\quad \\forall x\\in I_i, \\;\ni=1,\\cdots,n,\n$$\nonde $|I_i|$ representa o comprimento do intervalo $i$, $m_i$ e $n$ como na Deﬁnição 1.\n\nFoi provado por Robertson (1967) que, dados os intervalos $I_1,I_2,\\cdots,I_k$, o histograma $\\widehat{f}_n$ é um estimador de máxima verossimilhança dentre os estimadores expressados como funções simples e semi-contínuas superiormente, isto se o fecho de cada intervalos\ncontiver duas ou mais observações. Os gráﬁcos apresentados nas ﬁguras acima são histogramas também segundo a proposta de Robertson (1967).\n\nPode-se observar que este estimador tem duas limitações importantes: a dependência do comprimento do intervalo e o fato de o histograma não\nconstituir uma função contı́nua. A primeira destas limitações foi amplamente estudada por Wegman (1975). Ele provou que os pontos extremos\nde cada intervalo $I_k$ devem ser coincidentes com observações e que, se o número mínimo de observações em cada intervalo aumenta, conforme aumenta o tamanho da amostra, o estimador $\\widehat{f}_n$ é consistente.\n\nA segunda limitação importante do histograma, isto é, o fato de ele não constituir uma função contı́nua, incentivou diversos estudos na\nprocura de estimadores contı́nuos da função de densidad.\n\n### Cálculo automático do número de intervalos num histograma\n\nUma questão importante é determinar de maneira automatizada o número de intervalos disjuntos que serão utilizados para a construção do gráﬁco. Uma primeira forma de escolher o número de intervalos foi dada por Sturges (1926) e que constitui a forma padrão no **R**. Conhecida como fórmula de Sturges é dada por \n\n$$\nk= [\\log_2(n) + 1],\n$$\n\nisto signiﬁca que o número de intervalos é a parte inteira do logaritmo base 2 do número de observações mais 1.\n\nOutras expressões comumente utilizadas são a fórmula de Scott (Scott, 1979) $h = 3.5s/\\sqrt{n}$, onde $s$ é o desvio padrão amostral e a fórmula\nde Freedman-Diacconi (Freedman and Diaconis, 1981) $h = 2\\mbox{IQR}(x)/ n$, onde $\\mbox{IRQ}$ é a diferença entre o terceiro e o primeiro quantil.\n\nUma outra forma de definir o histograma é utilizando o estimador empírico da função de distribuição.\n\n## Histograma, definição moderna.\n\n>\n**Definição**: \nNa definição de função de densidade empírica, o parâmetro $h$ é chamado de largura de banda. Podemos\nescrever o histograma $\\widehat{f}_n$, como, \n$$\n\\widehat{f}_n(x)=\\dfrac{1}{2nh}\\sum_{i=1}^n\n\\pmb{1}_{(x-h;x+h)}(X_i)\\cdot\n$$\n\nPodemos excrever a função de densidade como \n$$\nf(x)=\\lim_{h\\to 0} \\dfrac{1}{h}\\big(F(x+h)−F(x−h)\\big),\n$$ \nmas não se pode definir daqui o histograma porque esse limite\né zero ou infinito e, assim, em algum momento é preciso parar, em outras\npalavras, não se pode chegar muito perto de zero.\n\n::: {.callout-note appearance=\"simple\"}\n\n# Teorema 5.1\n\n Seja $f$ a função de\ndensidade associda à função de distribuição $F$. Então, com probabilidade 1,\n$$\n\\widehat{f}_n(x)\\sim \\mbox{Binomial}(n,p),\n$$\ncom $p=F(x+h)-F(x-h)$.\nAssim, o comportamento assintótico do histograma pode ser derivado da\ndistribuição binomial como \n$$\n\\mbox{E}\\big(\\widehat{f}_n(x)\\big)=\\dfrac{F(x+h)-F(x-h)}{2h}\n$$ \ne \n$$\n\\mbox{Var}\\big( \\widehat{f}_n(x)\\big)=\\dfrac{p(1-p)}{2nh^2}\\cdot\n$$\n:::\n\n**Demonstração.** Exercício,\n\nDeste teorema segue que $\\widehat{f}_n(x)$ é um estimador\nconsistente pontual de $f(x)$ quando\n$h\\to 0$ e $nh\\to\\infty$. A seguir, o processo de\nlimite é entendido como $h=h_n$, ou\nseja, entende-se disto que $h$ não\nserá mais uma quantidade fica e sim uma quantida que depende do tamanho\nda amostra.\nA suposição $h=h_n$ dese\nsatisfazer as condições do Teorema 1, de maneira que, $h_n\\to 0$ e $nh_n\\to\\infty$. Estas condições podem ser\ninterpretadas como se fosse necessário $h_n$ ir a zero, mas não muito rápido. Isso\né exatamente o que temos especulado, exceto que agora temos a taxa exata\nde convergência, que pode ser escrita como $h_n=o(n)$.\n\nExemplo 1: Dados simulados.\n\n Utilaremos 50 dados simulados da distribuíão $N(0,1)$, com isso mostraremos o histograma\ndestes 50 dados utilizando duas formas diferentes de encontrarmos uma\nexpressão para $h_n$, a chamada\nlargura de banda.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1340)\nx = rnorm(50)\npar(mfrow = c(1,2), mar=c(4,3,1,1))\nhist(x, main = NULL, freq = FALSE, breaks = \"Sturges\")\nbox(); grid()\nhist(x, main = NULL, freq = FALSE, breaks = \"Scott\")\nbox(); grid()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\n\nNeste exemplo utilizamos duas formas de escolher a largura de banda\n$h_n$ dentre três diferentes\npossibilidades programadas na função **hist**. Por padrão\nescolhe-se **breaks = “Sturges”**, proposto por Sturges\n(1929), o qual sugere que \n\n$$\nh_n=\\dfrac{X_{(n)}-X_{(1)}}{1+3.322 \\ln(n)}\\cdot\n$$\n\nA segunda situação indica que se os dados provêm da distribuição\nNormal temos que $h_n=3.49sn^{−1/3}$\nsendo $s$ o desvio padrão estimado.\nEsta proposta deve-se à Scott (1979).\nEmbora o histograma é um estimador consistente quando $h_n\\to 0$ e $nh_n\\to\\infty$, verifica-se que se pode\nfazer melhor. A melhoria também é motivada por uma preocupação prática:\no histograma não é uma função suave, uma propriedade que se pode esperar\nque qualquer função de densidade real tenha.\n\n\n## Estimador kernel.\n\n>\n**Definição**:\nO estimador kernel da função de densidade é dado por \n$$\n\\widehat{f}_n(x)=\\dfrac{1}{nh_n}\\sum_{i=1}^n\nK\\left(\\dfrac{x-X_i}{h_n}\\right),\n$$\nonde $K(\\cdot)$ é uma função conhecida como kernel.\n\n\nÉ tipicamente assumido que $K$\nseja não-negativa, simétrica em torno de zero e satisfaz $\\int K(u)\\mbox{d}u=1$. Claro que o\nhistograma é um caso especial do estimador do kernel se $K$ for escolhido como a função de\ndensidade $\\mbox{Uniforme}(−1,1)$. O\núltimo não é uma função suave e é por isso que o histograma não é suave;\nmas escolhendo $K$ como uma função\nsuave, tem-se um estimador de $f$ que\nseja suave.\nPor exemplo, escolhendo a função de densidade $N(0,1)$, temos por resultado o conhecido\ncomo kernel Gaussiano e assim também se utilizamos a densidade de\ndensidade $\\mbox{Beta}$ simétrica,\ndada por \n$$\nK(u)=\\dfrac{\\Gamma(\\nu+3/2)}{\\Gamma(1/2)\\Gamma(\\nu+1)}(1-u^2)^\\nu, \\quad\n-1&lt;u&lt;-1\\cdot\n$$ \ne $K(u)=0$ caso\ncontrário.\nOs casos especiais $\\nu=0,1,2,3$\ncorrespondem às funções kernel uniforme, *Epanechnikov*, *biweight* e\n*triweight*, respectivamente.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1,2), mar=c(4,3,1,1))\nkernels = eval(formals(density.default)$kernel)\nplot (density(0, bw = 1), xlab = \"\", main = \"Diferentes kernel em R\"); grid()\nfor(i in 2:length(kernels)) lines(density(0, bw = 1, kernel =  kernels[i]), col = i)\nlegend(-1.0,0.2, legend = kernels, col = seq(kernels), lty = 1, cex = .8, y.intersp = 1)\nh.f = sapply(kernels, function(k) density(kernel = k, give.Rkern = TRUE))\nh.f = (h.f[\"gaussian\"] / h.f)^ .2\nbw = bw.SJ(x) # escolha automática\nplot(density(x, bw = bw), main = \"Larguras de banda equivalentes\"); grid()\nfor(i in 2:length(kernels)) lines(density(x, bw = bw, adjust = h.f[i], kernel = kernels[i]), col = i)\nlegend(55, 0.035, legend = kernels, col = seq(kernels), lty = 1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n## Escolha automática da largura de banda\n\nUm problema prático importante na estimação de densidades via kernel\né como escolher a largura de banda $h_n$ de forma automática. Note que, dadas\ncondições como $h_n\\to 0$ e $nh_n\\to\\infty$, ainda existem muitas\nopções para $h_n$. Então, de certo\nmodo, a ordem de convergência ou divergência não resolve o problema.\nUma solução para esse problema é conhecida como compensação de\nviés-variância. Antes de entrarmos nos detalhes, vamos primeiro\napressentar um resultado em relação ao viés assintótico do estimador\nkernel. Aqui, o viés é definido como \n\n$$\n\\mbox{viés}\\big( \\widehat{f}_n(x)\\big)=\\mbox{E}\\big(\n\\widehat{f}_n(x)\\big)-f(x),\n$$ \npara todo $x\\in\\mathbb{R}$.\n\n::: {.callout-note appearance=\"simple\"}\n\n# Teorema 5.2\n\n Seja $f$ uma função de\ndensidade contínua e limitada. Então, o viés do estimador kernel de\ndesnidade converge a zero quando $h_n\\to 0$, para todo $x$,\n:::\n\n<div>\n**Demonstração.** \n$$\n\\begin{array}{rcl}\n\\mbox{E}\\big(\\widehat{f}_n(x) \\big) &amp; = &amp; \\displaystyle\n\\dfrac{1}{n}\\sum_{i=1}^n \\dfrac{1}{h_n} \\int\nK\\left(\\dfrac{x-u}{h_n}\\right)f(u)\\mbox{d}u \\\\\n&amp; = &amp; \\displaystyle \\int K(u)f(x-h_nu)\\mbox{d}u \\, = \\, f(x) +\n\\int K(u)\\big( f(x-h_nu)-f(x)\\big) \\mbox{d}u\\cdot\n\\end{array}\n$$ \nUtilizando então o Teorema da Convergência Dominada\ncompleta-se a demonstração.\n\n::: {.callout-note appearance=\"simple\"}\n\n# Teorema 5.3\n\nSuponhamos que $f$ seja\ncontínua três vezes diferenciável, com terceira derivada limitada na\nvizinhança de $x$ e $K$, função kernel, satisfazendo que \n$$\n\\int K^2(u)\\mbox{d}u&lt; \\infty \\quad \\mbox{e} \\quad \\int |u^3|\nK(u)\\mbox{d}u &lt; \\infty \\cdot\n$$ \nSe $h_n\\to 0$, quando $n\\to\\infty$, temos que \n$$\n\\mbox{viés}\\big(\n\\widehat{f}_n(x)\\big)=\\dfrac{h_n^2}{2}f&#39;&#39;(x)\\int u^2\nK(u)\\mbox{d}u + o(h_n^2)\\cdot\n$$ \nSe, além disso, $bh_n\\to\\infty$ quando $n\\to\\infty$, temos \n$$\n\\mbox{Var}\\big(\\widehat{f}_n(x)\\big)=\\dfrac{f(x)}{nh_n}\\int\nK^2(u)\\mbox{d}u+o(1/nh_n)\\cdot\n$$\n:::\n\n**Demonstração.** \nA demonstração é baseada na expansão\nde Taylor, \n$$\nf(x-h_n u)=f(x)-h_n u f&#39; (x)+\\dfrac{h_n^2 u^2}{2}\nf&#39;&#39;(x)-\\dfrac{h_n^3 u^3}{6}f&#39;&#39;&#39; (\\epsilon),\n$$ \n\nsendo que $\\epsilon$ fica\nentre $x−h_n u$ e $x$. Os detalhes são deixados como um\nexercício.\n\nUma medida de precisão do estimador é o erro quadrático médio (EQM),\ndado por \n$$\n\\mbox{EQM}\\big(\n\\widehat{f}_n(x)\\big)=\\mbox{E}\\big(\\widehat{f}_n(x)-f(x)\\big)^2\\cdot\n$$ \n\nÉ fácil mostrar que o EQM combina o viés e a variância de tal\nmaneira que\n\n$$\n\\mbox{EQM}\\big(\n\\widehat{f}_n(x)\\big)=\\mbox{viés}\\big(\\widehat{f}_n(x)\\big)^2+\\mbox{Var}\\big(\\widehat{f}_n(x)\\big)\\cdot\n$$\nVemos que, sob as condições $h_n\\to\n0$ e $nh_n\\to\\infty$ e, se\nignorarmos os termos de baixa ordem, temos \n\n$$\n\\mbox{EQM}\\big( \\widehat{f}_n(x)\\big) \\approx \\dfrac{h_n^4}{4}\\big(\nf&#39;&#39;(x)\\big)^2\\tau^4+\\dfrac{f(x)}{nh_n}\\gamma^2,\n$$ \n\nonde $\\tau^2=\\int u^2\nK(u)\\mbox{d}u$ e $\\gamma^2=∫\nK^2(u)\\mbox{d}u$.\nO termo à direita da expressão acima é minimizada quando\n\n$$\nh_n=\\left( \\dfrac{\\gamma^2 f(x)}{\\tau^4 \\big(\nf&#39;&#39;(x)\\big)^2}\\right)^{1/5}n^{-1/5}\\cdot\n$$\n\nNote ainda que a expressão acima não é a solução ideal, isso\nporque $f$ é desconhecida na prática.\nNo entanto, dá-nos pelo menos alguma ideia sobre a taxa ideal de\nconvergência a zero, sendo esta $h_n=O(n^{-1/5})$.\n\nQuando $f$ é desconhecida, uma\nabordagem natural seria substituí-lo por um estimador e, assim, obter\numa largura de banda ideal estimada. Uma complicação é que a largura de\nbanda ideal depende de $x$ mas,\nidealmente, gostaríamos de usar uma largura de banda que funcionasse\npara diferentes $x$ dentro de um\ncerto intervalo, se não todos os $x$.\n\nPara obter uma largura de banda ideal que não depende de $x$, integramos os dois lados da expressão\ndo EQM em relação a $x$. Isto nos\nleva a \n\n$$\n\\int \\mbox{EQM}\\big(\\widehat{f}_n(x)\\big)\\approx \\dfrac{\\tau^4\nh_n^4}{4}\\int\n\\big(f&#39;&#39;(x)\\big)^2\\mbox{d}x+\\dfrac{\\gamma^2}{nh_n}\\int\nf(x)\\mbox{d}x = \\dfrac{\\tau^4 \\theta^2 h_n^4}{4}+\\dfrac{\\gamma^2}{nh_n},\n$$ \n\ncom $\\theta^2=\\displaystyle \\int \\big( f&#39;&#39;(x)\\big)^2 \\mbox{d}x$.\nPelo mesmo argumento, o lado direito acima é minimizado quando \n\n$$\nh_n=\\left( \\dfrac{\\gamma^2}{\\tau^4\\theta^2}\\right)^{1/5}n^{-1/5}\\cdot\n$$\nDesta vez, o $h_n$ ideal não\ndepende de $x$. Além disso, a\nintegral do EQM ou o IEQM mínimo é dado por \n\n$$\n\\mbox{IEQM}=\\int\n\\mbox{EQM}\\big(\\widehat{f}_n(x)\\big)\\mbox{d}x=\\dfrac{5}{4}\\big(\\tau\\gamma^2\\big)^{4/5}\\theta^{2/5}n^{-4/5}\\cdot\n$$\n\nUma implicação do resultado acima é a seguinte. Note que o IEQM\ndepende do kernel $K$ através de $c_K=\\big(\\tau\\gamma^2\\big)^{4/5}$.\nMostrou-se que para os kernels comumente usados, tais como aqueles\nlistados, o desempenho dos estimadores de kernel correspondentes é quase\no mesmo em termos dos valores de $c_K$.\n\nVoltando ao problema sobre a estimação da largura de banda ideal,\nvemos que tudo o que precisamos é encontrar um estimador consistente de\n$\\theta^2$. Se $f$ é a função de densidade da distribuição\nnormal com desvio padrão $\\sigma$,\nentão pode ser mostrado que $\\theta^2=3/8\\sqrt{\\pi}\\sigma^5$.\nNaturalmente, se alguém souber que $f$ é normal, então a estimação da\ndensidade não-paramétrica não seria necessária, porque um método\nparamétrico provavelmente seria melhor. Em geral, pode-se expandir $f$ em torno da densidade gaussiana usando\na expansão de Edgeworth.\n\nUtilizando a abordagem acima, Hjort and Jones (1996) obteveram o\nseguinte estimador ótimo para a largura de banda $$\n\\widehat{h}_n=\\widehat{h}_0\\left(\n1+\\dfrac{35}{48}\\widehat{\\gamma}_4+\\dfrac{35}{32}\\widehat{\\gamma}^2_3+\\dfrac{385}{1024}\\widehat{\\gamma}^2_4\\right)^{-1/2},\n$$ onde $\\widehat{h}_0$ é a\nestimativa ideal da largura de banda assumindo que $f$ é normal, isto é, com $\\theta^2$ substituído por $3/8\\sqrt{\\pi}\\sigma^5$ ou mais\nexplicitamente \n\n$$\n\\widehat{h}_0=1.06\\left( \\dfrac{\\widehat{\\sigma}}{n^{1/5}}\\right)\\cdot\n$$\n\nChamamos $\\widehat{h}_0$, a\nlargura de banda da linha de base e $\\widehat{\\sigma}^2$, a variância amostral,\ndada por\n\n$$\n\\widehat{\\sigma}^2 = \\dfrac{1}{n-1}\\sum_{i=1}^n\n(x_i-\\overline{x})^2\\cdot\n$$\nAlém disso, 4_3$ e $\\widehat{\\gamma}_4$ são os estimadores dos\ncoefcientes de assimetria e curtose amostrais, respectivamente, dados\npor\n$$\n\\widehat{\\gamma}_3=\\dfrac{1}{(n-1)\\widehat{\\sigma}^3}\\sum_{i=1}^n\n(x_i-\\overline{x})^3\n$$ \ne \n\n$$\n\\widehat{\\gamma}_4=\\dfrac{1}{(n-1)\\widehat{\\sigma}^4}\\sum_{i=1}^n\n(x_i-\\overline{x})^4-3\\cdot\n$$\nOs procedimentos de estimação de densidade mostrados até o momento\nforam implementados na função **density**.\n\n##\nExemplo 2: Estimador kernel de densidade em dados simulados.\n\n Utilizaremos os dados simulados da distribuíão $N(0,1)$ no exemplo anterior. Mostraremos o\nhistograma e o estimador kernel da função de densidade.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1,2), mar=c(4,3,1,1))\nhist(x, main = NULL, freq = FALSE, breaks = \"Sturges\")\nbox(); grid()\nlines(density(x, bw = \"nrd0\"), col = \"red\")\npar(mar=c(4,2,1,1))\nhist(x, main = NULL, freq = FALSE, breaks = \"Scott\")\nbox(); grid()\nlines(density(x, bw = \"bcv\"), col = \"red\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in bw.bcv(x): mínimo ocorreu em uma das extremidades do intervalo\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\n\nHouveram outras abordagens para a seleção da largura de banda ótima,\nincluindo o método de validação cruzada.\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}