{
  "hash": "1bcf7c819c7ca8804ca8c96a6dd81bb6",
  "result": {
    "engine": "knitr",
    "markdown": "---\nTitle: Módulo I - Estimação não paramétrica\ncode-fold: true\n---\n\n\n# Módulo I {#sec-modulo01}\n\n:::{.callout-warning}\nÚltima atualização: 03 de agosto de 2018.\n:::\n\nSejam $x_{1},  x_{2}, \\cdots  x_{n}$, variáveis aleatórias independentes identicamente distribuídas com distribuição comum $\\mathcal{L} (X)$ e seja $P$  a classe de todas as possíveis distribuições de $X$ que consiste de todas as distribuições absolutamente contínuas ou discretas.\n \n>\n**Definição**. A estatística $T(X)$ é suficiente para a família de distribuições $\\mathcal{p}$ se a distribuição condicional de $X|T=t$ é a mesma, seja qual for a função de distribuição $F \\in \\mathcal{p}$.\n \n**Exemplo**.  Sejam $x_{1},  x_{2}, \\cdots  x_{n}$ variáveis aleatórias independentes igualmente distribuídas com distribuição absolutamente contínua e seja $T = (X_{(1)}, \\cdots, X_{(n)})$ a estatística de ordem. Então:\n$$f(x|T = t) = 1n!,$$\ne vemos que $T$ é uma estatística suficiente para a família das distribuições absolutamente contínuas.\n\n>\n**Definição**. A família de distribuições $\\mathcal{P}$ é completa se somente a função zero for o estimador não viesado de 0, isto é,\n\n$$E_F \\Big( h(X) \\Big) = 0,$$\npara todo $F\\in\\mathcal{P}$ implica que $h(X)=0$. Isto para todo $x$ exceto para um conjunto nulo em relação \na cada $F\\in\\mathcal{P}$.\n\n**Exemplo**.  Seja $X\\sim Uniforme(0,\\theta)$, onde $\\theta\\in (0,\\infty)$. Mostraremos que esta família de distribuições é completa. Precisamos mostrar que\n\n$$E_\\theta \\Big( g(X) \\Big) = \\int_0^\\theta g(x)\\frac{1}{\\theta}\\mbox{d}x \\, = \\, 0, \\qquad \\forall \\,\\theta>0,$$\nse, e somente se, $g(x)=0$, para todo $x$. Em geral, esse resultado segue a teoria da integração. Se $g$ for \ncontínua, diferenciamos ambos os lados de\n\n$$\\int_0^\\theta g(x)\\mbox{d}x = 0,$$\n\npara obter $g(\\theta)=0$, para todo $\\theta>0$. Agora, seja $X_1,X_2,\\cdots,X_n$ uma amostra aleatória da \ndistribuição $Uniforme(0,\\theta)$. Também, seja $X_{(n)}=\\max(X_1,X_2,\\cdots,X_n)$. Então, a função de densidade de $X_{(n)}$ é dada por\n\n$$f_{X_{(n)}}(x|\\theta)=n\\theta^{-n}x^{n-1}, \\qquad 0 < x < \\theta,$$\n\nzero caso contrário. Vemos por um argumento semelhante que $X_{(n)}$ é completa, o que é o mesmo do que dizer \nque $ \\{f_{X_{(n)}}(x|\\theta); \\, \\theta>0 \\}$ é uma família de densidades completa. Claramente $X_{(n)}$ é suficiente.\n\n>\n**Definição**. Uma estatística $T(X)$ é dita ser completa em relação a uma classe de distribuições $\\mathcal{P}$ se a classe de distribuições induzidas de $T$ for completa.\n\nClaro que todos os exemplos já encontrados de estatísticas completas ou famílias completas de distribuições para o caso paramétrico podem ser aplicados nesta situação. \n        \n:::{.callout-note appearance=\"simple\"}\n## Teorema \n\nA estatística de ordem $(X_{(1)},\\cdots,X_{(n)})$ é uma estatística suficiente e completa desde que a amostra $X_1,X_2,\\cdots,X_n$ seja composta de variáveis aleatórias independentes identicamente distribuídas do tipo discreta ou contínua.\n:::\n\n>\n**Definição**. Diz-se que uma função real $g(F)$ é estimável se tiver um estimador não viciado, isto é, se \nexiste uma estatística $T(X)$ tal que\n$$\\mbox{E}_F\\Big( T(X)\\Big)=g(F),$$\npara todo $F\\in\\mathcal{P}$.\n\n**Exemplo**. Se $\\mathcal{P}$ é a classe de todas as distribuições para as quais o segundo momento existe, $X$ é um\nestimador não viciado de $\\mu(F)$, a média da população. Similarmente\n$$\\mu_2(F)=\\mbox{Var}_F(X),$$\né também estimável e um estimador não viciado é\n\n$$S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\overline{X})^2\\cdot$$\n\nDa mesma forma, $\\overline{X}-\\overline{Y}$ é um estimador não viciado de $\\mbox{E}(X)-\\mbox{E}(Y)$,\n\n$$\\frac{1}{n}\\{\\mbox{número de } X>c\\}$$\n\né um estimador não viciado de $P_F(X>c)$ e assim por diante.\n\n>\n**Definição**. \nO grau $m$, $m\\geq 1$, de um parâmetro estimável $g(F)$ é o menor tamanho de amostra para o qual o parâmetro é estimável, ou seja, é o menor $n$ para o qual existe um estimador não viciado $T(X_1,\\cdots,X_n)$ com\n$$\\mbox{E}_F\\Big( T(X)\\Big)=g(F),$$\npara todo $F\\in\\mathcal{P}$.\n\n**Exemplo**. O parâmetro \n$$g(F)=P_F(X>c),$$\nonde $c$ é uma constante conhecida, têm grau 1. Também $\\mu(F)$ é estimável com grau 1, para isto assumimos que existe ao menos um $F\\in\\mathcal{P}$ tal que $\\mu(F)\\neq 0$. Acontece que $\\mu_2(F)$ é estimável com grau 2, desde que $\\mu_2(F)$ não seja estimável de forma não viciada por somente uma observação. Ao menos duas observações são necessárias. De maneira similar, $\\mu^2(F)$ têm grau 2.\n\n>\n**Definição**. Um estimador não viciado de algum parâmetro baseado no tamanho mínimo de amostra, ou seja, com amostra iagual ao grau $m$ é chamado de kernel.\n\n**Exemplo**. Seja $X_1,\\cdots,X_n$ uma amostra aleatória com distribuição $F$. Então $X_i$ é o kernel de \n$\\mu(F)$; $X_iX_j$, $i\\neq j$, é o kernel de $\\mu^2(F)$ e cada  \n$$T(X_i,X_j)=X_i^2-X_iX_j, \\qquad i=1,\\cdots,n, \\quad i\\neq j,$$\né kernel de $\\mu_2(F)$.\n\n::: {.callout-note appearance=\"simple\"}\n## Teorema \n\nExiste um kernel simétrico para cada parâmetro estimável.\n:::\n\n\n**Demonstração**. Seja $T(X_1,\\cdots,X_m)$ um kernel para $g(F)$. Também é\n$$T_s(X_1,\\cdots,X_m)=\\frac{1}{m!}\\sum_P T(X_{i_1},X_{i_2}\\cdots,X_{i_m})$$\num kernel para $g(F)$, onde a soma $P$ acontece sobre todas as $m!$ permutacoes de $\\{1,2,\\cdots,m\\}$.\n\n\n\n\n**Exemplo**. Seja $X_1,\\cdots,X_n$ uma amostra aleatória com distribuição $F$. Um kernel simétrico para $\\mu_2(F)$ \né\n$$T_s(X_i,X_j)=\\frac{1}{2}\\Big( T(X_i,X_j)+T(X_j,X_i) \\Big) \\, = \\, \\frac{1}{2}\\big( X_i-X_j\\big)^2,  \n\t\\qquad i=1,\\cdots,n, \\quad i\\neq j\\cdot$$\n\n>\n**Definição**. \nSeja $g(F)$ um parâmetro estimável de grau $m$ e $X_1,X_2,\\cdots,X_n$ uma amostra aleatória de $F$ de tamanho \n$n$, $n\\geq m$. Correspondendo a qualquer kernel $T(X_1,\\cdots,X_n)$ de $g(F)$, definimos a U-estatística para a amostra como\n$$U(X_1,X_2,\\cdots,X_n)=\\frac{1}{{n \\choose m}}\\sum_C T_s(X_{i_1},X_{i_2},\\cdots,X_{i_n}),$$\nonde o índice da soma $C$ percorre todas as ${n \\choose m}$ permutações de $m$ inteiros $(i_1,i_2,\\cdots,i_m)$ \nescolhidos de $\\{1,2,\\cdots,n\\}$ e $T_s$ é um kernel simétrico, como definido na demonstração do Teorema \nanterior.\n\nClaramente, a U-estatística definida é simétrica nos $X$ e\n$$\\mbox{E}_F\\big(U(X)\\big)=g(F),$$\npara todo $F$.\n\n**Exemplo**. Seja $X_1,\\cdots,X_n$ uma amostra aleatória com distribuição $F$. Para estimarmos $\\mu(F)$ a U-estatística é dada por  \n$$U(X_1,X_2,\\cdots,X_n)=\\frac{1}{n}\\sum_{i=1}^n X_i\\cdot$$\nPara estimarmos $\\mu_2(F)$, um kernel simétrico é\n$$T_s(X_{i_1},X_{i_2}) = \\frac{1}{2}\\big( X_{i_1}-X_{i_2}\\big)^2,$$\npara $i_1=1,2,\\cdots,n$, $i_1\\neq i_2$. A correspondente U-estatística é\n$$\n\tU(X) \\, = \\, \\frac{1}{{n \\choose 2}}\\sum_{i_1< i_2}\\frac{1}{2}\\big(X_{i_1}-X_{i_2}\\big)^2 \n\t\\, = \\, \\frac{1}{n-1}\\sum_{i=1}^n \\big( X_i-\\overline{X}\\big)^2 \\, = \\, S^2\\cdot\n$$\n\nDe maneira similar, para estimarmos $\\mu_2(F)$, o kernel simétrico é $T_s(X_{i_1},X_{i_2})=X_{i_1}X_{i_2}$ e a \ncorrespondente U-estatística é\n$$\n\tU(X) \\, = \\, \\frac{1}{{n \\choose 2}}\\sum_{i< j}X_iX_j \\, = \\, \\frac{1}{n(n-1)}\\sum_{i\\neq j} X_iX_j\\cdot\n$$\nPara estimarmos $\\mu^3(F)$ um kernel simétrico seria $T_s(X_{i_1},X_{i_2},X_{i_3})=X_{i_1}X_{i_2}X_{i_3}$, sendo que \na U-estatística é\n$$\n\tU(X) \\, = \\, \\frac{1}{{n \\choose 3}}\\sum_{i< j< k}X_iX_jX_k \\, = \\, \\frac{1}{n(n-1)(n-2)}\\sum_{i\\neq j\\neq k} X_iX_jX_k\\cdot\n$$\n\nO seguinte resultado mostra a importância da U-estatística.\n\n::: {.callout-note appearance=\"simple\"}\n## Teorema \n\nSeja $\\mathcal{P}$ a classe de todas as distribuições absolutamente contínuas ou discretas. Qualquer função estimável $g(F)$, $F\\in\\mathcal{P}$, tem um estimador único que é não viciado, simétrico nas observações e uniformemente de variância mínima entre todos os estimadores não viciados.\n:::\n\n**Demonstração**. Seja $X_1,\\cdots,X_n$ uma amostra aleatória de $F$, $F\\in\\mathcal{P}$ e seja $T(X_1,\\cdots,X_n)$ um estimador não viciado de $g(F)$. Considere o conjunto de todas as $n!$ permutações de $\\{1,2,\\cdots,n\\}$ e indexá-los adequadamente. Seja $\\{i_1,i_2,\\cdots,i_n\\}$ o i-ésimo deste conjunto e seja \n$$\n\tT_i \\, = \\, T_i(X_1,X_2,\\cdots,X_n) \\, = \\, T(X_{i_1},X_{i_2}\\cdots,X_{i_n}), \\qquad i=1,2,\\cdots,n!\\cdot\n$$\nSeja\n$$\n\t\\overline{T}=\\frac{1}{n!}\\sum_{i=1}^{n!} T_i\\cdot\n$$\nClaro que $\\mbox{E}(T)=g(F)$ e\n$$\n\\begin{array}{rcl}\n\t\\mbox{Var}(\\overline{T}) & = & \\mbox{E}\\Big(\\frac{1}{n!}\\sum_{i=1}^{n!} T_i \\Big)^2 \\, - \\, \\big(g(F)\\big)^2 \\\\\n\t& \\leq & \\mbox{E}\\Big(\\frac{1}{(n!)^2}\\sum_{i=1}^{n!} T_i^2\\Big) \\, - \\, \\big(g(F)\\big)^2 \\, = \\, \\mbox{E}(T^2) \n\t\\, - \\, \\big(g(F)\\big)^2 \\, = \\, \\mbox{Var}(T)\\cdot\n\\end{array}\n$$\nA igualdade se mantém se, e somente se,\n$$\n\tT_i(X_1,X_2,\\cdots,X_n) \\, = \\, \\frac{\\alpha}{n!}, \\qquad i=1,2,\\cdots,n!,\n$$\npara todos os pontos no espaço amostral, exceto talvez para um conjunto nulo, em que $\\alpha$ é uma constante. Segue-se que $T(X)$ é simétrico nos argumentos $X_1,X_2,\\cdots,X_n$ com probabilidade 1 e $\\overline{T}$ é idêntico \na $T$. A exclusividade é deixada como exercício.\n\n::: {.callout-note appearance=\"simple\"}\n## Teorema \n\nSeja $T(X_1,\\cdots,X_n)$ um estimador não viciado para $g(F)$, $F\\in\\mathcal{P}$. A correspondente U-estatística \né essencialmente o único estimador não viciado uniformemente de mínima variância.\n:::\n\n**Demonstração** Consequência do teorema anterior.\n\nDe acordo com os teoremas acima precisamos apenas considerar estimadores que sejam simétricos nas observações e tudo \no que devemos fazer é torná-las não viciados. Este procedimento leva a um estimador não viciado com a menor \nvariância na classe de todos os estimadores não viciados do parâmetro. Por exemplo, como consequência destes \nteoremas, $\\overline{X}$ e $S^2$ são os únicos estimadores não viciados uniformemente de variância \nmínima de $\\mu(F)$ e $\\mu_2(F)$, respectivamente.\n\n**Exemplo**. Seja $\\mathcal{P}$ a classe de todas as distribuições absolutamente contínuas e $X_1,X_2,\\cdots,X_n$ uma amostra aleatória de tamanho $n$. Para estimarmos \n$$\n\tg(F) \\, = \\, P_F(X_1>c),\n$$\nonde $c$ é uma constante fixa, definimos\n$$\n\tY_i \\, = \\, \\left\\{ \\begin{array}{ll} 1, & X_i> c, \\\\ 0, & X_i\\leq c \\end{array}\\right. \\quad i=1,2,\\cdots,n\\cdot\n$$\nConsidere agora\n$$\n\tT(Y_1,Y_2,\\cdots,Y_n) = \\sum_{i=1}^n \\alpha_i Y_i,\n$$\ncomo um estimador de $g(F)$. Para encontrar o estimador não viciado de mínima variância de $g$ simetrizamos \n$T$ nos $Y_1,Y_2,\\cdots,Y_n$. Isso acontece se $\\alpha_i=\\alpha$, $i=1,2,\\cdots,n$ e $T(Y)=\\alpha\\sum_{i=1}^n Y_i$. Para $T$ ser não viciado, temos que\n$$\n\t\\mbox{E}_F(T) \\, = \\, \\alpha\\sum_{i=1}^n \\mbox{E}_F(Y_i) \\, = \\, \\alpha n g(F),\n$$\nde maneira que $\\alpha=\\frac{1}{n}$. Portanto, $\\frac{1}{n}\\sum_{i=1}^n Y_i$ é o estimador não viciado de \nmínima variância; também\n$$\n\t\\mbox{Var}_F(T) \\, = \\, \\frac{g(F)\\big(1-g(F)\\big)}{n} \\, \\leq \\, \\frac{1}{4n}\\cdot\n$$\nAlém disso, $Y_i$ têm distribuição $Bernoulli$, de modo a \n$$\n\t\\frac{\\frac{1}{n}\\big(T-g(F)\\big)}{\\big(g(F)(1-g(F))\\big)^\\frac{1}{2}} \\overset{D}{\\longrightarrow} Z, \\qquad n\\to\\infty,\n$$\nonde $Z\\sim N(0,1)$. Este resultado pode ser usado para encontrar limites de confiança em $g(F)$.\n\nSeja $\\mathcal{P}$ a classe de todas as distribuições absolutamente contínuas na reta \nreal. Sejam $F,G\\in\\mathcal{P}$ e definamos a função distância $\\Delta(F,G)$ como segue:\n$$\n\t\\Delta(F,G) \\, = \\, \\int_{-\\infty}^\\infty \\big( F(x)-G(x)\\big)^2 \\dfrac{F'(x)+G'(x)}{2}\\mbox{d}x \\cdot\n$$\nEsta função satisfaz as seguintes propriedades:\n<ul>\n\t<li>$\\Delta(F,G) = 0$ se, e somente se, $F=G$.</li>\n\t<li>$\\Delta(F,G) = \\Delta(G,F)$.</li>\n\t<li>$\\Delta(F,G) > 0$.\n\tPor outro lado, vamos supor que $F(x)\\neq G(x)$ para algum $x_1$, onde $F(x_1)-G(x_1)=d>0$. Dado que $F$ e $G$ \n\tsão distribuições absolutamente contínuas, existe um $ x_0 < x_1 $, tal que \n\t$$F(x_0)-G(x_0) \\, = \\, \\frac{d}{2} \\qquad \\mbox{e} \\qquad F(x)-G(x)\\geq \\frac{d}{2},$$\n\tpara $x_0\\leq x \\leq x_1$. Dado que tanto $F$ quanto $G$ são ambas não decrescentes, pelo menos um dos $F$ e $G$ deve aumentar pelo menos $d/2$ quando $x$ varia de $x_0$ a $x_1$. Então \n\t$$\\Delta(F,G) \\, \\geq \\, \\int_{x_0}^{x_1} \\big( F(x)-G(x)\\big)^2 \\dfrac{F'(x)+G'(x)}{2}\\mbox{d}x \\, \\geq \\, \n\t\t\\Big( \\frac{d}{2}\\Big)^2 \\frac{d/2}{2} \\, > \\, 0\\cdot$$\n\t</li>\n</ul>\n\n**Exemplo**. Encontremos um estimador não viciado de mínima variância para $\\Delta(F,G)$. Sejam $X_1,X_2,\\cdots,X_m$ uma amostra aleatória de $F$ e $Y_1,Y_2,\\cdots,Y_n$ uma amostra aleatória de $G$, independentes. Consideramos que $F,G\\in\\mathcal{P}$. Primeiro mostramos que\n$$\n\tg(F,G) \\, = \\, P\\Big( \\big\\{ \\max(X_1,X_2) < \\min(Y_1,Y_2)\\big\\} \\bigcup \\big\\{\\max(Y_1,Y_2) < \\min(X_1,X_2)\\big\\}\\Big) \\, = \\,\n\t\\frac{1}{3} \\, + \\, 2\\Delta(F,G)\\cdot\n$$\nTemos que\n$$\n\tg(F,G) \\, = \\, P\\Big( \\big\\{\\max(X_1,X_2) < \\min(Y_1,Y_2)\\big\\}\\Big) \\, + \\, P\\Big( \\big\\{\\max(Y_1,Y_2) < \\min(X_1,X_2)\\big\\}\\Big) \n$$\ne\n$$\n\tP\\big( \\max(X_1,X_2) \\leq x \\big) \\, = \\, F^2(x), \\qquad P\\big( \\min(Y_1,Y_2) \\geq y \\big) \\, = \\, [1-G(y)]^2\\cdot\n$$\nEntão\n$$\n\\begin{array}{rcl}\n\tg(F,G) & = & \\displaystyle \\int_{-\\infty}^\\infty [1-G(y)]^2 2F(y)F'(y)\\mbox{d}y \\, + \\, \n\t\\int_{-\\infty}^\\infty [1-F(x)]^2 2G(x)G'(x)\\mbox{d}x \\\\\n\t& = & \\displaystyle \\int_{-\\infty}^\\infty [1+G^2(y)-2G(y)] 2F(y)F'(y)\\mbox{d}y \\, + \\, \n\t\\int_{-\\infty}^\\infty [1+F^2(x)-2F(x)] 2G(x)G'(x)\\mbox{d}x \\\\\n\t& = & 2+ \\displaystyle \\int_{-\\infty}^\\infty 2\\Big[G^2(x)F(x)F'(x)+F^2(x)G(x)G'(x)-2F(x)G(x)\\big(F'(x)+G'(x)\\big)\\Big]\\mbox{d}x \\\\\n\t& = & 3-2\\displaystyle \\int_{-\\infty}^\\infty \\Big(\\big(F(x)+G(x)\\big)^2-\\big(F(x)-G(x)\\big)^2 \\Big)\n\t\\Big(\\frac{F'(x)+G'(x)}{2}\\Big)\\mbox{d}x \\\\\n\t& = & 3-8\\displaystyle\\int_{-\\infty}^\\infty \\Big(\\frac{F(x)+G(x)}{2}\\Big)^2\\Big(\\frac{F'(x)+G'(x)}{2}\\Big)\\mbox{d}x \\, + \\, \n\t2\\Delta(F,G) \\\\\n\t& = & 3-\\frac{8}{3} \\, + \\, 2\\Delta(F,G) \\, = \\, g(F,G)\\cdot\n\\end{array}\n$$\nPara utilizarmos os teoremas acima, vamos definir\n$$\n\t\\varphi(X_1,X_2,Y_1,Y_2) \\, = \\, \\left\\{ \\begin{array}{ll} 1, & \\mbox{se } \\, \\max(X_1,X_2) < \\min(Y_1,Y_2) \\, \\mbox{ ou se } \\, \n\t\\max(Y_1,Y_2) < \\min(X_1,X_2) \\\\ 0, & \\mbox{caso contrário} \\end{array}\\right.\n$$\nEntão $\\varphi(X_1,X_2,Y_1,Y_2)$ é um estimador não viciado de $g(F,G)$ e de fato é um kernel de $g(F,G)$. A U-estatística correspondente, portanto, deve ser o estimador não viciado de mínima variância. Nós temos\n$$\n\tU(X,Y) \\, = \\, \\frac{1}{{m \\choose 2}{n \\choose 2}} \\sum_{i_1< i_2}\\sum_{k_1< k_2} \\varphi(X_{i_1},X_{i_2},Y_{k_1},Y_{k_2}),\n$$\nde maneira que $U$ é o estimador não viciado de mínima variância de $g(F,G)$, assim como o estimador não viciado de mínima variância de $\\Delta(F,G)$ é\n$$\n\t\\widehat{\\Delta}(F,G) \\, = \\, \\frac{1}{2}U(X,Y) \\, - \\, \\frac{1}{6}\\cdot\n$$\n\n**Exemplo**. Seja $\\mathcal{P}$ a classe de todas as funções de distribuição  absolutamente contínuas na reta real e $X_1,X_2,\\cdots,X_m$ e $Y_1,Y_2,\\cdots,Y_n$ duas amostras aleatórias independentes de $F$ e $G$, respectivamente, com $F,G\\in\\mathcal{P}$. Queremos estimar\n$$\n\t\\rho(F,G) \\, = \\, P(X < Y)\\cdot\n$$\nCom esse objetivo, vamos definir\n$$\n\tZ_{ij}=\\left\\{\\begin{array}{ll} 1, & X_i< Y_j \\\\ 0, & X_i\\geq Y_j \\end{array}\\right.\n$$\npara cada par $X_i, Y_j$, $i=1,2,\\cdots,m$ e $j=1,2,\\cdots,n$. Então $\\displaystyle \\sum_{i=1}^m Z_{ij}$ é o número de vezes que $X< Y_j$ e $\\displaystyle \\sum_{j=1}^n Z_{ij}$ é o número de vezes que $X_i< Y$. Mann and Whitney (1947) sugeriram utilizar o estimador $U/mn$, onde\n$$\n\tU \\, = \\, \\sum_{i=1}^m \\sum_{j=1}^n Z_{ij}\n$$\ne\n$$\n\t\\mbox{E}(U) \\, = \\, mn \\mbox{E}(Z_{ij}) \\, = \\, mn P(X < Y)\\cdot\n$$\nEntão\n$$\n\t\\widehat{\\rho}(F,G) \\, = \\, \\frac{U}{mn},\n$$\né não viciado para $\\rho$. Além disso, $\\widehat{\\rho}$ é simétrico em $X$ e $Y$, de modo que tem uma variância mínima. Para calcular a variância mínima, temos\n$$\n\t\\mbox{E}\\big(U^2\\big) \\, = \\, \\sum_i\\sum_j\\sum_h\\sum_k \\mbox{E}(Z_{ij}Z_{hk}),\n$$\nonde\n$$\n\tZ_{ij}Z_{hk} \\, = \\, \\left\\{\\begin{array}{ll} 1, & \\mbox{se } \\, X_i < Y_j \\, \\mbox{ e } \\, X_h < Y_k \\\\ \n\t0, & \\mbox{caso contrário} \\end{array}\\right.,\n$$\nde modo a\n$$\n\t\\mbox{E}(Z_{ij}Z_{hk}) \\, = \\, P( X_i < Y_j, X_h < Y_k ) \\, = \\, \n\t\\left\\{ \\begin{array}{ll} \\displaystyle \\int F(x)G'(x)\\mbox{d}x, & \\mbox{ caso } \\, i = h, \\, j = k \\\\\n\t\\displaystyle \\int \\big( 1-G(x)\\big)^2 F'(x)\\mbox{d}x, & \\mbox{ caso } \\, i = h, \\, j \\neq k \\\\\n\t\\displaystyle \\int F^2(x)G'(x)\\mbox{d}x, & \\mbox{ caso } \\, i \\neq h, \\, j = k \\\\\n\t\\displaystyle \\Big(\\int F(x)G'(x)\\mbox{d}x\\Big)^2, & \\mbox{ caso } \\, i \\neq h, \\, j \\neq k \\\\\n\t\\end{array}\\right.\\cdot\n$$\nHá $mn$ termos com $i=h, j=k$; $m(m-1)n$ termos com $i\\neq h, j=k$; $mn(n-1)$ termos com $i=h, j\\neq k$ e $m(m-1)n(n-1)$ termos com $i\\neq h, j\\neq k$. Segue que\n$$\n\\begin{array}{rcl}\n\t\\mbox{E}\\big(U^2\\big) & = & \\displaystyle mn \\int F(x)G'(x)\\mbox{d}x \\, + \\, mn(n-1)\\int \\big( 1-G(x)\\big)^2 F'(x)\\mbox{d}x \\\\ \n\t& & \\, + \\, \\displaystyle m(m-1)n\\int F^2(x)G'(x)\\mbox{d}x \\, + \\, m(m-1)n(n-1)\\Big(\\int F(x)G'(x)\\mbox{d}x\\Big)^2,\n\\end{array}\n$$\nque leva &agrave; variância de $U$. Em particular, se $F = G$, então\n$$\n\t\\mbox{Var}(U) \\, = \\, \\frac{mn(m+n+1)}{12}\\cdot\n$$\n\n\n## Estimação de densidades\n\nDe certa forma, problemas de estimação não-paramétrica são extensões de problemas de estimação paramétrica, mas a natureza do primeiro é bem diferente do último. Considere, por exemplo, a situação de observações independentes identicamente distribuídas, digamos $X_1,X_2,\\cdots,X_n$. Em um problema paramétrico, assumimos que a distribuição de $X_i$ é $F(\\cdot;\\theta)$, a qual é totalmente especificada até o vetor de parâmetros $\\theta$; então o problema é essencialmente a estimaçã de $\\theta$. Em um problema não-paramétrico, a distribuição é totalmente desconhecida com, talvez, algumas restriçõs em propriedades gerais e, portanto, é denotada por $F$.\n\nAqui consideramos estimadores de $F$ em termos de função de densidade $f$. A função de densidade tem a vantagem de fornecer uma representação visualmente mais informativa da distribuição subjacente. Por exemplo, o histograma geralmente dá uma ideia aproximada da forma da distribuição. Este último ficou como o único estimador de densidade não paramétrico até 1950. Por essa razão, nossa discussão começará com os histogramas.\n\nEmbora o histograma seja usado extensivamente, não é tão frequente que seja necessária uma definição matemática. Uma maneira de defini-lo é através da função de densidade empírica.\n\n>\n**Definição**. Seja $f$ a derivada de $F$; por isso pode-se expressar como\n$$\n\tf(x)=\\lim_{h\\to 0}\\frac{F(x+h)-F(x-h)}{2h}\\cdot\n$$\nEntão, dizemos que $\\widehat{f}$, definido por\n$$\n\t\\widehat{f}(x)=\\frac{\\widehat{F}(x+h)-\\widehat{F}(x-h)}{2h},\n$$\né o histograma, sendo que $\\widehat{F}$ é a função de distribuição empírica.\n\n\nO parâmetro $h$ é chamado de largura de banda. Podemos escrever $\\widehat{f}$, definido acima como,\n$$\n\t\\widehat{f}(x)=\\frac{1}{2nh}\\sum_{i=1}^n \\pmb{1}_{(x-h;x+h)}(X_i)\\cdot\n$$\nPodemos excrever a função de densidade como $f(x)=\\lim_{h\\to 0} \\frac{1}{h}\\Big( F(x+h)-F(x-h) \\Big) $, mas não se \npode definir daqui o histograma porque, então, esse limite é zero ou infinito e assim em algum momento é preciso parar, em outras palavras, não se pode chegar muito perto de zero.\n\n::: {.callout-note appearance=\"simple\"}\n\n## Teorema \nSeja $f$ a função de densidade da função de distribuição $F$. Então, com probabilidade 1,  \n$$\n\t\\widehat{f}(x)\\sim Binomial(n,p),\n$$\ncom $p=F(x+h)-F(x-h)$. Assim, o comportamento assintótico do histograma pode ser derivado da distribuição binomial como\n$$\n\t\\mbox{E}\\big(\\widehat{f}(x)\\big)=\\frac{F(x+h)-F(x-h)}{2h}\n$$\ne\n$$\n\t\\mbox{Var}\\big(\\widehat{f}(x)\\big)=\\frac{p(1-p)}{4nh^2}\\cdot\n$$\n:::\n\n**Demonstração.** Exercício.\n\n\nDeste teorema segue que $\\widehat{f}(x)$ é um estimador consistente pontual de $f(x)$ quando $h\\to 0$ e $nh\\to \\infty$. A seguir, o processo de limite é entendido como $h=h_n$, de maneira que $h_n\\to 0$ e $nh_n\\to \\infty$. Estas \ncondições podem ser interpretadas como se fosse necessário $h_n$ ir a zero, mas não muito rápido. Isso é exatamente o que temos especulado, exceto que agora temos a taxa exata de convergência, que pode ser escrita como $h_n=o(n)$.\n\n**Exemplo**. Utilaremos dados simulados da distribuíão $N(0,1)$, com isso mostramos o histograma destes \n50 dados utilizando duas formas diferentes de encontrarmos uma expressão para $h_n$, a chamada largura de banda.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1340)\nx = rnorm(50)\npar(mar=c(4,2,1,1))\nhist(x, main = NULL, freq = FALSE, breaks = \"Sturges\")\nbox()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mar=c(4,2,1,1))\nhist(x, main = NULL, freq = FALSE, breaks = \"Scott\")\nbox()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-2.png){width=672}\n:::\n:::\n\n\nNeste exemplo utilizamos duas formas de escolher a largura de banda $h_n$ dentre três diferentes possibilidades programadas na função **hist**. Por padrão escolhe-se **breaks = \"Sturges\"**, porposto por Sturges (1929), o qual sugere que \n$$\n\th_n=\\frac{\\max(X_1,\\cdots,X_n)-\\min(X_1,\\cdots,X_n)}{1+3.322\\ln(n)}\\cdot\n$$\nA segunda situação indica que  o qual significa que se os dados provêm da distribuição Normal temos que $h_n=3.49 s n^{-1/3}$ sendo $s$ o desvio padrão estimado. Esta proposta deve-se &agrave; Scott (1979).\n\nEmbora o histograma é um estimador consistente quando $h_n\\to 0$ e $nh_n\\to \\infty$, verifica-se que se pode fazer melhor. A melhoria também é motivada por uma preocupação prática: o histograma não é uma função suave, uma propriedade que se pode esperar que qualquer função de densidade real tenha.\n\n>\n**Definição**. O estimador kernel da função de densidade é dado por\n$$\n\t\\widehat{f}(x)=\\frac{1}{nh_n}\\sum_{i=1}^n K\\Big( \\frac{x-X_i}{h_n}\\Big),\n$$\nonde $K(\\cdot)$ é uma função conhecida como kernel.\n\nÉ tipicamente assumido que $K$ seja não-negativa, simétrica em torno de zero e satisfaz $\\int K(u)\\mbox{d}u = 1$. \nClaro que o histograma é um caso especial do estimador do kernel se $K$ for escolhido como a função de densidade da \ndistribuição $ Uniforme(-1,1)$. O último não é uma função suave e é por isso que o histograma não é suave; mas escolhendo $K$ como uma função suave, tem-se um estimador de $f$ que seja suave.\n\nPor exemplo, escolhendo a função de densidade $N(0,1)$, temos por resultado o conhecido como kernel Gaussiano e assim \ntambém utilizando a densidade de densidade $Beta$ simétrica, dada por\n$$\n\tK(u)=\\frac{\\Gamma(\\nu+3/2)}{\\Gamma(1/2)\\Gamma(\\nu+1)}(1-u^2)^\\nu, \\qquad -1< u < 1,\n$$\ne $K(u)=0$ caso contrário. Os casos especiais $\\nu=0,1,2,3$ correspondem &agrave;s funções kernel uniforme, Epanechnikov, biweight e triweight, respectivamente. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nkernels = eval(formals(density.default)$kernel)\nplot (density(0, bw = 1), xlab = \"\", main = \"Diferentes kernel em R\")\nfor(i in 2:length(kernels)) lines(density(0, bw = 1, kernel =  kernels[i]), col = i)\nlegend(1.5,.4, legend = kernels, col = seq(kernels), lty = 1, cex = .8, y.intersp = 1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\nh.f = sapply(kernels, function(k) density(kernel = k, give.Rkern = TRUE))\nh.f = (h.f[\"gaussian\"] / h.f)^ .2\nh.f\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    gaussian epanechnikov  rectangular   triangular     biweight       cosine \n   1.0000000    1.0100567    0.9953989    1.0071923    1.0088217    1.0079575 \n   optcosine \n   1.0099458 \n```\n\n\n:::\n\n```{.r .cell-code}\nbw = bw.SJ(x) ## escolha automática\nplot(density(x, bw = bw), main = \"Larguras de banda equivalentes\")\nfor(i in 2:length(kernels)) lines(density(x, bw = bw, adjust = h.f[i], \n                                                           kernel = kernels[i]), col = i)\nlegend(55, 0.035, legend = kernels, col = seq(kernels), lty = 1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n:::\n\n\nUm problema prático importante na estimação de densidades via kernel é como escolher a largura de banda $h_n$. \nNote que dadas condições como $h_n\\to 0$ e $nh_n\\to\\infty$, ainda existem muitas opções para $h_n$. Então, de certo modo, a ordem de convergência ou divergência não resolve o problema. Uma solução para esse problema é conhecida como compensação de viés-variância. Antes de entrarmos nos detalhes, vamos primeiro apressentar um resultado em relação ao viés assintótico do estimador kernel. Aqui, o viés é definido como\n$$\n\t\\mbox{viés}\\big(\\widehat{f}(x)\\big)=\\mbox{E}\\big(\\widehat{f}(x)\\big)-f(x),\n$$\npara um dado $x$.\n\n::: {.callout-note appearance=\"simple\"}\n\n## Teorema\n\nSuponhamos que $f$ seja contínua e limitada. Então o viés do estimador kernel de densidade \nconverge a zero quando $h_n\\to 0$, para todo $x$.\n:::\n\n**Demonstração.** Observemos que\n$$\n\\begin{array}\n\t\\mbox{E}\\big(\\widehat{f}(x)\\big) & = & \\frac{1}{n}\\sum_{i=1}^n \\frac{1}{h_n}\\int K\\Big( \\frac{x-y}{h_n}\\Big)f(y)\\mbox{d}y \\\\\n\t& = & \\int K(u)f(x-h_n u)\\mbox{d}u \\, = \\, f(x)+\\int K(u)\\big( f(x-h_n u)-f(x)\\big)\\mbox{d}u\\cdot\n\\end{array}\n$$\nUtilizando então o teorema da convergência dominada completa-se a demonstração.&#9609\n\n::: {.callout-note appearance=\"simple\"}\n\n## Teorema\n\nSuponhamos que $f$ seja contínua três vezes diferenciável, com terceira derivada limitada na vizinhança de $x$ e $K$ satisfazendo\n$$\n\t\\int K^2(u)\\mbox{d}u<\\infty \\qquad \\mbox{e} \\qquad \\int |u|^3K(u)\\mbox{d}u<\\infty\\cdot\n$$\n<ul>\n\t<li> Se $h_n\\to 0$ quando $n\\to\\infty$, temos que\n\t$$\n\t\t\\mbox{viés}\\big(\\widehat{f}(x)\\big) = \\frac{h_n^2}{2}f''(x)\\int u^2K(u)\\mbox{d}u + o(h_n^2)\\cdot\n\t$$\n\t</li>\n\t<li> Se, além disso $nh_n\\to\\infty$ quando $n\\to\\infty$, então temos\n\t$$\n\t\t\\mbox{Var}\\big(\\widehat{f}(x)\\big) = \\frac{f(x)}{nh_n}\\int K^2(u)\\mbox{d}u + o\\big((nh_n)^{-1}\\big)\\cdot\n\t$$\n\t</li>\n</ul>\n:::\n\n**Demonstração**. A demonstração é baseada na expansão de Taylor,\n$$\n\tf(x-h_nu) = f(x)-h_nuf'(x)+\\frac{h_n^2u^2}{2}f''(x)-\\frac{h_n^3u^3}{6}f'''(\\epsilon),\n$$\nsendo $\\epsilon$ fica entre $x-h_nu$ e $x$. Os detalhes são deixados como um exercício.\n\nUma medida de precisão do estimador é o erro quadrático médio (EQM), dado por\n$$\n\tEQM\\big(\\widehat{f}(x)\\big) = \\mbox{E}\\big(\\widehat{f}(x)-f(x)\\big)^2\\cdot \n$$\né fácil mostrar que o $EQM$ combina o viés e a variância de tal maneira que\n$$\n\tEQM\\big(\\widehat{f}(x)\\big) = \\mbox{viés}\\big(\\widehat{f}(x)\\big)^2 + \\mbox{Var}\\big(\\widehat{f}(x)\\big)\\cdot\n$$\nVemos que, sob as condições $h_n\\to 0$ e $nh_n\\to \\infty$ e se ignorarmos os termos de baixa ordem, temos\n$$\n\tEQM\\big(\\widehat{f}(x)\\big) \\approx \\frac{h_n^4}{4}\\big(f''(x)\\big)^2\\tau^4 + \\frac{f(x)}{nh_n}\\gamma^2,\n$$\nonde $\\tau^2=\\int u^2K(u)\\mbox{d}u$ e $\\gamma^2=\\int K^2(u)\\mbox{d}u$. O termo &agrave; direita da expressão acima é \nminimizada quando\n$$\n\th_n = \\left( \\frac{\\gamma^2f(x)}{\\tau^4\\big(f''(x)\\big)^2} \\right)^{\\frac{1}{5}}n^{-\\frac{1}{5}}\\cdot\n$$\nNote ainda que a expressão acima não é a solução ideal, isso porque $f$ é desconhecida na prática. No entanto, dá-nos pelo menos alguma ideia sobre a taxa ideal de convergência a zero, sendo esta $h_n=O(n^{-\\frac{1}{5}})$.\n\nQuando $f$ é desconhecida, uma abordagem natural seria substituí-lo por um estimador e, assim, obter uma largura de banda ideal estimada. Uma complicação é que a largura de banda ideal depende de $x$ mas, idealmente, gostaríamos de \nusar uma largura de banda que funcionasse para diferentes $x$ dentro de um certo intervalo, se não todos os $x$. Para obter uma largura de banda ideal que não depende de $x$, integramos os dois lados da expressão de $EQM$ em relação a \n$x$. Isto nos leva a\n$$\n\t\\int EQM\\big(\\widehat{f}(x)\\big)\\mbox{d}x \\approx \\frac{\\tau^4h_n^4}{4}\\int \\big(f''(x)\\big)^2 \\mbox{d}x + \n\t\\frac{\\gamma^2}{nh_n}\\int f(x)\\mbox{d}x \\, = \\, \\frac{\\tau^4\\theta^2h_n^4}{4} + \\frac{\\gamma^2}{nh_n},\n$$\ncom $\\theta^2=\\int \\big(f''(x)\\big)^2 \\mbox{d}x$. Pelo mesmo argumento, o lado direito acima é minimizado quando\n\n$$\n\th_n = \\left( \\frac{\\gamma^2}{\\tau^4\\theta^2} \\right)^{\\frac{1}{5}}n^{-\\frac{1}{5}}\\cdot\n$$\nDesta vez, o $h_n$ ideal não depende de $x$. Além disso, a integral do $EQM$ ou o $IEQM$ mínimo é dado por\n$$\n\tIEQM \\, = \\, \\int EQM\\big(\\widehat{f}(x)\\big)\\mbox{d}x \\, = \\, \\frac{5}{4}\\big( \\tau\\gamma^2 \\big)^\\frac{4}{5}\\theta^{\\frac{2}{5}}\n\tn^{-\\frac{4}{5}}\\cdot\n$$\n\nUma implicação é a seguinte. Note que o $IEQM$ depende do kernel $K$ através de $c_K=\\big( \\tau\\gamma^2 \\big)^\\frac{4}{5}$. Mostrou-se que para os kernels comumente usados, tais como aqueles listados, o desempenho dos estimadores de kernel correspondentes é quase o mesmo em termos dos valores de $c_K$. Voltando ao problema sobre a estimação da largura de banda ideal, vemos que tudo o que precisamos é encontrar um estimador consistente de $\\theta^2$. Se $f$ é a função de densidade da distribuição normal com desvio padrão $\\sigma$, então pode ser mostrado que $\\theta^2=3/8\\sqrt{\\pi}\\sigma^5$. Naturalmente, se alguém souber que $f$ é normal, então a estimação \nda densidade não-paramétrica não seria necessária, porque um método paramétrico provavelmente seria melhor. Em geral, pode-se expandir $f$ em torno da densidade gaussiana usando a expansão de Edgeworth.\n\nUtilizando a abordagem acima, Hjort and Jones (1996) obteveram o seguinte estimador ótimo para a largura de banda\n$$\n\t\\widehat{h}_n = \\widehat{h}_0\\left(1+\\frac{35}{48}\\widehat{\\gamma}_4+\\frac{35}{32}\\widehat{\\gamma}_3^2+\n\t\t\\frac{385}{1024}\\widehat{\\gamma}_4^2\\right)^{-\\frac{1}{5}},\n$$\nonde $\\widehat{h}_0$ é a estimativa ideal da largura de banda assumindo que $f$ é normal, isto é, com $\\theta^2$ \nsubstituído por $3/8\\sqrt{\\pi}\\sigma^5$ ou mais explicitamente\n$$\n\t\\widehat{h}_0=1.06\\left(\\frac{\\widehat{\\sigma}}{n^{\\frac{1}{5}}}\\right),\n$$\nchamamos $\\widehat{h}_0$ a largura de banda da linha de base e $\\widehat{\\sigma}^2$ é a variância amostral dada por\n$$\n\t\\widehat{\\sigma}^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\overline{X})^2\\cdot\n$$\nAlém disso, $\\widehat{\\gamma}_3$ e $\\widehat{\\gamma}_4$ são os estimadores dos coefcientes de assimetria de amostra e curtose, dado por\n$$\n\t\\widehat{\\gamma}_3=\\frac{1}{(n-1)\\widehat{\\sigma}^3}\\sum_{i=1}^n (X_i-\\overline{X})^3\n$$\ne\n$$\n\t\\widehat{\\gamma}_4=\\frac{1}{(n-1)\\widehat{\\sigma}^4}\\sum_{i=1}^n (X_i-\\overline{X})^4-3,\n$$\nrespectivamente. Houve outras abordagens para a seleção da largura de banda ótima, incluindo o método de validação cruzada. Ambos procedimentos foram programados na função **density**.\n\n\n\n**Exemplo**. Utilaremos os dados simulados da distribuíão $N(0,1)$ no exemplo anterior para com isso mostrarmos o histograma e o estimador Kernel da função de densidade.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1340)\nx = rnorm(50)\npar(mar=c(4,2,1,1))\nhist(x, main = NULL, freq = FALSE, breaks = \"Sturges\")\nbox()\nlines(density(x, bw = \"nrd0\"), col = \"red\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mar=c(4,2,1,1))\nhist(x, main = NULL, freq = FALSE, breaks = \"Scott\")\nbox()\nlines(density(x, bw = \"bcv\"), col = \"red\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in bw.bcv(x): mínimo ocorreu em uma das extremidades do intervalo\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n:::\n\n\n## Exercícios\n\n1. Seja $T(X_1,\\cdots,X_n)$ uma estatística simétrica nas observaçóes. Mostre que $T$ pode ser escrita como função das estatísticas de ordem. Por outro lado, se $T(X_1,\\cdots,X_n)$ pode ser escrita como função das estatísticas de ordem, $T$ é simétrica nas observações.\n\n2. Sejam $X_1,X_2,\\cdots,X_m$ e $Y_1,Y_2,\\cdots,Y_n$ amostras independentes de duas distribuições absolutamente contínuas. Encontre o estimador não viciado de mínima variância de:\n\n(a) $\\mbox{E}(XY)$\n(b) $\\mbox{Var}(X+Y)$\n\n3. Seja $(X_1,Y_1), (X_2,Y_2), \\cdots, (X_n,Y_n)$ uma amostra aleatória com distribuição absolutamente contínua bivariada. Encontre o estimador não viciado de mínima variância de:\n\n(a) $\\mbox{E}(XY)$\n(b) $\\mbox{Var}(X+Y)$\n\n4. Considere $(\\mathcal{R},\\mathcal{B},P_\\theta)$ um espaço de probabilidade e $\\mathcal{P}=\\{P_\\theta \\, : \\theta\\in\\Theta\\}$. Seja $A$ um elemento da $\\sigma$-álgebra de Borel e considere $d(\\theta)=P_\\theta(A)$.  \n\n(a) A função $d$ é estimável? Se sim, qual é o grau?\n(b) Encontre o estimador não viciado de mínima variância de $d$, baseado em uma amostra de tamanho $n$ e assumindo que $\\mathcal{P}$ seja a classe de todas as distribuições contínuas.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}