{
  "hash": "a421ca8975a8bf061cf1b8a3ca9d7895",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Estatísticas de Ordem\"\ndate: 2024-03-01\ncode-fold: true\n---\n\n\n\n\n\nQuando estudadas as idéias e técnicas da teoria das probabilidades fundamentais, criamos um modelo matemático de um ensaio aleatório, associando-o com um espaço de amostragem no qual os acontecimentos aleatórios correspondem a um conjunto de uma certa $\\sigma$-álgebra. A noção de probabilidade definida nesta $\\sigma$-álgebra corresponde à noção de incerteza no resultado em qualquer realização do experimento aleatório.\n\nVamos começar o estudo de alguns problemas de estatística matemática. Suponha que buscamos informações sobre algumas características numéricas de um conjunto de elementos, chamado de uma população. Por razões de custo, de tempo ou simplesmente para não destruir todos os elementos amostrados podemos não querer ou não poder estudar cada elemento da população. Nosso objetivo é tirar conclusões sobre as características desconhecidas da população com base em informações sobre algumas características de uma amostra adequadamente selecionada.\n\nFormalmente, seja $X$ uma variável aleatória que descreve a população sob investigação e seja $F$ a função de distribuição de $X$. Há duas possibilidades: ou $X$ tem função de distribuição $F(\\cdot ; \\theta)$ com uma forma funcional conhecida exceto, talvez, para o parâmetro $\\theta$, o qual pode ser um vetor ou $X$ tem uma função de distribuição $F$ sobre a qual não sabemos nada, exceto talvez que $F$ seja, digamos, absolutamente contínua. No primeiro caso, seja $\\Theta$ o conjunto dos possíveis valores do parâmetro desconhecido $\\theta$. Seguidamente, o trabalho de um estatístico é decidir com base em uma amostra selecionada adequadamente que membro ou membros da família $\\{F (\\cdot ; \\theta) : \\theta \\in \\Theta\\}$ pode representar a função de distribuição de $X$. Problemas desse tipo são chamados de problemas de inferência estatística paramétrica e o espaço estatístico é dado por $(\\mathbb{R}, {F (·; \\theta) : \\theta \\ \\Theta}, \\beta (\\mathbb{R}))$, sendo $\\mathbb{R}$ o conjunto dos reais na reta, ${F (\\cdot; \\theta) : \\theta \\in  \\Theta}$ a família de distribuições de $X$ e $\\beta (\\mathbb{R})$ a $\\sigma$-álgebra dos Borelianos na reta. O caso em que nada se sabe sobre a forma funcional da função de distribuição $F$ de $X$ é claramente muito mais difícil. Problemas de inferência deste tipo de são o domínio de estudo da estatística não paramétrica. Neste livro abordamos os problemas da estatística paramétrica.\n\n## Amostras aleatórias\n\nConsidere-se um experimento estatístico que culmina em desfechos $x$, que são os valores assumidos por uma variável aleatória $X$. Seja $F$ a função de distribuição de $X$. Na prática, $F$ não será completamente conhecida, isto é, um ou mais parâmetros associados com $F$ serão desconhecidos. O trabalho de um estatístico é estimar esses parâmetros desconhecidos ou testar a validade de certas afirmações sobre eles. Ele pode, por exemplo, obter $n$ observações independentes de $X$. Isso significa que ele observa $n$ valores $x1, x2, \\cdots , x_n$ assumidos da variável aleatória $X$. Cada $x_i$ pode ser considerado como o valor assumido pela variável aleatória $X_i, i = 1, 2, \\cdots , n$ onde $X_1, X_2, \\cdots, X_n$ são variáveis aleatórias independentes com distribuição comum $F$ . Os valores observados $(x_1, x_2, \\cdots, x_n)$ são então valores assumidos por $(X_1, X_2, \\cdots, X_n)$. O conjunto $(X_1, X_2, \\cdots , X_n)$ é, então, uma amostra de tamanho $n$ da distribuição da população $F$ . O conjunto de $n$ valores $x_1, x_2, \\cdots , x_n$ é chamado de uma realização ou estimativa da amostra. Note-se que os possíveis valores do vector aleatório $(X_1, X_2, \\cdots , X_n)$ podem ser olhados como pontos em $\\mathbb{R}^n$, os quais podem ser chamados de elementos do espaço amostral. Na prática podemos não observar $x_1, x_2, \\cdots, x_n$ mas alguma função $g(x_1, x_2, \\cdots , x_n)$. Então $g(x_1, x_2, \\cdots , x_n)$ serão considerados os valores assumidos pela variável aleatória $g(X)$.\n\nVamos agora formalizar esses conceitos.\n\n>\n**Definição**.\nSeja $X$ uma variável aleatória com função de distribuição $F$ e $X_1, \\cdots, X_n$ variáveis aleatórias independentes com distribuição comum $F$. Chamaremos a coleção $X_1, \\cdots, X_n$ de uma amostra aletória de tamanho $n$ de $F$ ou simplesmente como $n$ observações independentes de $X$.\n\nSe $X_1, X_2, \\cdots , X_n$ é uma amostra aleatória de $F (\\cdot; \\theta)$, a função de distribuição conjunta é dada por:\n\n$$F(x_1, \\cdots, x_n; \\theta) = \\prod_{i=1}^{n} F(x_i; \\theta)$$\n\n>\n**Definição**.\nSejam $X_1, X_2, \\cdots, X_n$ $n$ observações independentes da variável aleatória $X$ e seja  $g: \\mathbb{R}^n \\to \\mathbb{R}$ uma função real derivável. Então a variável aleatória $g(X_1, X_2, \\cdots, X_n)$ é chamada de estatística, desde que não dependa de parâmetros desconhecidos.\n\nSegundo esta definição cada uma das variáveis na amostra isoladamente é uma estatística assim como funções destas que, eventualmente, podem não fornecer informações úteis. Duas das estatísticas mais comumente utilizadas são mostradas no exemplo a seguir.\n\n>\n**Definição**.\nSeja $X_1, X_2, \\cdots, X_n$ uma amostra aleatória da função de distribuição $F$. A estatística\n$$\\overline{X}_n = \\sum_{i=1}^{n} \\displaystyle \\frac{Xi}{n},$$\né chamada de média amostral e a estatística \n$$S_{n}^{2} = \\sum_{i=1}^{n} \\displaystyle \\frac{(X_i - \\overline{X}_n)^2}{n-1}$$\né chama de variância amostral.\n\nDeve-se lembrar que as estatísticas amostrais apresentadas neste exemplo $\\overline{X}_n, S_{n}^{2}$ e outras que irão definir-se posteriormente são variáveis aleatórias, com todas as consequências que isso implica, enquanto os parâmetros populacionais $\\mu, \\sigma^2$ e assim por diante são constantes fixas, que podem ser desconhecidas.\n\n**Exemplo**. Seja $X \\sim Bernoulli(p)$, onde $p$ é desconhecido. A função de distribuição de $X$, mostrada na   Figura 2.1, é dada por\n\n$$F(x) = p\\delta (x − 1) + (1 − p)\\delta(x),\tx \\in \\mathbb{R},$$\nonde a função $\\delta(\\cdot)$ foi definida em (1.2) como $\\delta(x) = \\begin{cases} 1, \\ x \\ \\geq \\ 0, \\\\ 0, \\ x \\ < 0 \\end{cases}$, chamada de função delta.\n \nSuponha que cinco observações independentes de $X$ sejam 0, 1, 1, 1, 0. Então 0, 1, 1, 1, 0 é uma realização da amostra $X_1, X_2, \\cdots , X_5$. A estimativa da média amostral é\n\n$$\\overline{x}_5 = \\displaystyle \\frac{0 + 1 + 1 + 1 + 0}{5} = 0, 6$$\no qual é o valor assumido pela variável aleatória $\\overline{X}_n$. A estimativa da variância amostral é\n$$S_{5}^{2} = \\sum_{i=1}{5} \\displaystyle \\frac{(x_i - \\overline{x}_5)}{5 - 1}= \\displaystyle \\frac{2 \\ \\times (0,6)^2 \\ + \\ 3 \\ \\times (0,4)^2}{4}= 0,3$$\n\nsendo este o valor assumido pela variável aleatória $S_{5}^{2}$.\n\n\n![Figura 2.1: Representação da função de distribuição Bernoulli para três valores do parâmetro $p$ = 0.3, 0.5 e 0.8. Observe que nesta curva a reta no intervalo (0, 1) depende de 1 - $p$, isso porque a função $\\delta$ é sempre zero para $x$ - 1 nesse intervalo.](Figura 2.1.png)\n\n**Exemplo**. Seja $X \\sim N (\\mu, \\sigma^2)$, $\\mu$ conhecida, $\\sigma^2$ desconhecido e $X_1, \\cdots , X_n$ uma amostra aleatória dessa distribuição. De acordo com nossa definição, a função $\\sum_{i=1}^{n} X_{i} / \\sigma^2$ não é uma estatística. Suponha que cinco observações de $X$ -0,864; 0,561; 2.355; 0,582 e -0,774. Então, a estimativa da média amostral é 0.372 e a estimativa da variância amostral é 1.648.\n\n##\tEstatísticas de ordem\n\nSeja $(X_1, X_2, \\cdots , X_n)$ um vetor aleatório n-dimensional e $(x_1, x_2, \\cdots , x_n)$ uma $n$-tupla assumida por $(X_1, X_2, \\cdots , X_n)$. Vamos organizar $x_1, x_2, \\cdots , x_n$ em ordem crescente de magnitude, para que\n$$x_{(1)}\\le x_{(2)} \\le \\cdots ≤ x_{(n)},$$\nonde $x_{(1)}$ = min$(x_1, x_2, \\cdots , x_n)$, $x_{(2)}$ é o segundo menor valor em $x_1, \\cdots , x_n$ e assim por diante, $x_{(n)}$ = max$(x_1, \\cdots , x_n)$. Se quaisquer dois $x_i$, $x_j$ forem iguais, a ordem não importa.\n \n>\n**Definição**.\nA função $X_{(k)}$ de $(X_1, \\cdots ,X_n)$ que assume o valor $x_{(k)}$ em cada possível sequência $(x_1, x_2, \\cdots , x_n)$ de valores assumidos por $(X_1,X_2, \\cdots , X_n)$ é conhecida como a $k$-ésima estatística de ordem ou a estatística de ordem $k$.\nO conjunto $(X_{(1)},X_{(2)}, \\cdots , X_{(n)})$ é chamado de estatísticas de ordem para $(X_1, X_2, \\cdots , X_n)$.\n\n\n**Exemplo**. Consideremos $X_1$, $X_2$, $X_3$ três variáveis aleatórias discretas de maneira que, $X_1$ e $X_3$ sejam tais que assumam somente valores 0, 1 e que $X_2$ assuma valores 1, 2, 3. O vetor aleatório $(X_1$, $X_2$, $X_3)$ assume os valores: (0, 1, 0), (0, 2, 0), (0, 3, 0), (0, 1, 1), (0, 2, 1), (0, 3, 1), (1, 1, 0), (1, 2, 0), (1, 3, 0), (1, 1, 1), (1, 2, 1) e (1, 3, 1). Então $X_{(1)}$ assume somente valores 0 ou 1; $X_{(2)}$ assume somente valores 0 ou 1 e $X_{(3)}$ assume somente valores 1, 2 ou 3.\n\n::: {.callout-note appearance=\"simple\"}\n## Teorema 2.1\n\nSeja $(X_{(1)}, X_{(2)}, \\cdots , X_{(n)})$ um vetor aleatório de dimensão $n$ e seja $X_{(k)}, 1 \\le k \\le n$, a $k$-ésima estatística de ordem. Então $X_{(k)}$ é também uma variável aleatória.\n:::\n\n**Exercício**. Na apresentação dos resultados a seguir assumiremos que $X_1, X_2, \\cdots , X_n$ são variáveis aleatórias independentes e igualmente distribuídas contínuas com função de densidade $f$ . Seja $\\{ X_{(1)}, X_{(2)}, \\cdots , X_{(n)} \\}$ o conjunto das estatística de ordem para $X_1, X_2, \\cdots , X_n$. Dado que todas as $X_i$ são contínuas segue que, com probabilidade 1\n$$X_{(1)} \\le X_{(2)} \\le \\cdots \\le X_{(n)}·$$\n\n### Propriedades das estatísticas de ordem\n\nComeçaremos o estudo das propriedades encontrando a função de densidade conjunta de $(X_{(1)}, X_{(2)}, \\cdots , X_{(n)})$.\n\n::: {.callout-note appearance=\"simple\"}\n## Teorema 2.2\n\nSejam $X_1, \\cdots, X_n$ variáveis aleatórias contínuas independentes, igualmente distríbuidas com densidade $f$. A função de desindade conjunta de $(X_{(1)}, \\cdots, X_{(n)})$ é dada por\n$$\nf(x_{(1)}, \\ldots, x_{(n)}) = \n\\begin{cases} \nn! \\prod\\limits_{i=1}^{n} f(x_{(i)}), & \\text{se } x_{(1)} < x_{(2)} < \\cdots < x_{(n)} \\\\\n0, & \\text{caso contrário}\n\\end{cases}.\n$$\n\n:::\n\n**Demonstração**: A transformação de $(X_1, \\cdots , X_n)$ a $(X_{(1)}, \\cdots , X_{(n)})$ não é biunívoca. De fato, existem um total de $n!$ possíveis arranjos de $x_1, \\cdots , x_n$ em ordem crescente de magnitude. Assim, existem $n!$ inversas para a transformação.\n\nPor exemplo, uma das $n!$ permutações pode ser\n$$\nx_4 < x_1 < x_{n−1} < x_3 < \\cdots < x_n < x_2·\n$$ \n\nA inversa correspondente é\n$$\nx_4 = x_{(1)}, x_1 = x_{(2)}, \\ x_{n−1} = x_{(3)}, \\ x_3 = x_{(4)} \\cdots x_n = x_{(n−1)}, \\ x_2 = x_{(n)}·\n$$\nO determinante Jacobiano desta transformação é a matriz $n \\times n$ identidade com as colunas reorganizadas, isto devido a que cada $x_{(i)}$ é igual a uma, e somente uma, das $x_1, x_2, \\cdots , x_n$. Portanto $J = \\pm 1$ e\n\n$$\nf (x_{(2)}, \\ x_{(n)}, \\ x_{(4)}, \\ x_{(1)}, \\ \\cdots , \\ x_{(3)}, \\ x_{(n−1)}) \\ \\mid J \\mid =\n\\prod\\limits_{i=1}^{n} \\ f(x_{(i)}),\n$$\n\nquando $x_{(1)} < x_{(2)} < \\cdots < x_{(n)}$. A mesma expressão é válida para cada um dos $n!$ arranjos. Segue então que\n \n$$\n\\begin{align*}\nf(x_{(1)}, \\ldots, x_{(n)}) &= \\sum \\prod\\limits_{i=1}^{n} f(x_{(i)}) \\\\\n&\\ \\ \\ \\ \\ \\ \\text{Todas as } n! \\text{ permutações} \\\\\n& = \\begin{cases} \nn! \\prod\\limits_{i=1}^{n} f(x_{(i)}), & \\text{se } x_{(1)} < x_{(2)} < \\cdots < x_{(n)} \\\\\n0, & \\text{caso contrário}\n\\end{cases}.\n\\end{align*}\n$$\n\n**Exemplo**.\nSejam $X_1, \\cdots , X_n$ variáveis aleatórias independentes com função de densidade comum\n$$\nf(x) = \n\\begin{cases} \n1, & \\text{se } 0 < x < 1 \\\\\n0, & \\text{caso contrário}\n\\end{cases}.\n$$\nEntão a função de densidade conjunta de $X_{(1)}, X_{(2)}, \\cdots , X_{(n)}$ é\n$$\nf(x_{(1)}, \\ \\cdots, \\ x_{(n)}) = \n\\begin{cases} \nn!, & \\text{se } 0 < x_{(1)} < x_{(2)} < \\cdots < x_{(n)} \\\\\n0, & \\text{caso contrário}\n\\end{cases}.\n$$\n \nEstamos confiados que como resultado do Teorema 2.2 temos funções de densidade. Vejamos neste exemplo se isso é realmente acontece. Consideremos, para simplificar, o caso $n$ = 3 e verifiquemos se a integral da função de densidade é 1. Então\n\n$$\n\\begin{align*}\n\\int \\int\\limits_\\mathbb{R} \\int f(x_{(1)},x_{(2)},x_{(3)}) \\ dx_{(1)}dx_{(2)}dx_{(3)})\n& = 6 \\int_{0}^1 \\left[\\int_{x_{(1)}}^1 \\left(\\int_{x_{(2)}}^1 dx_{(3)} \\right) dx_{(2)} \\right] dx_{(1)} \\\\\n& = 6 \\int_{0}^1 \\left[\\int_{x_{(1)}}^1 \\left(1 - x_{(2)} \\right) dx_{(2)} \\right] dx_{(1)} \\\\\n& = 6 \\int_{0}^1 \\left[\\frac{1}{2} - x_{(1}) + \\frac{x^2_{(1)}}{2} \\right] dx_{(1)} = 1. \\\\\n\\end{align*}\n$$\n\nUm detalhe interessante é que esta e outras propriedades demonstradas aqui somente são válidas quando as variáveis aleatórias são contínuas. Isso não significa que estatísticas de ordem não possam ser definidas no caso discreto. O que estamos dizendo é que estas propriedades somente podem ser demonstradas no caso contínuo.\n\n**Exemplo**.\nConsideremos a situação em que temos somente três variáveis aleatórias independentes $X_1$, $X_2$ e $X_3$ com distribuição geométrica de parâmetro $p$, isto é,\n\n$$\nP (X = x; p) = (1 − p)p^x, \\ \\ \\ \\ \\ \tx = 0, 1, 2, \\cdots\n$$\nEncontremos $P (X_{(1)} < X_{(2)} < X_{(3)})$.\nNesta situação a probabilidade requerida pode ser escrita como:\n$$\n\\begin{align*}\nP (X_{(1)} < X_{(2)} < X_{(3)})  =  1 − P (X_1 = X_2 \\neq X_3) − P (X_1 = X_3 \\neq X_2) \\\\\n−P (X_2 = X_3 \\neq X_1) − P (X_1 = X_2 = X_3)\n\\end{align*}\n$$\na qual pode ser escrita como\n$$\n\\begin{align*} \nP (X_{(1)} < X_{(2)} < X_{(3)}) \n& = 1 − 3P (X_1 = X_2 \\neq X_3) − P (X_1 = X_2 = X_3) \\\\\n& = 1 − 3 [P (X_1 = X_2) − P (X_1 = X_2 = X_3)] − P (X_1 = X_2 = X_3) \\\\\n& = 1 − 3P (X_1 = X_2) + 2P (X_1 = X_2 = X_3)· \\\\\n\\end{align*}\n$$\n \nNão é difícil perceber que\n$$\nP(X_1 = X_2) = \\frac{(1-p)^2}{1-p^2},\n$$\n\ne que\n$$\nP(X_1 = X_2 = X_3) = \\frac{(1-p)^3}{1-p^3},\n$$\n\ndo qual obtemos que\n$$\nP(X_{(1)} = X_{(2)} = X_{(3)}) = \\frac{6p^3}{(1-p)(1+p+p^2)}.\n$$\nAs propriedades das estatísticas de ordem que serão demonstradas valerão somente caso as variáveis sejam contínuas. Isto deve-se a que, caso as variáveis sejam discretas, a probabilidade\n\n$$\nP (X_{(1)} = X_{(2)} = \\cdots = X_{(n)}) \\neq 0,\n$$\n\ncomo vai ser mostrado no seguinte exemplo. Acontece que o fato da probabilidade das estatística de ordem poderem coincidir, com probabilidade diferente de zero, altera a estrutura da demonstração e não nos permite obtermos estes resultados para o caso discreto.\n\n**Exemplo**.\nSejam $X_1, X_2, \\cdots , X_n$ variáveis aleatórias independentes assumindo somente 0 e 1 com probabilidade 1/2. Observemos que\n$$\n\\begin{align*}\nP(X_{(1)} = X_{(2)} = \\cdots = X_{(n)})\n& = \\prod\\limits_{k=1}^{n} P(X_{(k)} = 0) + \\prod\\limits_{k=1}^{n} P(X_{(k)} = 1)  \\\\\n& = \\prod\\limits_{k=1}^{n} P(X_{(k)} = 0) + \\prod\\limits_{k=1}^{n} P(X_{k} = 1) = \\frac{1}{2^{n-1}} . \\\\\n\\end{align*}\n$$\n\nEstudemos agora o comportamento marginal, ou seja, nos interessa agora encontrar a função de distribuição marginal de cada estatística de ordem.\n \n::: {.callout-note appearance=\"simple\"}\n## Teorema 2.3\n\nSejam $X_1, \\cdots, X_n$ variáveis aleatórias contínuas independentes e igualmente distribuídas e $(X_{(1)}, \\cdots, X_{(n)})$ as estatísticas de ordem. A função de densidade marginal de $X_{(r)}$ é dada por\n$$\nf_r(x_{(r)}) = \\frac{n!}{(r-1)!(n-r)!}[F(x_{(r)})]^{r-1}[1-F(x_{(r)})]^{n-r}f(x_{(r)}),\n$$\nonde $F$ é a função de distribuição comum de $X_1, \\cdots, X_n$.\n:::\n \nDemonstração : Partimos da expressão da função de densidade conjunta das estatísticas de ordem obtida no Teorema 2.2. Então,\n\n$$\n\\begin{align*}\nf_r(x_{(r)}) \n& = n!f(x_{(r)})  \n\\int_{-\\infty}^{x_{(r)}} \\int_{-\\infty}^{x_{(r-1)}} \\cdots\n\\int_{-\\infty}^{x_{(2)}} \\int_{x_{(r)}}^{+\\infty} \\int_{x_{(r+1)}}^{+\\infty} \\cdots\n\\int_{x_{(n-1)}}^{+\\infty}\n\\prod\\limits_{i \\neq r}^{n} f(t_i) \\ \n\\text{d}t_n \\cdots\\text{d}t_{r+1} \\ \\text{d}t_{1} \\cdots \\text{d}t_{r-1}  \\\\\n\n& = n!f(x_{(r)})\n\\frac{[1-F(x_{(r)})]^{n-r}}{(n-r)!} \n\\int_{-\\infty}^{x_{(r)}} \\cdots\n\\int_{x_{(-\\infty)}}^{x_{(2)}}\n\\prod\\limits_{i =1}^{r-1} f(t_i) \\ \\text{d}t_i \\\\\n\n& = n!f(x_{(r)})\n\\frac{[1-F(x_{(r)})]^{n-r}}{(n-r)!} \n\\frac{[F(x_{(r)})]^{r-1}}\n{(r-1)!}.\n\n\\end{align*}\n$$\n \nComo utilidade deste teorema podemos mencionar o fato de agora podermos encontrar os momentos das estatística de ordem. Faremos isso como consequência do seguinte exemplo.\n\n**Exemplo**.\nSejam $X_1, X_2, \\cdots , X_n$ variáveis aleatórias independentes $U$ (0, 1). Então\n$$\nf_r(x_{(r)}) =\n\\begin{cases} \n\\frac{n!}{(r-1)!(n-r)!} x_{(r)}^{r-1}(1-x_{(r)})^{n-r}, \n& \\text{se } 0 < x_{(r)} < 1 \\\\\n\n& \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  (1 \\leq r \\leq n) \\\\\n0, & \\text{caso contrário}\n\\end{cases}.\n$$\n\nObservemos que, na situação do exemplo acima,\n\n$$\nX_{(r)} \\sim \\beta(r, n − r + 1),\n$$\n\nlogo, valem os resultados da distribuição Beta e, por exemplo,\n\n$$\nE(X_{(r)}) = r/(n + 1)·\n$$\n\nPara uma densidade qualquer e somente quatro variáveis aleatórias a forma da densidade marginal, de uma qualquer estatística de ordem, é mostrada no seguinte exemplo.\n\n**Exemplo**.\nSejam $X_1, X_2, X_3 ,X_4$ variáveis aleatórias independentes com densidade comum $f$.\tA função de densidade conjunta das estatísticas de ordem $X_{(1)}, X_{(2)}, X_{(3)}, X_{(4)}$ é\n\n$$ \nf(x_{(1)}, x_{(2)}, x_{(3)}, x_{(4)}) =\n\\begin{cases}\n4!f (x_{(1)})f (x_{(2)})f (x_{(3)})f (x_{(4)}),\t\n& \\text{se }\tx_{(1)} < x_{(2)} < x_{(3)} < x_{(4)} \\\\\n\n0,\t\n& \\text{caso contrário}\n\\end{cases}.\n$$\n \n\nVamos calcular a função de densidade marginal de $X_{(2)}$. Temos que, se $x_{(1)} < x_{(2)} < x_{(3)} < x_{(4)}$\n\n$$\n\\begin{align*}\nf2(x_{(2)})\n& =  4! \\int \\int \\int \\int f(t_1) f(x_{(2)}) f(t_3) f(t_4)\n\\ \\text{d}t_1 \\ \\text{d}t_3 \\ \\text{d}t_4 \\\\\n\n& =  4! f(x_{(2)}) \n\\int_{-\\infty}^{x_{(2)}}\n\\int_{2}^{+\\infty}\n\\left[\\int_{t_3}^{+\\infty} f(t_4) \\ \\text{d}t_4\\right]\nf(t_3) f(t_1) \\ \\text{d}t_3 \\ \\text{d}t_1  \\\\\n\n& =  4! f(x_{(2)}) \n\\int_{-\\infty}^{x_{(2)}}\n\\Biggl\\{\n\\int_{x_{(2)}}^{+\\infty}[1 - F(t_3)]f(t_3) \\text{d}t_3\n\\Biggl\\}\nf{(t_1)} \\text{d}t_1\\\\\n\n& =  4! f(x_{(2)}) \n\\int_{-\\infty}^{x_{(2)}}\n\\frac{[1-F(x_{(2)})]^2}{2}\nf(t_1) \\ \\text{d}t_1 =\n4! f(x_{(2)}) \n\\frac{[1-F(x_{(2)})]^2}{2}\nF(x_{(2)})\n\n·\n\\end{align*}\n$$\n \nEvidentemente, a expressão acima coincide com o resultado apresentado no Teorema 2.3.\n\n\n\n\n\n\n## pg8\n\n \nDemonstração :\n \n\n\n\n\nfrs(x(r), x(s))  =\n \n\n\n\n∫ x(r)\n \n\n\n\\cdots\n \n\n\n∫ t2\n \n\n\n\n∫ x(s)\n \n\n\n\\cdots\n \n\n\n∫ x(s) ∫ +∞\n \n\n\n\\cdots\n \n\n\n∫ +∞\n \n−∞\t−∞\n \nx(r)\n \nts−2\n \nx(s)\n \ntn−1\n \nn!f (t1) \\cdots $F$ (tn) dtn \\cdots dts+1 dts−1 \\cdots dtr+1 dt1 \\cdots dtr−1\n \n\n=  n!\n \n∫ x(r)\n \n\n\\cdots\n \n∫ t2\n \n∫ x(s)\n \n\n\\cdots\n \n∫ x(s) [1 − $F$ (x(s))]n−s\n \n\n \n−∞\t−∞\n \nx(r)\n \nts−2\n \n(n − s)!\n \n×f (t1)f (t2) \\cdots $F$ (x(s)) dts−1 \\cdots dtr+1 dt1 \\cdots dtr−1\n \n\n=  n!\n \n[1 − $F$ (x(s))]n−s\n(n − s)!\tf (x(s))\n \nx(r)\n\n−∞\n \n\\cdots\n \nt2\nf (t1) \\cdots $F$ (x(r))×\n−∞\n \n[F (x(s)) − $F$ (x(r))]s−r−1\n×\t(s − r − 1)!\tdt1 \\cdots dtr−1\n \n\tn!\t\n=\t(n − s)!(s − r − 1)![1 − $F$ (x\n \n\n(s)\n \n)]n−s×\n \n\n\n[F (x(r))]r−1\n \n\n\ncaso x(r) < x(s).\n \n×[F (x(s)) − $F$ (x(r))]s−r−1f (x(s))f (x(r))\n \n,\n(r − 1)!\n\n \n\nDe modo semelhante, podemos mostrar que a função de densidade conjunta de X(k1), \\cdots , X(km) se 1 ≤ k1 <\n \n\n \nk2 < \\cdots < km ≤ n, 1 ≤ m ≤ n, é dada por\nfk1k2···km (x(k1), x(k2), \\cdots , x(km)) =\n \n\n\n(k1\n \n\n\n— 1)!(k2\n \n\n\n— k1\n \n\nn!\n×\n— 1)! × \\cdots (n − km)!\n \n×Fk1−1(x(k ))f (x(k ))[F (x(k )) − $F$ (x(k ))]k2−k1−1f (x(k )) × \\cdots ×\n×[F (x(k\t)) − $F$ (x(k\t))]km−1−km−2−1f (x(k\t))[1 − $F$ (x(k ))]n−km $F$ (x(k )),\ncaso x(k1) < x(k2) < \\cdots < x(km) e zero noutras situações.\n\n**Exemplo 2.9** (Continuação do Exemplo 2.7)\nSabemos que as variáveis aleatórias $X_1, X_2, \\cdots , X_n$ são independentes e tem como função de densidade comum\nf (x) =\t1,\tse\t0 < x < 1  ·\n0,\tcaso contrário\nEntão, a função de densidade conjunta de X(r) e X(s) é dada por\n \n\n\n\nfrs\n \n\n\n\n(x(r)\n \n\n\n, x(s)) =\n \n n!\n \n\nxr−1(x(r) − x(s))s−r−1(1 − x(s))n−s\n(r − 1)!(s − r − 1)!(n − s)!\n \n\n,  se\tx\n \n\n\n\n(r)\n \n\n\n< x(s)\n·\n \n\n\nonde 1 ≤ r < s ≤ n.\n \n0,\tcaso contrário\n \nUma situação mais complexa é trabalharmos com funções de estatísticas de ordem. Não temos um resultado simples para o caso de qualquer funções destas estatísticas. Mas, no exemplo a seguir, podemos encontrar um resultado interessante para o comportamento da diferença de estatísticas de ordem.\n\n**Exemplo 2.10**\nSejam X_{(1)}, X_{(2)}, X_{(3)} as estatísticas de ordem das variáveis aleatórias independentes e igualmente distribuídas\nX1, X2, X3 com função de densidade comum\n{ βe−xβ,\tse\tx ≥ 0\nsendo β > 0. Sejam Y1 = X_{(3)} − X_{(2)} e Y2 = X_{(2)}. Mostraremos que Y1 e Y2 são independentes. Para isso primeiro observemos que a função de densidade conjunta de X_{(2)} e X_{(3)} é dada por\n\n \nf23(x, y) =\n \n1!0!0!\n \n·\n0,\tcaso contrário\n \nA função de densidade conjunta de (Y1, Y2) é então\nf (y1, y2) = 3!β2(1 − e−y2β )e−y2βe−(y1+y2)β =\n[3!βe−2y2β (1\te−y2β )][βe−y1β ],  se\t0 < y  < +\t, 0 < y  < +\n=\t·\n0,\tcaso contrário\nDo qual segue que Y1 e Y2 são independentes.\nDuas estatísticas de ordem importantes são o máximo e mínimo. Nesses casos é possível encontrar, de maneira\n \n\nanalítica, expressões para a função de distribuição.\tVejamos no teorema a seguir as expressões da função de distribuição das estatísticas de ordem X_{(1)} e X_{(n)}.\n\n\nDemonstração : Exercício.\n\nAcerca da função de distribuição de qualquer estatística de ordem temos o resultado a seguir.\n\n\nDemonstração : O evento {X(k) ≤ x} ocorre se, e somente se, pelo menos k dos $X_1, X_2, \\cdots , X_n$ são menores ou iguais a x, por isso o somatório começa em k.\n\nNos dois teoremas seguintes relacionamos a distribuição condicional de estatísticas de ordem, condicionadas em outra estatística de ordem, com a distribuição de estatísticas de ordem de uma população cuja distribuição é uma forma truncada da função de distribuição da população original $F$ .\n\n\nDemonstração : A densidade condicional de X(j) dado que X_{(i)} = xi calcula-se dividindo a densidade conjunta de\nX_{(i)} e X(j), dada em (2.7), pela densidade marginal de X_{(i)}, esta obtida no Teorema 2.4. Temos então que, quando\n \n\ni < j ≤ n e xi ≤ xj < ∞,\n\n \nf (xj|X_{(i)}\n \n= x ) =\tfij(xi, xj) i\t\tfi(xi)\n\t(n − i)!\t [ $F$ (xj) − $F$ (xi)]j−i−1\n \n\n[ 1 − $F$ (xj)]n−j   $F$ (xj) \n \n(j − i − 1)!(n − j)!\t1 − $F$ (xi)\n \n1 − $F$ (xi)\t1 − $F$ (xi)\n \nO resultado segue observando que $F$ (xj ) − $F$ (xi) e  $F$ (xj )  são, respectivamente, as funções de distribuição e de\n1 − $F$ (xi)\t1 − $F$ (xi)\ndensidade truncando à esquerda em xi a distribuição $F$ .\n\nNa demonstração do teorema anterior utiliza-se o conceito de distribuição truncada, o que é isso? define-se a seguir este conceito e incluem-se exemplos explicativos.\n\n\nCaso a variável aleatória X seja discreta com função de probabilidade P , a distribuição truncada de X é dada\npor\n \n\nP (X = x|X ∈ A) =\n \n\nP (X = x, X ∈ A) P (X ∈ A)\n \n= \n\n \nP (X = x)\nP (X = a),  se x ∈ A\na∈A\n0,\tse x ∈/ A\n \nNa situação X do tipo contínua, com função de densidade $F$ , temos que\n \n\n\nP (X ≤ x|X ∈ A) =\n \n\nP (X ≤ x, X ∈ A) =\nP (X ∈ A)\n \n∫(−∞,x]∩A\n \n\nf (y) dy\n \n\n\n·\t(2.8)\n \n\n \nConcluindo então que, a função de densidade da distribuição truncada é dada por\n \n\n\nh(x) =\n \n ∫\n \nf (x)\nf (y) dy\n \n\n, caso x ∈ A,\n \n\n\n·\t(2.9)\n \n 0 A\n \ncaso x ∈/ A\n \nExemplo 2.11\nSuponhamos X uma variável aleatória com distribuição normal padrão e A = (\t, 0]. Então, P (X\tA) = 1/2, dado que X é simétrica e contínua. Para a densidade truncada temos que\n{ 2f (x),  caso − ∞ < x ≤ 0,\n\n \n\nO truncamento é especialmente importante nos casos em que a distribuição $F$ em questão não tem média finita. Se X é uma variável aleatória, truncamos X em algum c > 0, onde c é finito, substituindo X por Xc = X caso\n|X|\tc e zero caso |X| > c. Então Xc é X truncada em c e todos os momentos de Xc existem e são finitos. Na verdade, sempre podemos selecionar c suficientemente grande para que\nP (X ̸= Xc) = P (|X| > c),\n\n \nseja arbitrariamente pequena.\nA distribuição de Xc é então dada por\n\nP (Xc ≤ x) = P (X ≤ x| |X| ≤ c) = no caso contínuo com função de densidade $F$ e é dada por\n \n\n\n\nf (y) dy\n (−∞,x]∩[−c,+c]\t , P (|X| ≤ c)\n \n\n\nP (Xc = x) =\n \n\n\n \nP (X = x)\n\n \nP (X = a)\na∈[−c,+c]\n \n\n,  se x ∈ [−c, +c]\n,\n \n0,\tse x ∈/ [−c, +c]\nno caso discreto. Observemos que, para algum α > 0,\nE(|Xc|)α  ≤ cα·\n\n**Exemplo 2.12**\nCaso X\tCauchy(0, 1), sabemos que E(X) não existe. Seja c > 0 um número finito, truncando X em c\ndefinimos\n \n\n\nEntão\n \nXc =\n \nX,  caso\t|X|\tc,\n·\n0,\tcaso\t|X| > c\n \n1 ∫ +c   1\t\n\n \n2\t−1\n \nSendo que a função de densidade truncada é dada por\n \n 1\t1\n \n1\n, caso x ∈ [−c, +c],\n \nh(x) =\n\nDesta expressão obtemos que\n \n2 1 + x2 tan−1(c)\t·\n 0,\tcaso x ∈/ [−c, +c]\n\n\t1\t ∫ +c   x\t\n \n\n\ne também que\n \nE(Xc) =\n \n2 tan−1(c)\n \n−c  1 + x2\n \ndx = 0,\n \nE(Xc)2 =\n \n\t1\t ∫ +c\n \nx2\ndx =\n \n \n\tc\t\n— 1·\n \n2 tan−1(c)\n \n−c  1 + x2\n \ntan−1(c)\n \n\nPor último, temos o seguinte resultado estabelecendo novamente relação entre estatísticas de ordem e distri- buições truncadas.\n \n\n \nDemonstração : A densidade condicional de X_{(i)} dado que X(j) = xj calcula-se dividindo a densidade conjunta de X_{(i)} e X(j), dada em (2.7), pela densidade marginal de X(j), esta obtida no Teorema 2.4. Temos então que, quando i < j ≤ n e xi ≤ xj < ∞,\n\t(j − i)!\t [ $F$ (xi) ]i−1\t[ $F$ (xj) − $F$ (xi)]j−i−1  $F$ (xi)\nO resultado segue observando que $F$ (xi)/F (xj) e $F$ (xi)/F (xj) são, respectivamente, as funções de distribuição e de densidade truncando à direita em xj a distribuição $F$ .\n\n### 2.2.2\tQuantis\nLembremos que a função de distribuição $F$ é contínua à direita e que o número de descontinuidades é, no máximo, enumerável. Estas são propriedades importantes que farão toda diferença na definição dos quantis amostrais, por isso, demonstraremos as propriedades mencionadas da função de distribuição.\nA prova de que $F$ é contínua à direita advém do seguinte fato\nF (x + hn) − $F$ (x) = P (x < X ≤ x + hn),\nonde {hn} é uma sequência de números reais estritamente positivos tais que limn→∞ hn = 0. Segue, da propriedade de continuidade da função de probabilidade,1 que\nlim [F (x + hn)\tF (x)] = 0,\nn→∞\ne, portanto, $F$ é contínua à direita.\nDefinamos por D o conjunto dos pontos de descontinuidade de $F$ e seja\nD  = {x ∈ D : P (X = x) ≥ 1 } ,\nonde n é um inteiro positivo. Dado que $F$ (\t)\tF (\t) = 1, o número de elementos em Dn não pode exceder n. Logicamente\n∞\nD =\tDn\nn=1\ne, então, o conjunto D é enumerável. Demonstrando-se assim a segunda propriedade importante mencionada da função de distribuição. Definimos a seguir o conceito de quantil teórico e depois mostramos a forma de cálculo.\n\n \n \n1A função de distribuição é contínua, devido a que\nP\tlim\nn→∞\n \n\nAn  = lim\nn→∞\n \n\nP (An),\n \nse o limite limn→∞ An existir.\n \n\n \nA função $F$ −1(t), 0 < t < 1 foi definida em (1.29) e é chamada de função inversa de $F$ . O seguinte teorema fornece-nos propriedades úteis. Fica claro que as propriedades apresentadas no seguinte teorema nos permitirão o cálculo dos quantis e é por isso que dedicamos atenção a este conceito.\n\n\nDemonstração : Exercício.\n\n**Exemplo 2.13**\n\nSeja X\tExponencial(\\Theta). Sabemos que a função de distribuição neste caso é $F$ (x) = 1\te−x/\\Theta. Resulta que a expressão de qualquer um dos quantis é possível de ser encontrada de maneira exata via\n\n \n\n\n\n\nobtendo-se que\n \nF (ξp)  =  p\n1 − e−ξp/\\Theta\t=  p\n1 − p  =  e−ξp/\\Theta,\n\nξp = −\\Theta ln(1 − p)\n \né a expressão teórica do p-ésimo quantil. Devemos mencionar que a expressão dos quantis está bem definida, no sentido de que o resultado é sempre positivo. Isto é importante porque devemos lembrar que a distribuição exponencial está definida somente para valores positivos, então o quantil teórico deve ser positivo, já que é um dos possíveis valores da variável.\nObservemos que caso $F$ seja contínua e estritamente crescente, $F$ −1 é definida como\nF −1(y) = x\tquando\ty = $F$ (x)·\nAinda podemos observar que, se x0 é um ponto de descontinuidade de $F$ e supondo que\nF (x−) < y < $F$ (x0) = $F$ (x+)\n0\t0\n \n\nvemos que, embora não exista x tal que y = $F$ (x), $F$ −1(y) é definido como igual a x0. A situação na qual $F$ não é estritamente crescente, por exemplo, caso da variável aleatória ser discreta, podemos escrever\n\n \nF (x) =\n \n= y,  caso  a ≤ x ≤ b  ·\n > y,  caso  x > b\n \nEntão, qualquer valor a\tx\tb poderia ser escolhido como x = $F$ −1(y). A convenção é que, neste caso, definimos\nF −1(y) = a. Em particular\nξ1/2 = $F$ −1(1/2),\t(2.10)\né chamada de mediana de $F$ . Observemos que ξp satisfaz a desigualdade\nF (ξp− ) ≤ p ≤ $F$ (ξp)·\nExemplo 2.14 (Continuação do Exemplo 2.13)\nCaso p = 1/2, a mediana amostral será ξ1/2 = −\\Theta ln(1/2) = 0.6931472\\Theta.\n\n## Momentos amostrais\nNesta seção vamos estudar algumas estatísticas amostrais comumente utilizadas e suas distribuições.\n\nObservemos que nFn(x) é o número de Xk (1 ≤ k ≤ n) menores ou iguais a x. Se X_{(1)}, X_{(2)}, \\cdots , X_{(n)} são as estatísticas de ordem de $X_1, X_2, \\cdots , X_n$ então claramente\n\n \nFn(x) = \n\n \nk\n,  se  X\nn\n \n\n\n(k)\n \n\n≤ x < X\n \n\n\n(k+1)\n \n\n,\t(k = 1, 2, \\cdots , n − 1)\n \n\n·\t(2.11)\n \n 1,\tse x ≥ X_{(n)}\n\n \n\n\n \ncom esperança e variância\n \n\n\n\nVar[Fb\n \nE[Fn(x)] = $F$ (x)\t(2.13)\n\nF (x)[1 − $F$ (x)]\n \n\n\nDemonstração : Dado que δ(x − Xi), i = 1, 2, \\cdots , n são variáveis aleatórias independentes igualmente distribuídas cada uma com função de probabilidade\nP [δ(x − Xi) = 1] = P (x − Xi ≥ 0) = $F$ (x)\ne\nP [δ(x − Xi) = 0] = 1 − $F$ (x),\nsua soma nF ∗(x) é uma variável aleatória com distribuição Binomial(n, p), onde p = $F$ (x). As relações (2.12), (2.13) e (2.14) seguem-se imediatamente.\n\n\n\n \nDemonstração :\n \nE´ uma consequência da Lei dos Grandes Números .\n \n\n\n\n \nCorolário 2.12\n\n\n\nonde Z ∼ N (0, 1).\n \n\n√n[F (x)\tF (x)]\n√F (x)[1 − $F$ (x)] −→ Z\tquando n → ∞,\n \n\n \nDemonstração :\n \nE´ consequência do Teorema do Limite Central.\n \n\n**Exemplo 2.15**\n\nVamos apresentar o conceito de função distribuição empírica no caso de termos uma amostra aleatória da distribuição N (0, 1). A lista de comandos na linguagem de programação R está disponível abaixo. O primeiro comando destina-se a fixar o gerador de amostras e, assim, em qualquer momento podemos obter a mesma amostra aleatória.\nNa Figura 2.2 mostramos a forma da distribuição empírica, de três formas diferentes, para uma amostra de tamanho 12. A representação da função de distribuição empírica é realizada permitindo escolher qual utilizar segundo o agrado.\n \n\n\n\n\nlwd = 2\n\n\n\n\n\n\n\n\n \n\n−1.5\t−1.0\t−0.5\t0.0\t0.5\n\nx\n \n\n−1.5\t−1.0\t−0.5\t0.0\t0.5\n\nx\n \n\n−1.5\t−1.0\t−0.5\t0.0\t0.5\n\nx\n \n\nFigura 2.2: Representação da função de distribuição amostral ou empírica, de três formas diferentes, para uma amostra normal padrão de tamanho 12.\n\nA linhas de comando a seguir permitiram-nos gerar os gráficos na Figura 2.2: construímos :\nset.seed(5739); x=rnorm(12); Fn=ecdf(x) par(mar=c(5,4,3,1), cex=0.9)\nplot(Fn,  main=\"\")\nplot(Fn, verticals = TRUE, do.points = FALSE, main=\"\") plot(Fn , lwd = 2, main=\"\"); mtext(\"lwd = 2\", adj = 1) xx=unique(sort(c(seq(-3, 2, length = 201), knots(Fn12)))) lines(xx, Fn(xx), col = \"blue\")\nabline(v = knots(Fn), lty = 2, col = \"gray70\")\nObservemos que a convergência da distribuição empírica, segundo o Teorema 2.10, é para cada valor de x. E´\npossível fazer uma demonstração da convergência em probabilidade simultaneamente para todos os x, ou seja, da convergência uniforme.\n\nDemonstração : Seja ϵ > 0. Escolhemos um inteiro k > 1/ϵ e números\n−∞ = x0 < x1 ≤ x2 ≤ \\cdots ≤ xk−1 < xk = ∞,\ntais que $F$ (x−) ≤ j/k ≤ $F$ (xj), para j = 1, \\cdots , k − 1. Observe que se xj−1 < xj, então\n \n\n\nPela Lei dos Grandes Números\n \nF (x−) − $F$ (xj−1) ≤ ϵ·\n \nq.c.\nFn(xj) −→ $F$ (xj)\ne\n—\tq.c.\t−\n \n\npara j = 1, \\cdots , k − 1. Consequentemente,\n \nFbn(xj ) −→ $F$ (xj ),\n \n∗\t−\tq.c.\n∆n = max{|Fbn(xj) − $F$ (xj)|, |Fn (xj ) − $F$ (xj)|, j = 1, \\cdots , k − 1} −→ 0·\n \n\nSeja x arbitrário e encontremos j tal que xj−1 < x ≤ xj. Então,\nFbn(x) − $F$ (x) ≤ Fbn(x−) − $F$ (xj−1) ≤ Fbn(x−) − $F$ (x−) + ϵ,\n\n \ne\n\nIsto implica que\n \nFbn(x) − $F$ (x) ≥ Fbn(xj−1) − $F$ (x−) ≥ Fbn(xj−1) − $F$ (xj−1) − ϵ·\n \nq.c.\nsup |Fn(x)\tF (x)|\t∆n + ϵ\tϵ·\nx\nComo isso vale para todo ϵ > 0, o teorema segue.\n\nAgora, dado que $F$ ∗(x) tem pontos de salto em Xi, i = 1, 2, \\cdots , n é claro que existem todos os momentos de $F$ ∗(x). Vamos considerar alguns valores típicos da função de distribuição $F$ , chamados de estatísticas amostrais. Escrevamos\na  = 1 ∑ Xk,\t(2.15)\n\npara os momentos de ordem k ao redor do 0 (zero). Aqui ak, serão chamados de momentos amostrais de ordem k. Com esta notação\n \n\n\nO momento amostral central é definido por\n \nn\na1 =\tXi\nn\ni=1\n \n= X·\n \nb  = 1 ∑(X − a )k = 1 ∑(X\n \n— X) ·\t(2.16)\n \nLogicamente,\n \nk\tn\ti\t1\ni=1\n \nn\ti\ni=1\n \nb = 0\te\tb\n \n= (n − 1 ) S2·\n \nComo mencionado anteriormente, não chamamos b2 a variaˆncia amostral. S2\n \nserá chamada como a variaˆncia\n \namostral por razões que se tornarão claras posteriormente. Temos que\nb2 = a2 − a2·\nPara a função geradora de momentos de Fn podemos afirmar que\nn\n \nMFbn\n \n(t) = 1\tetXi ·\nn\n \ni=1\n\nDefinições similares são realizadas para momentos amostrais de distribuições multivariadas. Por exemplo, se (X1, Y2), (X2, Y2), \\cdots , (Xn, Yn) é uma amostra de uma distribuição bivariada, podemos escrever\nn\tn\nX = 1 ∑ X ,\tY = 1 ∑ Y\n\npara as duas médias amostrais e para os momentos de segunda ordem centrais escrevemos\n\n \n\nb20\n \nn\n=\t(Xi\nn\ni=1\n \n\n \n— X) ,\tb02\n \nn\n=\t(Yi\nn\ni=1\n \n\n \n— Y ) ,\n \n\n \n\n\n\nMais uma vez, escrevemos\n \n\nb11\n\n\n\nn\n \nn\n=\t(Xi\nn\ni=1\n \n\n \n—\tX)(Yi\n \n\n \n—\tY )·\n\nn\n \nS2 =   1\t ∑(X\n\n \n—\tX)2,\tS2 =   1\t ∑(Y\n\n \n \n\n \n—\tY ) ,\t(2.17)\n \npara as duas variaˆncias amostrais e para a covariância amostral utilizamos\nn\n \nS11\n \n=   1\t\t(X\nn − 1 i=1\n \n\n \n—\tX)(Yi\n \n\n \n—\tY )·\t(2.18)\n \nEm particular, o coeficiente de correlação amostral é definido por\nb11\tS11\n \nR =\n20\n \nb02\n \n=\t·\nS1S2\n \nPode ser demonstrado que |R| ≤ 1 e que os valores extremos ±1 ocorrem somente quando todos os pontos amostrais (X1, Y2), (X2, Y2), \\cdots , (Xn, Yn) estão alinhados.\nCorrespondendo a uma amostra $X_1, X_2, \\cdots , X_n$ de observações em $F$ , p-ésimo quantil amostral é definido como o p-ésimo quantil da função de distribuição amostral, ou seja, como $F$ −1.\nOs quatis amostrais são definidos de maneira similar. Então, se 0 < p < 1, o quantil amostral de ordem p,\n\n \nr =\t\t[np]\tse n é um número par [np] + 1 se n é um número ímpar\n \n·\t(2.19)\n \nComo usual, [x] denota o maior inteiro  x. Observe que, se [n] for par, podemos escolher qualquer valor entre X([np]) e X([np]+1) como o p-ésimo quantil amostral. Então, se p = 1 e n par podemos escolher qualquer valor entre X(n/2) e X(n/2+1), os dois valores do meio, como a mediana amostral. Habitualmente é escolhido o ponto médio. Assim, a mediana amostral é definida como\n \nξb1/2 = \n \nX((n+1)/2)\tse n é ímpar\nX(n/2) + X((n/2)+1)\tse n é par\n \n\n·\t(2.20)\n \n\nObserve que\n \n2\n[n + 1] = (n + 1 )\n \n2\t2\nse n é ímpar.\nConsideraremos agora os momentos de características amostrais. Nos seguintes desenvolvimentos denotaremos E(Xk) = mk e E[(X µ)k] = µk como os momentos populacionais e os momentos populacionais centrais de k-ésima ordem, respectivamente. Nas situações onde utilizamos mk ou µk assumiremos que estes existem. Também, σ2 representará a variância populacional.\n\n \n\n \nDemonstração : Para provar (2.23) observemos que\n \n(∑n\n \nXi\t= ∑\n \nX3 + 3 ∑ ∑\n \nX2Xk + ∑ ∑ ∑ XiXjXk,\n \ni=1\n \ni=1\n \ni=1 j=1\nj̸=k\n \ni=1 j=1 k=1 i̸=j, i̸=k\nj̸=k\n \ndesta expressão obtemos o resultado em (2.23). Similarmente\n \n\n(∑n\t)4\n \n( n\t)  n\tn\tn\n \n\n\n∑ ∑ ∑\n \nXi\t=\n \n∑ Xi\t∑ X3 + 3 ∑ ∑ X2Xj +\n \ni  j  k\n \ni=1\n \ni=1\n\nn\n \n i=1\n \ni\ni=1 j=1\ni̸=j\nn\tn\n \ni=1 j=1 k=1 i̸=j, i̸=k\nj̸=k\n \n=  ∑ X4 + 4 ∑ ∑ XiX3 + 3 ∑ ∑ X2X2\n \ni\ni=1\n \ni=1 j=1\ni̸=j\n \nj\ti\tj\ni=1 j=1\ni̸=j\n \nn\tn\tn\tn\tn\tn\tn\n=\t+6 ∑ ∑ ∑ X2XjXk + ∑ ∑ ∑ ∑ XiXjXkXl·\n \ni=1 j=1 k=1 i̸=j, i̸=k\nj̸=k\n \ni=1 j=1 k=1 l=1 i̸=j, i̸=k, i̸=l\nj̸=k, j̸=l\nk̸=l\n \nUm detalhe importante é que os momentos centrais podem ser calculados a partir dos momentos, por exemplo,\nµ2\t=\tE[(X − µ)2] = m2 − µ2,\nµ3\t=\tE[(X − µ)3] = m3 − 3µm2 + 2µ3\n\ne assim por diante. Sabemos agora como calcular os momentos, até quarta ordem, de X. Vejamos a seguir como calcular os momentos centrais.\n\n \n\n \nDemonstração : Temos que\nµ (X) = E(X − µ)3 =\tE {∑n\n \n\n(X − µ)3} =\t∑n\n \n\n\nE(X\n \n\n\n3\tµ3\n— µ)  =\t·\n \n3\n\nNo caso do quarto momento central\n \nn3\ti=1\ti\n\n\n\n1\n \nn3\n\n\n{∑n\n \ni=1\ti\tn2\n\n\n}\n \n\n\nda qual obtemos que\n \n\n \t \nµ (X) = E(X\tµ)4 =\tE\nn4\n \n\ni=1\n \n(Xi − µ)4\t,\n \n1\nµ4(X) =\n \n\nE(Xi − µ)4 +\n \n_{(4)} 1 + ∑ ∑\n \n\nE{(Xi − µ)2(Xj − µ)2}·\n \nn4\ni=1\n \n2  n4\n \ni=1 j=1\ni̸=j\n \nDesenvolvendo adequadamente chegamos ao resultado em (2.26).\n\n \n**Exemplo 2.16**\n \n\n, Xn uma amostra aleatória da distribuição Gamma(α, β). Sabemos da Seção 1.2 que\nE(X) = αβ,\tVar(X) = αβ2\n \n\nmk = βk(α + k − 1)(α + k − 2) \\cdots α,\tk ≥ 1·\nαβ2\nE(X) = αβ,\tVar(X) =\nn\n1\t1\nµ (X) =\tµ  =  (6α3β3 + 3α2β3 + 2αβ3)·\n3\tn2  3\tn\n\nAté o momento estudamos como calcular os momentos da média amostral. Mais complexo é obter expressões para os momentos da variância amostral S2. O teorema a seguir dedica-se ao objetivo de encontrarmos expressões, até segunda ordem, dos momentos amostrais centrais. Como consequência deste resultado obtemos os momentos da variaˆncia amostral.\n\n \n\n \nDemonstração : Temos que\n \n\n\nn\nE(b2)  =\tE\tn\ni=1\n \n\n\n\n\nX2\nn2\n \nn\t2\nXi\ni=1\n \n=  m2 −\n \n1\t n\nE \n \nX2 + ∑\n \n∑ X2Xj\n \n\n\n\n\nAgora\n \n\n=  m2\n \n\n 1 \n— n2 [nm2\n \n+ n(n − 1)µ2] = ( n − 1 ) (m\n \n\n— µ )·\n \nn2b2 =\n \nn\n\ni=1\n \n2\n\n(Xi − µ)2 − n(X − µ)2\t·\n \nEscrevendo Yi = Xi − µ, vemos que E(Yi) = 0, Var(Yi) = σ2 e E(Y 4) = µ4. Temos então que\n \n\nn2 E(b2)  =\tE\n \nn\ni=1 n\n \n2\n\nY 2 − nY 2\n\nn\tn\n \n\n\n\n n\tn\tn\t\n \n=\tE ∑ Y 4 + ∑ ∑ Y 2Y 2 − 2 ∑ ∑ Y 2Y 2 + ∑ Y 4\n \n+ 1 3 ∑ ∑\n \n\nY 2Y 2 + ∑\n \nY 4 ·\n \n\n\nSegue então que\n \n \nn\ti\tj\ni=1 j=1\ni̸=j\n \ni=1\n \ni \n \n2\t 1 \nn2 E(b2)  =  nµ + n(n − 1)σ2 −\t[n(n − 1)σ4 + nµ ] +\t[3n(n − 1)σ4 + nµ ]\n \n=  (n − 2 + 1 ) µ + (n − 2 + 3 ) (n − 1)µ2 ·\t(µ\n \n\n= σ2)\n \n\nPortanto\n \nn\t4\tn\t2\t2\n \nVar(b2)  =\tE(b2) − [ E(b2)]2\n \n=  (n − 2 +\n \n1 ) µ4\n \n+ (n − 1) (n − 2 + 3\tµ\n2\n—\n \n( n − 1 )2\n \n \n=  (n − 2 +\n \n1 ) µ4\n\n \t \n \nµ2\n+ (n − 1)(3 − n)\t,\n \ncomo afirmado. As relações (2.29) e (2.30) podem ser provadas de forma semelhante.\n\n\nEste é justamente o motivo pelo qual chamamos S2 e não b2 de variância amostral.\n \n\n \n**Exemplo 2.17** (Continuação do Exemplo 2.16)\n\ninteir Nesta situação, σ2 = αβ2, µ2 = σ2 e µ4 = m4 − 4m3µ + 6m2µ2 − 3µ4. Obtemos que\nE(S2) = αβ2\ne\nVar)(S2) = µ4 +  3 − n  α2β4·\nn\tn(n − 1)\n\nO seguinte resultado fornece uma justificativa para a nossa definição de covariaˆncia amostral.\n\nDemonstração : Do Corolário 2.17 sabemos que E(S2) = σ2 e E(S2) = σ2. Para provar que E(S11) = ρσ1σ2\n1\t1\t2\t2\nobservemos que Xi é independente de Xj, (i ̸= j) e de Yj, (i ̸= j). Temos que\n \n\n\n\nAgora\n \n\n(n − 1) E(S11) = E\n\n\nE{(Xi − X)(Yi − Y )} =\n \nn i=1\n \n(Xi − X)(Yi − Y )] ·\n \n(\t∑n\tYj\n\n \n\n \n∑n\tYj\n\n \n\n \n∑n\tXj ∑n\n\n \n\n \nYj )\n \n\n\n\n\n\n\n\n\ne segue que\n \n1\n= E(XY ) − n [ E(XY ) + (n − 1) E(X) E(Y )]\n1\n− n [ E(XY ) + (n − 1) E(X) E(Y )]\n1\n− n2 [n E(XY ) + n(n − 1) E(X) E(Y )]\n= n − 1 [ E(XY )\tE(X) E(Y )]\nn\n \n(n − 1) E(S11) = n (\t) [ E(XY ) − E(X) E(Y )],\nn − 1\n \n\n \nisto é\n \n\nE(S11) = E(XY ) − E(X) E(Y ) = Cov(X, Y ) = ρσ1σ2·\n \n\nA seguir, voltamos nossa atenção para as distribuições das características da amostra. Existem várias possi- bilidades. Se for necessária a distribuição exata o método de transformação de variáveis pode ser utilizado. As vezes, a técnica da função geradora de momentos pode ser aplicada. Assim, se $X_1, X_2, \\cdots , X_n$ é uma amostra aleatória de uma população com distribuição para a qual existe a função geradora de momentos, a função geradora de momentos da média amostral X é dada por\nn\nM (t) =\tE(etXi/n) = [MX(t/n)]n ,\t(2.32)\ni=1\nonde MX é a função geradora de momentos da distribuição populacional. Se MX (t) tiver alguma forma conhecida seria possível escrever a função de probabilidade ou de densidade de X. Embora este método tem a desvantagem\nóbvia que se aplica apenas à distribuições para as quais existem todos os momentos, veremos sua efetividade na situação importante de amostras da distribuição normal.\n\n**Exemplo 2.18**\n\nSeja $X_1, X_2, \\cdots , X_n$ uma amostra aleatória de tamanho n da distribuição Gama(α, 1). Nesta situação podemos encontrar a função de densidade de X. Temos que\n\n \nMX (t) = [MX\n \n(t/n)]n =\t1\t,\tt\n(1 − t/n)αn\tn\n \n< 1,\n \n\n \nda qual obtemos que X ∼ Gama(nα, 1/n).\n\n**Exemplo 2.19**\n\nSeja $X_1, X_2, \\cdots , X_n$ uma amostra aleatória da distribuição Uniforme no intervalo (0, 1). Considere a média geométrica\n \n\nYn =\n \nn\n\n\ni=1\n \n1/n\nXi\t·\n \nSabemos que log(Yn) = (1/n) ∑n\tlog(Xi) e, desta forma, log(Yn) é a média amostral de log(X1), \\cdots , log(Xn).\nA função de densidade comum de log(X1), \\cdots , log(Xn) é\n\nex,  se x < 0\nf (x) =\t,\n0,\tcaso contrário\n\nque é a distribuição exponencial negativa com parâmetro β = 1. Vemos que a função geradora de momentos de\nlog(Yn) é dada por\n \n\nMlog(Yn)\n \nn\n(t) =\tE(et log(Xi)/n) =\t,\n(1\tt/n)n\ni=1\n \ne a função de densidade de log(Yn) é dada por\n \n\n\n\nflog(Yn)\n \n(y) = \n \n\nnn\nΓ_{(n)}[−y]\n \n\n\nn−1\n \n\n\neny\n \n\n,  se − ∞ < y < 0  ·\n \n\t0,\tcaso contrário\n \n\nSegue então que Yn tem por função de densidade\n \n\n\n\nfYn\n \n(y) = \n \n\nnn\ny\nΓ_{(n)}\n \n\nn−1\n \n\n[− log(y)]\n \n\nn−1\n \n\n,  se 0 < y < 1\t·\n \n\n\n\nVoltemos ao quantil amostral de ordem p,\n \n0,\tcaso contrário\n\nξbp, o qual sabemos é ou X([np]) ou X([np]+1) dependendo se [np] é\n \num número par ou ímpar, como definido em (2.19). Simplificando, vamos discutir as propriedades de X([np]), onde\np ∈ (0, 1) e n é grande. Isso, por sua vez, nos informará sobre as propriedades de ξp.\nPrimeiro observemos que, se U1, U2, \\cdots , Un é uma amostra aleatória da distribuição U (0, 1) então, pelo Teorema 2.3, temos que\n \n\ndo qual obtemos que\n \nU([np]) ∼ Beta([np], n − [np] + 1),\n\n[np]\n \nE(U([np]))  =\n \n\n \nn + 1\n \nn−→→∞ p,\n \nCov(U\t, U\n \n) =  n [np1](n − [np2] + 1)\n \n−→ p (1 − p )·\n \nUtilizando este resultado e a desigualdade de Chebychev, demonstramos que\nU\t−P→ p·\t(2.33)\n \nIsso gera a questão\n \n\nξbp −→ ξp?\n \nqualquer seja a distribuição da amostra aleatória X1, \\cdots , Xn.\nPara respondermos a pergunta acima vamos utilizar o Lema de Hoeffding, ou seja, para respondermos se o\nquantil amostral de ordem p converge em probabilidade para o quantil teórico correspondente, utilizaremos o seguinte resultado devido a Hoeffding (1963).\n\nDemonstração : Dado que as variáveis aleatórias são limitadas ao intervalo (0, 1), sabemos que\nehX ≤ (1 − X) + Xeh,\nisto deve-se a que a função exponencial ehX é convexa e, portanto, seu gráfico é limitado por cima no intervalo 0 ≤ X ≤ 1 pela linha que conecta as ordenadas X = 0 e X = 1. Então\nE(ehX ) ≤ (1 − E(X)) + E(X)eh·\t(2.35)\n \nSeja Sn = ∑n\n \nXi. Sabemos que\n \n\nP (Sn − E(Sn) ≥ nt) = E(1[Sn− E(Sn)−nt≥0]),\n \n\n \ntambém sabemos que\n \n1[Sn− E(Sn)−nt≥0] ≤ exp (h(Sn − E(Sn) − nt)),\n \nqualquer seja h uma constante positiva arbitrária. Então\nP Sn − E(Sn) ≥ nt  ≤ E eh(Sn− E(Sn)−nt)\t(2.36)\ne como estamos assumindo que as variáveis são independentes, podemos escrever\n \n(  (\t))\n \n∏\t(  (\t))\n\n \nEscrevendo µi = E(Xi) temos, pela expressão em (2.35) que\nE(eh(Xi−µi)) ≤ e−hµi ((1 − µi) + µieh) = ef(h),\t(2.38) onde $F$ (h) = −hµi + ln(1 − µi + µieh). As primeiras duas derivadas são:\n \n′\t\tµi\t\n \n′′\tµie−h(1 − µi)\n \nf (h) = −µi + e−h(1 − µ ) + µ\n \ne\tf (h) =\n[µi\n \n+ e−h(1 − µ )]2 ·\n \n\tµi\t\nNa segunda derivada, escolhendo u = µ + e−h(1 − µ )\n0 < u < 1. Portanto, $F$ ′′(h) ≤ 1 . Pela série de Taylor\n \nvemos que este quociente é da forma u(1 − u), sendo\n \n\n\nEntão, pela expressão em (2.38)\n \nf (h) ≤\n \nf (0) + $F$ ′(0)h +\n \n1 h2 =\n8\n \n1 h2·\n8\n \n\nSubstituindo em (2.36) temos que\n \nE(eh(Xi−µi)) ≤ e 1 h2 ·\n \nP (Sn\n \n—\tE(Sn\n \n) ≥ nt) ≤ e−nht+ 1 nh2 ,\n \ne o mínimo no expoente é atingido quando h = 4t. Então, o mínimo do limite superior da probabilidade é exp(−2nt2).  \n\nDevemos lembrar que esta não é a única maneira de termos uma taxa de convergência para Teorema do Limite Central. Por exemplo, se Y1, Y2, \\cdots , Yn forem variáveis aleatórias independentes e identicamente distribuídas, utilizando o Teorema de Berry-Esseen2, temos que\n \n( ∑n\t∑\n \n)\t( √ Var(Y1))\n \nC E|Y1\n \n—\tE(Y1)|\n \nP\ni=1\n \nXi −\n \n\ni=1\n \nE(Xi) ≥ nt\t≤ Φ\tt\tn\n \n+ √n\n \nVar3/2(Y )\t·\n \n\n \n2\n\nDemonstração : Berry (1941); Esseen (1942).\n\nPode-se consultar o livro de Feller (1971) para uma demonstração moderna.\n \n**Exemplo 2.20**\n\nCaso a amostra aleatória seja Bernoulli(µ), temos que\nn\nXk ∼ Binomial(n, µ)·\ni=1\nEntão, segundo a desigualdade de Hoeffding\n\nP (X − µ ≥ t) ≤ exp(−2nt2)·\nUma vantagem da desigualdade no Lema de Hoeffding é que não assume-se conhecimento da variância e, em geral, o limite da probabilidade é mais acurado do que outras desigualdades. Caso as variáveis aleatórias sejam limitadas como a ≤ Xi ≤ b, com a < b, o limite superior da desigualdade (2.34) seria exp − 2nt2/(b − a)2 .\n\n**Exemplo 2.21**\n\nSejam X1, \\cdots , Xn variáveis aleatórias com distribuição U ( 1, 1). Nesta situação E(X) = 0, a =\t1 e b = 1. A desigualdade de Hoeffding assume a forma\nP (X ≥ t) ≤ exp ( − nt2/2)·\n\n\nDemonstração : Para ϵ > 0 qualquer, podemos escrever\nP (|ξp − ξp| > ϵ) = P (ξp > ξp + ϵ) + P (ξp < ξp − ϵ)·\nPelo Teorema 2.9, podemos escrever\nP (ξbp > ξp + ϵ) =  P (p > Fbn(ξp + ϵ))\n \nn\n=  P\ni=1\n \n1[Xi>ξp+ϵ] > n(1 − p))\n \nn\n=  P\ni=1\n \nVi −\n \n∑i=1\n \nE(Vi) > nδ1),\n \nonde Vi = 1[Xi>ξp+ϵ] e δ1 = $F$ (ξp + ϵ) − p. Da mesma forma,\nP (ξbp < ξp − ϵ) =  P (p > Fbn(ξp − ϵ))\n \nn\n=  P\ni=1\n \n\nWi −\n \n∑i=1\n \nE(Wi) > nδ2),\n \n\nonde Wi = 1[Xi<ξp−ϵ] e δ2 = p − $F$ (ξp − ϵ) − p. Portanto, utilizando o Lema de Hoeffding (Lema 2.20), temos\nP (ξbp > ξp + ϵ) ≤ exp(−2nδ2)\nP (ξbp < ξp − ϵ) ≤ exp(−2nδ2)·\nColocando δϵ = min{δ1, δ2}, a prova está completa.\n\n \nDemonstramos que\n \nlim P (|ξbp − ξp| > ϵ) ≤ lim 2 exp(−2nδ2) = 0,\n \no qual significa que ξp −→ ξp. Em outras palavras, sempre que ξp seja solução única da desigualdade $F$ (ξp ) ≤ p ≤ $F$ (ξp), 0 < p < 1, o quantil amostral converge em probabilidade para o quantil populacional e isto sempre acontece nas distribuições contínuas. Um detalhe importante é que para demonstrarmos a convergência em probabilidade de ξp utilizamos o Lema de Hoeffding e ele depende da existência da esperança.\nO seguinte resultado fornece a distribuição assintótica da r-ésima estatística de ordem amostral de uma po- pulação com uma função de distribuição $F$ , absolutamente contínua, e função de densidade $F$ .\n\nDemonstração : Vamos demonstrar somente para o caso p = 1/2. Observemos que ξ1/2 é mediana única dado que\nf (ξ1/2) > 0. Primeiro, consideremos que n seja ímpar, por exemplo, n = 2m − 1, logo\nP [√n(X(m) − $F$ −1(1/2)) ≤ t] = P (X(m) ≤ t/√n + $F$ −1(1/2))·\n\nSeja Sn o número de X que excedem t/ n + F\t(1/2). Então\n \n\n\nPercebemos que\n \nt\nX(m) ≤ √n + F\n \n(1/2)\tse, e somente se,\tSn ≤ m − 1 =\n \nn − 1 ·\n2\n \nSn ∼ Binomial(n, 1 − $F$ (F −1(1/2) + t/√n))·\nFazendo pn = 1 − $F$ (F −1(1/2) + t/√n), temos que\n \nP [√n(X\n \n\n\n(m)\n \n—\tF −1(1/2)) ≤ t]  =  P (Sn\n \n≤ n − 1 )\n \n(   Sn − npn\t\t 1 (n − 1) − npn )\n \n=  P\n\nUtilizando o Teorema de Berry-Esseen, temos que\n \n√npn(1 − pn) ≤ √npn(1 − p )\n·\nn\n \n{\t(\tn − 1 )\t(  1 (n − 1) − npn )}\n \nlim\tP\nn→∞\n \nSn ≤\t2\n \n— Φ\t√np\n \n(1 − pn)\n \n= 0·\n \n\n \nEscrevendo\n \n\n1 (n − 1) − npn npn(1 − pn)\n\n=\n \n\n √n( 1 − pn) 1/2\n√n( − 1 + $F$ (t/√n + $F$ −1(1/2)))\n \n\n \n\n=  2t\n \n1/2\nF (t/√n + $F$ −1(1/2)) − $F$ (F −1(1/2))\n\n   \n\n \n\n−→ 2tf\n \n\n(F −1(1/2))·\n \nEntão\n \n(  1 (n − 1) − npn )\n \n(\t(  −1\t))\n \nΦ\nnpn\nou\n \n(1 − pn)\n \n≈ Φ 2tf F\n \n(1/2)\n \n√n(X\n \n\n(m) − F\n \n( 1 )) −D→ N (0,\n \n\n4f 2\n \n1\n(F −1(1/2)\n \n)) ·\n \nQuando n é par, digamos n = 2m, ambos P (√n X(m)\tF −1(1/2)\tt) quanto P (√n X(m+1)\tF −1(1/2)\tt)\nconvergem a Φ(2tf (F −1(1/2))).\n\n \nObserve que o quantil amostral de ordem p, assintótica\n \nξbp, como consequência do Teorema 2.23, tem por distribuição\n \nN (ξ , \t1\tp(1 − p)) ,\nonde ξp é o correspondente quantil populacional e $F$ é a função de densidade populacional. Por exemplo, suponha temos uma amostra aleatória da di√stribuição N (µ, σ2) de tamanho n. Seja ξb1/2 a mediana amostral obtida dessa\nb\t(\tπσ2 )\n\nTambém devemos ter em consideração que para demonstrarmos o Teorema 2.23 utilizamos a Teorema de Berry- Esseen, o qual depende da existência dos primeiros dois momentos da variável aleatória. Com isso, caso X\nCauchy(µ, σ), o Teorema 2.23 não se aplica.\n\n## Gráficos descritivos\nVejamos alguns conjuntos de dados disponíveis na linguagem de programação R (R Core Team, 2014), especifi- camente na libraria datasets, que nos permitiram mostrar a utilidade dos momentos amostrais para resumir as informações contidas nos dados.\nPara consultar estes conjuntos de dados basta digitar\nlibrary(help = \"datasets\")\nAlguns dos diversos exemplos disponíveis serão apresentados aqui.\n\n**Exemplo 2.22** (Puromicina)\n\nOs dados sobre a velocidade de uma reação enzimática são obtidos por Treloar (1974) e disponíveis no arquivo de dados Puromycin. O número de contagens por minuto de produto radioativo a partir da reação foi medida como uma função da concentração do substrato em partes por milhão (ppm) e a partir destas contagens a taxa\n \n\ninicial (ou velocidade) da reação foi calculada (contagens/min/min). O experimento foi realizado uma vez com a enzima tratada com puromicina e depois com a enzima não tratada.\nA estrutura destes dados tem 23 linhas e 3 colunas, cada coluna contendo as informações das variáveis:\nconc:\tum vector numérico de concentrações de substrato (ppm);\nrate:\tum vector numérico de taxas de reação instantânea (contagens/min/min); state: um fator com níveis treated (tratada) ou untreated (não tratada).\nPara a leitura e observação dos nomes das variáveis utilizamos os comandos a seguir:\ndata(Puromycin) names(Puromycin)\nUma maneira de obtermos estatísticas descritivas é utilizando as linhas de comando a seguir:\nsummary(rate[state==’treated’])\nMin. 1st Qu.\tMedian\tMean 3rd Qu.\tMax.\n47.0\t104.5\t145.5\t141.6\t193.2\t207.0\ne\nsummary(rate[state==’untreated’])\nMin. 1st Qu.\tMedian\tMean 3rd Qu.\tMax.\n51.0\t85.0\t115.0\t110.7\t137.5\t160.0\npara o caso da variável rate, as concentrações, obtidas as estatísticas descritivas segundo os níveis do fator state, se as concentrações foram ou não tratadas com puromicina.\nNo caso das estatísticas descritivas acerca das concentrações de substrato, variável conc, temos:\nsummary(conc[state==’treated’])\nMin. 1st Qu.\tMedian\tMean 3rd Qu.\tMax.\n0.020\t0.060\t0.165\t0.345\t0.560\t1.100\ne\nsummary(conc[state==’untreated’])\nMin. 1st Qu.\tMedian\tMean 3rd Qu.\tMax. 0.0200\t0.0600\t0.1100\t0.2764\t0.3900\t1.1000\nOs valores mínimos é máximos foram registrados sempre com os nomes de Min. e Max., respectivamente. O\nprimeiro e terceiro quantis ou quantis de 25% e 75% respectivos são registrados com os nomes 1st Qu. e 3rd Qu. e, finalizando, o resumo de informações de estatísticas de posição temos os valores de medianas (Median) e médios (Mean).\n\n**Exemplo 2.23** (Rock )\n\nMedições em 48 amostras de rochas de um reservatório de petróleo estão disponíveis no arquivo de dados rock. Este conjunto de dados contem 48 linhas e 4 colunas numéricas, descritas a seguir:\narea área do espaço de poros, em pixels de 256 por 256; peri perímetro em pixels;\nshape perímetro/sqrt(area)\nperm permeabilidade em mili-Darcies.\nDoze amostras do núcleo de reservatórios de petróleo foram amostrados por 4 seções transversais. Cada amostra foi medida no núcleo para a permeabilidade e cada seção transversal tem uma área total de poros, perímetro total de poros e forma. A fonte destes dados é a BP Research e a análise das imagens foi de Ronit\n Katz, Oxford University.\nNa geologia, a permeabilidade é a medida da capacidade de um material (tipicamente uma rocha) para transmitir fluídos. E´ de grande importância na determinação das características de fluxo dos hidrocarbonetos em reservatórios de petróleo e gás e da água nos aquíferos. A unidade de permeabilidade é o Darcy ou, mais habitualmente, o mili- Darcy ou mD.\n \n### 2.4.1\tGráfico de Boxplot\n\nEm 1977, John Tukey (Tukey, 1977) publicou uma proposta que posteriormente foi reconhecida como sendo um eficiente método para mostrar cinco número que sumarizam qualquer conjunto de dados. O gráfico proposto é chamado de boxplot (também conhecido como box and whisker plot) e resume as seguintes medidas de posição estatísticas: mediana, quantis inferior e superior e os valores mínimos e máximos. Os quantis inferior e superior entendem-se serem os quantis de 25% e 75%, respectivamente.\nNo caso do exemplo 2.22, deixamos a disposição os dados digitando\nattach(Puromycin)\ne com isso podemos mudar o nome dos níveis do fator da forma\nstate=factor(state,labels=c(’Tratada’,’N~ao tratada’))\nEntão, com os comandos a seguir geramos o gráfico de boxplot, tanto para a variável rate quanto para a variável\nconc, estas segundo os níveis do fator state.\npar(mar=c(5,4,3,1))\nboxplot(rate ~ state, col = grey(c(0.4,1)),\nmain=’Taxas de reaç~ao instant^anea’)\n\npara o caso do rate. Observemos que a primeira linha par(mar=c(5,4,3,1)) serve somente para dimensionar a janela gráfica. Para o caso da variável conc utilizamos comandos semelhantes.\npar(mar=c(5,4,3,1))\nboxplot(conc ~ state, col = grey(c(0.4,1)),\nmain=’Concentraç~oes de substrato’)\nO resultado deste trabalho pode ser observado na Figura 2.3.\nInterpretemos o gráfico de boxplot. A caixa (box) propriamente contém a metade 50% dos data. O limite superior da caixa indica o percentil 75% dos dados e o limite inferior da caixa indica o percentil 25%. A distancia entre esses dois quantis é conhecida como inter-quantil. A linha na caixa indica o valor de mediana dos dados. Se a linha mediana dentro da caixa não é equidistante dos extremos, diz-se então que os dados são assimétricos. O boxplot da variável rate (esquerda na Figura 2.3) é um exemplo de dados simétricos já a situação da variável conc (direita na Figura 2.3) é um caso clássico de assimetria dos dados. Os extremos do gráfico indicam os valores mínimo e máximo, a menos que valores outliers3 estejam presentes, nesse caso o gráfico de estende ao máximo de\n1.5 vezes da distância inter-quantil. Os pontos fora do gráfico são então outliers ou suspeitos de serem outliers. Mais elegante seria utilizar a biblioteca de funções ggplot2, para isso, digitamos:\nlibrary(ggplot2)\nPara gerar os gráficos de boxplot respectivos, fazemos:\npar(mar=c(5,4,3,1))\nqplot(state, rate, geom=c(\"boxplot\", \"jitter\"),\nmain=\"Taxas de reaç~ao instant^anea\", xlab=\"\", ylab=\" \")\ne\npar(mar=c(5,4,3,1))\nqplot(state, conc, geom=c(\"boxplot\", \"jitter\"), main=\"Concentraç~oes de substrato\", xlab=\"\", ylab=\" \")\n\n3Em estatística, outlier, valor aberrante ou valor atípico, é uma observação que apresenta um grande afastamento das demais observações em uma amostra. A existência de outliers implica, tipicamente, em prejuízos a interpretação dos resultados dos testes estatísticos aplicados as amostras.\n \n\n\nTaxas de reação instantânea\tConcentrações de substrato\n\nTratada\tNão tratada\tTratada\tNão tratada\n\n\n\nFigura 2.3: Gráfico de boxplot da variável rate à esquerda e da variável conc à direita, segundo os níveis do fator state, se a enzima foi tratada ou não com puromicida. Gráfico gerado utilizando a função R boxplot,\n\n\n\n\nTaxas de reação instantânea\tConcentrações de substrato\n\n200\n\n\n0.9\n\n\n150\n\n0.6\n\n\n\n100\n0.3\n\n\n \n50\n\nTratada\tNão tratada\n \n\n0.0\n \n\n\n\nTratada\tNão tratada\n \n\n\nFigura 2.4: Gráfico de boxplot da variável rate à esquerda e da variável conc à direita, segundo os níveis do fator state, se a enzima foi tratada ou não com puromicida. Gráfico gerado utilizando a função R qplot, opção geom=c(”boxplot”, ”jitter”).\n \n\nobtendo-se assim os gráficos na Figuras 2.4. Além de melhor qualidade gráfica acrescentamos os pontos observados no boxplot, isso permite termos uma ideia também da dispersão dos dados.\nVejamos as vantagens do boxplots. Mostra graficamente a posição central dos dados (mediana) e a tendência. Fornece algum indicativo de simetria ou assimetria dos dados. Ao contrário de muitas outras formas de mostrar os dados, o boxplots mostra os outliers. Utilizando o boxplot para cada variável categórica no mesmo gráfico, pode-se facilmente comparar os dados. Esta é a situação no exemplo na Figura 2.3, podemos observar o comportamento das variáveis rate e conc segundo os níveis do fator state.\nUm detalhe do boxplot é que ele tende a enfatizar as caudas da distribuição, que são os pontos ao extremo nos dados. Também fornece detalhes da distribuição dos dados. Mostrar o histograma (Seção 2.4.2) em conjunto com o boxplot ajuda a entender a distribuição dos dados, constituindo estes dos gráficos ferramentas importantes na análise exploratória.\nLogicamente, o comportamento dos dados dentro da caixa (box), como podemos perceber nas figuras 2.3 e 2.4, permanece um mistério. Isso porque caso estejam os dados bem espalhados ou não, o gráfico boxplot continua mostrando uma caixa. Somente perceberemos algum comportamento diferente se o valor da mediana estiver mais próximo de um dos extremos desta caixa. Para tentar diminuir essa limitação foi sugerido uma melhoria, obtendo-se o chamada boxplot entalhado (notched boxplot).\nCom as linhas de comando a seguir se obtém os gráficos na Figura 2.5.\n\npar(mar=c(5,4,3,1))\nboxplot(rate ~ state, col = grey(c(0.4,1)), notch=TRUE, main=’Taxas de reaç~ao instant^anea’)\n\ne\npar(mar=c(5,4,3,1))\nboxplot(conc ~ state, col = grey(c(0.4,1)), notch=TRUE, main=’Concentraç~oes de substrato’)\n\nObserva-se que a única diferença é a inclusão da opção notch=TRUE, permanecendo todas as outras instruções iguais.\nMais elaborado é o chamado violin plot, mistura de boxplot com estimação de densidade, tema este tratado na Seção 4.3. Este gráfico, introduzido no artigo Hintze & Nelson (1998), sinergicamente combina o gráfico de boxplot e a estimação da densidade, também chamado de histograma suavizado, em uma única tela que revela a estrutura encontrada nos dados.\nCom as linhas de comando a seguir se obtém os gráficos na Figura 2.6.\n\npar(mar=c(5,4,3,1))\nqplot(state, rate, geom = c(\"violin\", \"jitter\"), notch=TRUE, main=\"Taxas de reaç~ao instant^anea\", xlab=\"\", ylab=\" \")\n\ne\npar(mar=c(5,4,3,1))\nqplot(state, conc, geom=c(\"violin\", \"jitter\"), notch=TRUE, main=\"Concentraç~oes de substrato\", xlab=\"\", ylab=\" \")\n\nEste gráfico é similar ao boxplot excepto que mostra também a densidade de probabilidade dos dados. Pode incluir também um marcador para a média dos dados e uma caixa que indica a distância interquartil, como nos gráficos boxplot. O objetivo do gráfico violin plot é o mesmo do que o boxplot original porém, considera de alguma maneira o comportamento dos dados dentro da caixa (box). Assim, percebemos melhor a distribuição dos dados dentro do intervalo interquartil.\n \n\n\nTaxas de reação instantânea\tConcentrações de substrato\n\n\n\n\n\n\n\n\n\n\n\n\nTratada\tNão tratada\tTratada\tNão tratada\n\n\n\nFigura 2.5: Gráfico de boxplot entalhado da variável rate à esquerda e da variável conc à direita, segundo os níveis do fator state, se a enzima foi tratada ou não com puromicida. Gráfico gerado utilizando a função R qplot, opção geom=c(”boxplot”, ”jitter”), notch=TRUE.\n\n\n\nTaxas de reação instantânea\tConcentrações de substrato\n\n200\n\n\n0.9\n\n\n150\n\n0.6\n\n\n\n100\n0.3\n\n\n \n50\n\nTratada\tNão tratada\n \n\n0.0\n \n\n\n\nTratada\tNão tratada\n \n\n\nFigura 2.6: Gráfico de violin plot da variável rate à esquerda e da variável conc à direita, segundo os níveis do fator state, se a enzima foi tratada ou não com puromicida. Gráfico gerado utilizando a função R qplot, opção geom=c(”vioplot”, ”jitter”), notch=TRUE.\n \n### 2.4.2\tHistograma\nUm histograma é uma representação gráfica da função de probabilidades ou da função de densidade de um conjunto de dados independentes e foi introduzido pela primeira vez por Karl Pearson4. A representação mais comum do histograma é um gráfico de barras verticais. A palavra histograma é de origem grega, derivada de duas: histos que pode significar testemunha no sentido de aquilo que se vê, como as barras verticais do histograma, e da também palavra grega gramma que significa desenhar, registrar ou escrever.\nHistograma\tHistograma com a curva norma\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n−2\t−1\t0\t1\t2\nDados simulados\n \n−2\t−1\t0\t1\t2\nDados simulados\n \n\nFigura 2.7: Gráfico de histograma para dados simulados.\n\nPara construir um exemplo controlado do gráfico de histograma, simulamos uma amostra de tamanho 150 da distribuição normal padrão, com o comando\nx=rnorm(150)\ne, depois, construímos um gráfico colorido com as linhas de comando\npar(mar=c(5,4,2,1))\nhist(x, breaks=12, col=\"red\", xlab=\"Dados simulados\", ylab=’Frequ^encia’, main=\"Histograma\")\nbox()\nPosteriormente, acrescentamos a este gráfico uma linha com a densidade normal\npar(mar=c(5,4,2,1))\nh=hist(x, breaks=10, col=\"red\", xlab=\"Dados simulados\", ylab=’Frequ^encia’, main=\"Histograma com a curva normal\")\nxfit=seq(min(x),max(x),length=40) yfit=dnorm(xfit,mean=mean(x),sd=sd(x)) yfit=yfit*diff(h$mids[1:2])*length(x) lines(xfit, yfit, col=\"blue\", lwd=2) box()\n\n4Pearson, K. (1895).  Contributions to the Mathematical Theory of Evolution.  II. Skew Variation in Homogeneous Material.\nPhilosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 186: 343-414.\n \n\nDesta forma geramos os gráficos na Figura 2.7. A ideia é mostrar que o histograma assemelha-se ao gráfico da densidade normal, a densidade dos dados.\n\n\nHistograma c2(6)\tHistograma c2(6)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n2\t4\t6\t8\t10\t12\t14\n14 intervalos\n \n2\t4\t6\t8\t10\t12\t14\n26 intervalos\n \n\nFigura 2.8: Histogramas da distribuição χ2 com 6 graus de liberdade. Número de intervalos 14 e 26, respectivamente.\n\nO histograma é um gráfico composto por retângulos justapostos em que a base de cada um deles corresponde ao intervalo de classe e a sua altura à respectiva frequência. A construção de histogramas tem caráter preliminar em qualquer estudo e é um importante indicador da distribuição de dados. Pode indicar se uma distribuição aproxima-se de uma densidade normal como pode indicar mistura de densidades, quando os dados apresentam várias modas.\nOs histogramas podem ser um mau método para determinar a forma de uma distribuição porque são fortemente influenciados pelo número de intervalos utilizados. Por exemplo, decidimos gerar 50 amostras da densidade χ2(6), da forma\nset.seed(5678) z=rchisq(50, df=6)\nOs gráficos de histogramas correspondentes com 14 e 26 intervalos são apresentados na Figura 2.8 e foram gerados com as linhas de comando\n \n\npar(mar=c(5,4,2,1))\nhist(z, breaks=14, col=\"blue\", main=expression(paste(’Histograma ’, chi^2,’(6)’)), ylab=’Frequ^encia’, xlab=’14 intervalos’)\nbox()\n\ne\n\npar(mar=c(5,4,2,1))\nhist(z, breaks=26, col=\"blue\", main=expression(paste(’Histograma ’, chi^2,’(6)’)), ylab=’Frequ^encia’, xlab=’26 intervalos’)\nbox()\n\nNa Figura 2.9 podemos observar os gráficos de histograma obtidos das variáveis descritas no Exemplo 2.23. A situação em (a) representa o caso em a distribuição dos dados de assemelha à distribuição normal, já a situação descrita no gráfico em (b) mostra-se uma mistura de densidades, percebemos a existência de duas modas.\n(a) Área do espaço de poros\t(b) Perímetro em pixels\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n0\t4000\t8000\t12000\npixels de 256 x 256\n \n0\t1000\t3000\t5000\n \n\nFigura 2.9: Histogramas das variáveis no Exemplo 2.23.\n\nOutras situações no mesmo exemplo, mas diferentes variáveis, são descritas nos gráficos na Figura 2.10. Nessa figura apresentamos dois gráficos, chamados de (c) e (d), nesta figura. Correspondem, como podemos observar, à distribuições assimétricas e descrevem os dados coletados nas variáveis shape e perm do arquivo de dados Rock, Exemplo 2.23. Os histogramas foram pensados somente para o caso de variáveis contínuas, porém é uma descrição discreta delas. Logicamente, também podemos utiliza-los em situações de variáveis aleatórias discretas, nada impede isso.\nEstas figuras foram geradas utilizando a configuração padrão do comando hist, isto é, utilizamos uma maneira automática de determinar o número de intervalos, mais adiante dedicamos maior atenção a diferentes formas de calcular este número.\nComo pode ter sido observado, além de não ficar claro como determinar o número de intervalos nem como delimitar os intervalos, também não ficou claro o que queremos realmente observar com o gráfico desta função.\nVejamos agora uma definição mais clara do histograma, esta definição nos permitirá obter propriedades impor- tantes.\n \n\n(c) Perímetro/sqrt(Área)\t(d) Permeabilidade\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n0.1\t0.2\t0.3\t0.4\t0.5\n \n0\t200\t600\t1000\t1400\nmili−Darcies\n \n\nFigura 2.10: Histogramas das variáveis no Exemplo 2.23.\n\n\n\nFoi provado por Robertson (1967) que, dados os intervalos I1, I2, \\cdots , Ik, o histograma $F$ é um estimador de máxima verossimilhança5 dentre os estimadores expressados como funções simples e semicontínuas superiormente, isto se o fecho de cada intervalos contiver duas ou mais observac¸ões. Os gráficos apresentados nas figuras 2.7, 2.9 e 2.10 são histogramas também segundo a proposta de Robertson (1967).\nPode-se observar que este estimador tem duas limitações importantes: a dependência do comprimento do intervalo e o fato de o histograma não constituir uma função contínua. A primeira destas limitações foi amplamente estudada por Wegman (1975). Ele provou que os pontos extremos de cada intervalo Ik devem ser coincidentes com observações e que, se o número mínimo de observações em cada intervalo aumente, conforme aumenta o tamanho da amostra, o estimador $F$ é consistente6.\nA segunda limitação importante do histograma, isto é, o fato de ele não constituir uma função contínua, incentivou diversos estudos na procura de estimadores contínuos da função de densidade. No Capítulo 3, a Seção\n4.3 dedica-se a mostrar estimadores contínuos da função de densidade.\n\n5Os estimadores de máxima verossimilhanc¸a serão estudados na Seção 4.2\n6Estimadores consistentes serão estudados na Seção 3.1.1\n \n\nCálculo automático do número de intervalos num histograma\nUma questão importante é determinar de maneira automatizada o número de intervalos disjuntos que serão utili- zados para a construção do gráfico.\nUma primeira forma de escolher o número de intervalos foi dada por Sturges (1926) e que constitui a forma padrão no R. Conhecida como fórmula de Sturges é dada por\nk = [log2_{(n)} + 1],\t(2.41)\nisto significa que o número de intervalos é a parte inteira do logaritmo base 2 do número de observações mais 1.\nOutras expressões comumente utilizadas são a fórmula de Scott (Scott, 1979) h = 3.5s/√3 n, onde s é o desvio padrão e a fórmula de Freedman Diacconi (Freedman & Diaconis, 1981) h = 2IQR(x)/√3 n, onde IRQ é a diferença\nentre o terceiro e o primeiro quantil.\n\n**Exemplo 2.24**\n\nNa libraria de funções R robustbase temos disponíveis dados do teor de cálcio e do pH em amostras de colo coletadas em diferentes comunidades da região de Condroz, na Bélgica.\nPodemos ler estes dados digitando as linhas de comando abaixo, primeiro para escolher a libraria de funções e depois para selecionar os dados.\nlibrary(robustbase) data(condroz)\nTemos registadas duas variáveis: Ca que registra o tero de cálcio na amostra de solo e o pH, o pH corres- pondente. Construímos histogramas da variável Ca segundo a três formas de escolha do número de intervalos e os apresentamos na Figura 2.11.\nOs dados deste exemplo foram publicados em: Hubert, M. and Vandervieren, E. (2006). An Adjusted Boxplot for Skewed Distributions, Technical Report TR-06-11, KULeuven, Section of Statistics, Leuven.\n\n \nSturges\n \nScott\n \nFreedman−Diaconis\n \n\n\n\n\n\n\n\n\n\n\n\n\n \n0\t1000\t2000\t3000\t4000\nCa\n \n0\t1000\t2000\t3000\t4000\nCa\n \n0\t1000\t2000\t3000\t4000\nCa\n \n\nFigura 2.11: Diferentes histogramas da variável Ca no Exemplo 2.24.\n\n\n### 2.4.3\tGráficos para verificar normalidade\nUm primeiro gráfico chamado de qq-norm permite a comparação de duas distribuições de probabilidades traçando seus quantis uns contra os outros. Depois exploramos um gráfico mais recente, conhecido como worm plot (gráfico de minhoca), consistindo numa determinada coleção de de qq-norm.\n \n\nQQ-norm\nO gráfico quantil-quantil ou qq-plot, proposto por Wilk & Gnanadesikan (1968), é um dispositivo gráfico explo- ratório utilizado para verificar a validade de um pressuposto de distribuição para um conjunto de dados. Em geral, a ideia básica é a de calcular o valor teoricamente esperado para cada ponto de dados com base na distribuição em questão. Se os dados de fato seguirem a distribuição assumida os pontos deste gráfico formarão aproximadamente uma linha reta.\nPercebemos que podemos verificar com este gráfico qualquer densidade contínua, eventualmente pode ser uti- lizado também para funções de probabilidade. O qq-plot vai apresentar-se como uma linha reta se a densidade assumida estiver correta. Vejamos o caso particular de verificarmos se a densidade é normal, nesta situação o gráfico qq-plot será chamado de qq-norm. Primeiro consideraremos a situação da densidade normal padrão.\nSeja z1, z2, \\cdots , zn uma amostra aleatória de uma distribuição normal com média µ = 0 e desvio padrão σ = 1.\nAs estatísticas de ordem amostrais são\nz_{(1)} ≤ z_{(2)} ≤ \\cdots ≤ z_{(n)}·\nEstes valores desempenharão o papel dos quantis da amostra. Agora, quais devemos tomar como os quantis teóricas correspondentes? Se a função de distribuição cumulada da densidade normal padrão fosse denotada por Φ, usando a notação quantil, se ξq é o q-ésimo quantil de uma distribuição normal, então\nΦ(ξq) = q,\nou seja, a probabilidade de uma amostra normal ser inferior a ξq é, de fato, apenas q.\nConsidere o primeiro valor ordenado z_{(1)}. O que podemos esperar que o valor Φ(z_{(1)}) seja? Intuitivamente, esperamos que essa seja a probabilidade de assumir um valor no intervalo (0, 1/n). Do mesmo modo, espera-se que Φ(z_{(2)}) seja a probabilidade de assumir um valor no intervalo (1/n, 2/n). Continuando, esperamos que Φ(z_{(n)}) seja a probabilidade de assumir um valor no intervalo (n  1)/n, 1). Assim, o quantil teórico desejamos seja definido pelo inverso da função de distribuição acumulada normal padrão. Em particular, o quantil teórico correspondente ao quantil empírico z_{(i)} deve ser\n \n\n\npara i = 1, 2, \\cdots , n.\n \nξ  = q\ti − 0, 5 ,\nq\tn\n \n\n \nQQ−plot nomal\n \nQQ−plot nomal\n \nQQ−plot nomal\n \n\n \t \t \n \n−3\t−2\t−1\t0\t1\t2\t3\nQuantis teóricos\n \n−3\t−2\t−1\t0\t1\t2\t3\nQuantis teóricos\n \n−3\t−2\t−1\t0\t1\t2\t3\nQuantis teóricos\n \n\nFigura 2.12: Diferentes qqplot para dados normais.\n\nNa Figura 2.12, a esquerda acima exibimos o qq-norm de uma pequena amostra normal de tamanho 5. Os restantes quadros na Figura 2.12 exibem as plotagens de qq-norm para amostras normais de tamanhos n = 100 e\n \n\nn = 1000, respectivamente. Como o tamanho da amostra aumenta, os pontos encontram-se mais perto da linha\ny = x.\nEstes gráficos (Figura 2.12) foram gerados utilizando as linhas de comando:\nset.seed(1278) x=rnorm(5)\nqqnorm(x, xlim=c(-3,3), ylim=c(-3,3), cex=0.6, pch=19, ylab=’Quantis amostrais’, xlab=’Quantis teóricos’, main=’QQ-plot nomal’)\nqqline(x,col=\"red\") text(-1,2,’n=5’)\npara a situação de amostra de tamanho 5. A primeira linha de comando serve para fixar o gerador de números laetórios e, dessa forma, podermos simular sempre a mesma amostra e reproduzir o gráfico idêntico. Nas outras situações somente muda-se o tamanho da amostra que se quer gerar.\n\nQQ−plot nomal\tQQ−plot nomal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n−3\t−2\t−1\t0\t1\t2\t3\nQuantis teóricos\n \n−3\t−2\t−1\t0\t1\t2\t3\nQuantis teóricos\n \n\nFigura 2.13: Diferentes qqplot para dados não normais.\nAssim, os comandos para gerar o segundo e terceiro gráficos são:\nx=rnorm(100)\nqqnorm(x, xlim=c(-3,3), ylim=c(-3,3), cex=0.6, pch=19, ylab=’Quantis amostrais’, xlab=’Quantis teóricos’, main=’QQ-plot nomal’)\nqqline(x,col=\"red\") text(-1,2,’n=100’)\n\nx=rnorm(1000)\nqqnorm(x, xlim=c(-3,3), ylim=c(-3,3), cex=0.6, pch=19, ylab=’Quantis amostrais’, xlab=’Quantis teóricos’, main=’QQ-plot nomal’)\nqqline(x,col=\"red\") text(-1,2,’n=1000’)\nCaso os dados não forem padronizados bastar aplicar a transformação (X − µ)/σ, onde X representa os dados originais e µb e σb representam os estimadores dos parâmetros µ e σ, respectivamente.\n \n\nEstes gráficos podem indicar afastamentos da normalidade por isso apresentamos duas situações de dados não simétricos e com cuadas pesadas. Na Figura 2.13, mostramos o que acontece se os dados forem da distribuição t-Student(8) e da distribuição χ2(5), sempre de tamanho n = 1000. Observe, em particular, que os dados a partir da distribuição t-Student seguem a curva normal bem de perto até os últimos pontos em cada extremo. Na outra situação o afastamento da distribuição normal é evidente.\nFoi mencionado que o qq-norm é uma situação particular do qq-plot devido a este último permitir comparar os quantis amostrais com os quantis distribucionais. Com isto queremos dizer que o qq-plot serve para verificar se os dados forem t-Student ou χ2(5), por exemplo. Na Figura 2.14 apresentamos a aparência dos gráficos qq-plot caso queira-se verificar se as amostras seguem distribuição t-Student(8) ou χ2(5), respectivamente.\n\n \nQQ plot para t−Student(8)\n\n\n\n\n\n\n\n\n\n−4\t−2\t0\t2\t4\nt−Student(8)\n \nQQ plot para c2(5)\n\n\n\n\n\n\n\n\n\n\n0\t5\t10\t15\t20\nc2(5)\n \n\nFigura 2.14: Diferentes qqplot para dados não normais.\nOs gráficos na Figura 2.14 foram gerados pelas linhas de comandos\nqqplot(qt(ppoints(1000), df = 8), x, cex=0.6, pch=19, main = \"QQ plot para t-Student(8)\", xlab=\"t-Student(8)\")\nqqline(x, distribution = function(p) qt(p, df = 8), prob = c(0.1, 0.6), col = 2)\nno caso t-Student(8) e\nqqplot(qchisq(ppoints(1000), df = 5), x, cex=0.6, pch=19,\nmain = expression(\"QQ plot para\" ~~ {chi^2}(5)), xlab=expression({chi^2}(5))) qqline(x, distribution = function(p) qchisq(p, df = 5), prob = c(0.1, 0.6), col = 2)\npara o caso χ2(5).\n\nWorn plot\nO worm-plot é uma série de parcelas de gráficos qq-plot retificados. Constitui uma ferramenta de diagnóstico para visualização de quão bem um modelo estatístico se ajusta aos dados, para encontrar locais em que o ajuste pode ser melhorado e para comparar o ajuste de diferentes modelos.\nNa Figura 2.15 mostramos este gráfico para duas situações: a esquerda os dados são normais e a direita os dados são t-Student com 8 graus de liberdade. Nesta situação aparece bem a qualidade da observação com esta\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n−4\t−2\t0\t2\t4\nUnit normal quantile\n \n−4\t−2\t0\t2\t4\nUnit normal quantile\n \n\nFigura 2.15: Diferentes worm-plot para dados normais.\n\nfigura. Se os dados forem normais o curva worm-plot ou gráfico de minhoca deve aparentar um verme achatado, os pontos próximos a curva vermelha e com poucas oscilações. Quando aplicamos este gráfico ao caso t-Student percebemos uma oscilação grande no verme e com pontos fugindo da banda de confiança. Isso comprova que os dados não seguem como referência a distribuição normal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n−4\t−2\t0\t2\t4\nUnit normal quantile\n \n−4\t−2\t0\t2\t4\nUnit normal quantile\n \n\nFigura 2.16: Diferentes worm-plot para dados não normais.\nAs linhas a seguir mostram os comandos necessários para gerar os gráficos na Figura 2.15. Utilizamos a libraria de comandos R gamlss (Rigby & Stasinopoulos, 2005).\n \n\nlibrary(gamlss) x=rnorm(1000) wp(gamlss(x~1), cex=0.6) x=rt(1000, df=8) wp(gamlss(x~1), cex=0.6)\n\nNa Figura 2.16, a esquerda temos o caso de dados com distribuição χ2(5) e a direita dados com distribuição Cauchy padrão. Nestas situações fica claro que os dados não são normais. Oa gráficos na figura foram gerados pelas linhas de comando a seguir.\nx=rchisq(1000, df=5) wp(gamlss(x~1), cex=0.6) x=rcauchy(1000) wp(gamlss(x~1), cex=0.6)\n \n## Exercícios\nExercícios da Seção 2.1\n1.\tSeja X ∼ Bernoulli( 1 ) e considere todas as possíveis amostras aleatórias de tamanho n = 3. Calcule Xn e S2 cada uma das\n \n2\tn\noito amostras. Encontre a função de probabilidade de Xn e S2.\n2.\tUm dado é lançado. Seja X o valor da face superior que aparece e X1, X2 duas observações independentes de X. Encontre a função de probabilidade de Xn.\n3.\tSeja X1, \\cdots , Xn uma amostra aleatória de alguma população. Mostre que\n(n − 1)Sn\n \nmax |Xi\tXn| <\n1≤i≤n\n\nonde Sn é a raiz quadrada positiva da variância amostral S2.\n \n√n\t,\n \n\nExercícios da Seção 2.2\n1.\tSeja (X_{(1)}, X_{(2)}, \\cdots , X_{(n)}) o conjunto das estatísticas de ordem de n variáveis aleatórias independentes $X_1, X_2, \\cdots , X_n$ com função de densidade comum\n \nf (x) =\n \nβe−xβ,\tse  x\t0\n·\n0,\tcaso contrário\n \na)\tMostre que X(s) e X(r) − X(s) são independentes para quaisquer r > s.\nb)\tEncontre a função de densidade de X(r+1) − X(r).\nc)\tSeja Z1 = nX_{(1)}, Z2 = (n − 1)(X_{(2)} − X_{(1)}), Z3 = (n − 2)(X_{(3)} − X_{(2)}), ..., Zn = ((X_{(n)} − X(n−1))).  Prove que\n(Z1, Z2, \\cdots , Zn) e $(X_1, X_2, \\cdots , X_n)$ são identicamente distribuídas.\n2.\tProvar o Teorema 2.1\n3.\tSejam $X_1, X_2, \\cdots , X_n$ variáveis aleatórias com distribuição geométrica de parâmetros p1, p2, \\cdots , pn, respectivamente. Prove que\nNn = min$(X_1, X_2, \\cdots , X_n)$ têm também distribuição geométrica de parâmetro\nn\np = 1 −\t(1 − pi)·\ni=1\n4.\tAs X1, \\cdots , Xn variáveis aleatórias independentes e identicamente distribuídas tem por função de probabilidade BN (1; p) se, e somente se, Nn = min(X1, \\cdots , Xn) tem distribuição geométrica de parâmetro 1 − (1 − p)n.\n5.\tSejam $X_1, X_2, \\cdots , X_n$ variáveis aleatórias independentes e igualmente distribuídas com função de densidade comum\n\n \nf (x) =\n \nσ\n0,\tse  x ≤ \\Theta\n \nMostre que X_{(1)}, X_{(2)} − X_{(1)}, X_{(3)} − X_{(2)}, \\cdots , X_{(n)} − X(n−1) são independentes.\n6.\tSejam $X_1, X_2, \\cdots , X_n$ variáveis aleatórias independentes e igualmente distribuídas com função de distribuição acumulada comum\n\n \nF (t) =\n \ntα,  se  0 < t < 1\n 1,\tse  t ≥ 1\n \npara α > 0. Mostre que X_{(i)}/X_{(n)}, i = 1, 2, \\cdots , n − 1 e X_{(n)} são independentes.\n7.\tSejam X1 e X2 duas variáveis aleatórias discretas independentes com função de probabilidade comum\nP (X = x) = \\Theta(1 − \\Theta)x−1,\tx = 1, 2, \\cdots ;\t0 < \\Theta < 1· Mostre que X_{(1)} e X_{(2)} − X_{(1)} são independentes.\n8.\tSejam X1, \\cdots , Xn duas variáveis aleatórias independentes com função de densidade comum $F$ . Encontre a função de densidade\nde X_{(1)} e de X_{(n)}.\n \n\n9.\tSejam X_{(1)}, X_{(2)}, \\cdots , X_{(n)} as estatísticas de ordem de n variáveis aleatórias independentes e igualmente distribuídas $X_1, X_2, \\cdots , X_n$\ncom função de densidade comum\nf (x) =\t1\tse  0 < x < 1  ·\n0,\tcaso contrário\nProve que Y1 = X_{(1)}/X_{(2)}, Y2 = X_{(2)}/X_{(3)}, \\cdots , Yn−1 = X(n−1)/X_{(n)} e Yn = X_{(n)} são independentes. Encontre a função de densidade conjunta de Y1, Y2, \\cdots , Yn.\n10.\tSejam X1.X2, \\cdots , Xn variáveis aleatórias independentes identicamente distribuídas não negativas contínuas. Prove que se E|X| < ∞, então E|X(r)| < ∞. Definamos Mn = X_{(n)} = max$(X_1, X_2, \\cdots , X_n)$. Mostre que\n∫ ∞\n   \t \nEncontre E(Mn) em cada uma das seguintes situações:\na)\tXk tem como função de distribuição comum $F$ (x) = 1 − e−xβ, se x ≥ 0.\nb)\tXk tem como função de distribuição comum $F$ (x) = x, se 0 < x < 1.\n\n11.\tProvar que, qualquer seja a amostra aleatória X1.X2, \\cdots , Xn sempre cumpre-se que X_{(1)} ≤ X ≤ X_{(n)}.\n12.\tDemonstrar o Teorema 2.5.\n13.\tDemonstrar o Teorema 2.9.\n\nExercícios da Seção 2.3\n1.\tDemonstre o Corolário 2.17.\n2.\tDemonstre o Corolário 2.18.\n\n3.\tSeja X1, \\cdots , Xn uma amostra aleatória Poisson(\\Theta). Encontre Var(S2) e compare-a com Var(X). Observe que E(X) = \\Theta = E(S2).\n4.\tSeja $X_1, X_2, \\cdots , X_n$ uma amostra aleatória da função de distribuição $F$ e seja $F$ ∗(x) a função de distribuição amostral. Encontre\nCov[F ∗(x), $F$ ∗(y)] para números reais fixos x, y.\nn\tn\n5.\tSeja $F$ ∗ a função de distribuição empírica de uma amostra aleatória com função de distribuição teórica $F$ . Prove que\n{\t∗\t ϵ   }\t 1\n\n6.\tSejam $X_1, X_2, \\cdots , X_n$ n observacões independentes da variável aleatória X. Encontre a distribuição amostral de X, a média amostral, se:\na)\tX ∼ P (\\Theta);\nb)\tX ∼ Cauchy(1, 0);\nc)\tX ∼ χ2(m).\n\n7.\tSeja X1, \\cdots , Xn uma amostra aleatória Poisson(\\Theta). Encontre Var(S2) e compare-a com Var(X). Observe que E(X) = \\Theta = E(S2).\n8.\tDemonstre o Teorema 2.23. [Dica: para quaisquer reais µ e σ > 0, encontre a função de densidade de (U(r) − µ)/σ e mostre que as variáveis padronizadas de U(r), (U(r) − µ)/σ, são assintoticamente N (0, 1) sob as condições do teorema.]\n9.\tProvar que o momentos amostral central b1 é sempre zero.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}