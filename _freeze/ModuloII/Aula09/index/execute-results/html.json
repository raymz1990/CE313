{
  "hash": "615a753d6108fe6ae17ad1c9a209a578",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Problemas de Posição\"\ndate: 2024-03-27\ncode-fold: true\n---\n\n\nSeja $X_1, X_2, \\cdots, X_n$ uma amostra aleatória de tamanho $n$ de alguma distribuição desconhecida $F$. Seja $p$ um número rela positivo satisfazendo $0 < p < 1$ e $k_p(F)$ denotando o quantil de ordem $p$ da distribuição $F$, ou seja,, $k_p(F)$ é tal que\n$$\nP \\Big(X \\leq k_p(F) \\Big) = p,\n$$\n\nisto devido a que no seguinte estudo vamos considerar $F$ absolutamente contínua.\n\nO **problema de posição** consiste em verficar se $H_0 : k_p(F) = k_0$, sendo $k_0$ um valor dado, contra alguma das alternativas $k_p(F) > k_0, k_p(F) < k_0$ ou $k_p(F) \\neq k_0$. O problema de posição e simetria consiste em verificar se $H_0 : k_{0.5}(F) = k_0$ e $F$ é simétrica contra $H_0 : k_{0.5}(F) \\neq k_0$ ou $F$ não é simétrica.\n\n## Teste do Sinal\n\nSeja $X_1, X_2, \\cdots, X_n$ uma amostra aleatória com função de densidade comum $f$. O problema aqui é verificarmos\n$$\nH_0 : κ_p(f) = κ_0 \\ \\ \\ \\ \\ H_1 : κ_p(f) > κ_0,\n$$\nonde $κ_p(f)$ é o quantil de ordem $p$ para $f, 0 < p < 1$. Vejamos como construir a estatística de teste.\n\nSeja $i(X_1, X_2, \\cdots, X_n)$ o número de elementos positivos em \n$$\nX_1 − κ_0, X_2 − κ_0, \\cdots, X_n − κ_0·\n$$\n\nObservemos que $P(X_i = κ_0) = 0$, desde que $X_i$ seja uma variável aleatória do tipo contínua.\n\nPode ser demonstrado que o teste uniformemente mais poderoso para verificar $H_0$ versus $H_1$ é dado por:\n\n$$\n\\varphi (x_1, x_2, \\cdots, x_n) =\n\\begin{cases} \n1,      & i(x_1, x_2, \\cdots, x_n) > c, \\\\\n\\gamma, & i(x_1, x_2, \\cdots, x_n) = c, \\\\\n0,      & i(x_1, x_2, \\cdots, x_n) < c.\n\\end{cases}\n$$\n\nonde $c$ e $\\gamma$ são escolhidos de forma que\n\n$$\n\\sum_{k=c}^{n} \\binom{n}{i} \\ q^i \\ p^{n-i} \\ +\n\\gamma \\binom{n}{c} \\ q^c \\ p^{n-c} = \\alpha,\n$$\n\ncom $q = 1 − p$. Isto é devido a que, sob $H_0$ verdadeira, $κ_p(f) = κ_0$, de maneira que $P(X \\leq κ_0) = p$ e $P(X \\geq κ_0) = q$ e $i(X) \\sim Binomial(n, q)$.\n\nA mesma estatística de teste pode ser utilizada em qualquer outra situação, ou seja, utiliza-se também caso $H_0 : κ_p(f) \\neq κ_0$ versus $H_1 : κ_p(f) \\geq κ_0, H_0 : κ_p(f) \\geq κ_0$ versus $H_1 : κ_p(f) \\leq κ_0$ e $H_0 : κ_p(f) = κ_0$ contra $H_1 : κ_p(f) 6 \\neq κ_0$.\n\nAs expressões do $p-$valor para este teste podem ser obtidas de maneira geral quando $p = 0.5$. Por exemplo, se a alternativa é de cauda superior $H_1 : κ_p(f) \\geq κ_0$, o $p-$valor para o teste de sinal é dado pela probabilidade binomial na cauda superior\n\n$$\n\\sum_{i=i(X)}^{n} \\ \\binom{n}{i} \\ 0.5^n,\n$$\n\nsendo $i(x) = i(x_1, x_2, \\cdots , x_n)$ o valor observado da estatística de teste. Poderíamos gerar tabelas e aplicar o teste de sinal exato para qualquer tamanho de amostra. Sabemos que a aproximação normal à binomial é especialmente boa quando $p = 0.5$.\n\nPortanto, para valores moderados de $n$ digamos, pelo menos 12, a aproximação normal pode ser usada para determinar as regiões de rejeição.\n\nComo esta é uma aproximação contínua a uma distribuição discreta, uma correção de continuidade de 0.5 pode ser incorporada aos cálculos. Por exemplo, para a alternativa, $H_1 : κ_{0.5}(f) \\geq κ_0$, $H_0$ é rejeitada para $i(x_1, x_2, \\cdots , x_n) \\geq κ_\\alpha$, sendo $κ_\\alpha$ satisfazendo\n\n$$\nk_\\alpha = 0.5n \\ + \\ 0.5 \\ + \\ 0.5 \\sqrt{nZ_\\alpha}.\n$$\n\nSimilarmente, o p-valor aproX_imado é\n$$\n1 - \\phi \\Bigg(\\frac{k_o - 0.5 - 0.5}{\\sqrt{0.25n}} \\Bigg)\n·\n$$\n\nO teste do sinal para as diferentes hipóteses está programado no pacote **DescTools**, função **SignTest**.\n\n**Exemplo**:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(DescTools)\nx = c(203,168,187,235,197,163,214,233,179,185,197,216)\nSignTest(x, mu = 195, alternative = \"greater\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne-sample Sign-Test\n\ndata:  x\nS = 7, number of differences = 12, p-value = 0.3872\nalternative hypothesis: true median is greater than 195\n98.1 percent confidence interval:\n 179 Inf\nsample estimates:\nmedian of the differences \n                      197 \n```\n\n\n:::\n:::\n\n\nO teste do sinal de amostra única, descrito aqui, pode ser modificado para se aplicar à amostragem de uma população bivariada.\n\n## teste de Wilcoxon\n\nO teste de sinal perde informação, pois ignora a magnitude da diferença entre as observações e o quantil hipotético. O teste de postos sinalizados de Wilcoxon fornece um teste alternativo de posição e simetria que também leva em conta a magnitude dessas diferenças.\n\nSeja $X_1, X_2, \\cdots , X_n$ uma amostra aleatória com função de distribuição $F$ absolutamente contínua que é simétrica a respeito\nda mediana $m$. O problema é testar\n\n$$H_0 : m = m_0$$\n\ncontra as alternativas usuais unilaterais ou bilaterais.\n\nSem perda de generalidade, assumimos que $m_0 = 0$. Então $F(−x) = 1 − F(x) \\  \\text{para todo } x \\in \\mathbb{R}$.\n\nPor exemplo, para testar\n\n$$\nH_0 : F(0) = \\frac{1}{2} \\ \\ \\ \\ \\ \n\\text{ou} \\ \\ \\ \\ \\ \nm = 0,\n$$\n\nprimeiro organizamos $\\mid X_1 \\mid , \\mid X_2\\mid , \\cdots , \\mid X_n\\mid$  em ordem crescente de magnitude e atribuímos postos $1, 2, \\cdots, n$ mantendo-se a par dos sinais originais de $X_i$.\n\nPor exemplo, se $n = 4$ e $\\mid X_2\\mid \\ < \\ \\mid X_4\\mid \\ < \\ \\mid X_1 \\mid  \\ < \\ \\mid X_3 \\mid$  oposto de $\\mid X_1 \\mid$ \né 3, de $\\mid X_2\\mid$  é 1, de $\\mid X_3\\mid$ é 4 e de $\\mid X_4 \\mid$  é 2.\n\nDefinimos\n$$\n\\begin{cases} \nT^+ & = & \\text{soma dos postos dos } X \\text{ positivos}, \\\\\nT^- & = & \\text{soma dos postos dos } X \\text{ negativos}.\n\\end{cases}\n$$\n\nEntão, considerando $H_0$ verdadeira, esperamos que $T^+$ e $T^−$ sejam o mesmo.\n\nObserve que\n\n$$\nT^+ + T^- = \\sum_{i=1}^{n}i = \\frac{n(n + 1)}{2},\n$$\n\nde maneira que $T^+$ e $T^−$ são linearmente relacionados e oferecem critérios equivalentes. Definimos agora\n\n$$\nZ_i = \n\\begin{cases} \n1, & \\text{caso } & X_i > 0 \\\\\n0, & \\text{caso } & X_i < 0\n\\end{cases},\n\\ \\ \\ \\ \\ i = 1, 2, \\cdots, n,\n$$\n\ne escrevemos $r(\\mid X_i\\mid ) = r_i$, para o posto de $\\mid X_i \\mid$.\n\nEntão $T^+ = \\displaystyle \\sum_{i=1}^n r_i Z_i$ e $T^- = \\displaystyle \\sum_{i=1}^n r_i (1 - Z_i)$ . Também,\n\n$$\nT^+ - T^- = - \\sum_{i=1}^n r_i + 2\\sum_{i=1}^n r_i Z_i = 2 \\sum_{i=1}^n r_i Z_i - \\frac{n(n+1)}{2}.\n$$\n\nAs estatísticas $T^+$ e $T^-$ são conhecidas como estatísticas de Wilcoxon. Um grande valor de $T$ ou, equivalentemente, um pequeno valor de $T$ significa que a maioria dos grandes desvios de 0 são positivos e, portanto, rejeitamos $H_0$ em favor da alternativa, $H_1 : m > 0$.\n\n| $H_0$   | $H_1$       | Rejeitamos $H_0$ se           |\n|:-------:|:-----------:|:-----------------------------:|\n| $m = 0$ | $m > 0$     | $T^+ > c_1$                   |\n| $m = 0$ | $m < 0$     | $T^+ < c_2$                   |\n| $m = 0$ | $m \\neq  0$ | $T < c_3 \\text{ ou } T > c_4$ |\n\nVamos encontrar agora a distribuição de $T^+$.\n\nSeja\n$$\nZ_{(i)} = \n\\begin{cases} \n1, & \\text{se o } \\mid X_i \\mid \\text{ que tem posto } i \\text{ é } > 0, \\\\\n0, & \\text{caso contrário }.\n\\end{cases}\n$$\nÉ claro que $T^+ = \\displaystyle \\sum_{i=1}^n iZ_(i)$. As variáveis aleatórias\n\n$$\nZ_(1), Z_(2), \\cdots, Z(_n),\n$$\n\ntem por distribuição Bernoulli, são não correlacionadas mas não necessariamente igualmente distribuídas.\n\nTemos\n\n$$\n\\begin{array}{rcl}\n\t\\mbox{E}\\Big(Z_{(i)} \\Big) \n\t& = & P\\Big( Z_{(i)} = 1 \\Big) = P\\Big( \\big[ r \\big(\\mid X_i \\mid \\big) = i, X_{j} > 0 \\big] \\text{ para algum } j \\Big), \\\\ \n\t& = & P \\Big(i-\\text{ésima estatística de ordem em } \\\\\n\t& & \\mid X_1 \\mid, \\mid X_2 \\mid,  \\cdots, \\mid X_n \\mid \\text{ corresponde a um } X_j \\text{ positivo}  \\Big).\n\\end{array}\n$$\n\nEntão, \n\n$$\nE \\Big(Z_{(i)} \\Big) = \\int_0^\\infty n \\binom{n-1}{i-1} \n[F_{\\mid X \\mid}(u)]^{i-1}\n[1 - F_{\\mid X \\mid}(u)]^{n-1}\nf(u)du\n$$\n\na qual pode ser escrita como\n\n$$\nE \\Big(Z_{(i)} \\Big) = \nn \\binom{n-1}{i-1} \n\\int_0^\\infty \n[F(u)-F(-u)]^{i-1}\n[1-F(u)+F(-u)]^{n-i}\nf(u)du,\n$$\n\nonde $f$ é a função de densidade de $X$.\n\nAlém disso\n$$\nVar \\Big(Z_{(i)} \\Big) = E \\Big( Z_{(i)} \\big( 1 - E \\big( Z_{(i)} \\big) \\big) \\Big)\n$$\n\ne\n\n$$\nCov \\ Z_{(i)}, Z_{(j)} = 0, \\ \\ \\ \\ \\ i \\neq j·\n$$\n\nSob $H_0$, $X$ é simétrica a respeito de 0, de modo que $F(0) = \\frac {1}{2}$ e\n$F(−u) = 1 − F(u)$, para todo $u > 0$.\n\nEntão\n\n$$\nE Z_{(i)} = \nn \\binom {n − 1}{i − 1}\n\\int_0^\\infty \n\\big( 2F(u) − 1 \\big)^{i−1}\n\\big( 2 − 2F(u) \\big)^{n−i}\nf(u)du·\n$$\n\nEscolhendo $\\nu = 2F(u) − 1$, temos que\n\n$$\n\\begin{array}{rcl}\n\t\\mbox{E}\\Big(Z_{(i)} \\Big) \n\t& = & \\frac{n}{2} \n\t\\binom {n − 1}{i − 1}\n\t\\int_0^1 \n\t\\nu^{i-1}\n\t(1 - \\nu)^{n-1}d\\nu\n\t\\\\\n\t& = & \n\t\\frac{n}{2} \n\t\\binom {n − 1}{i − 1}\n\tB(i, n - 1 + 1) = \\frac{1}{2}.\n\\end{array}\n$$\nOs momentos de $T^+$, em geral, são dados por\n\n$$\nE(T^+) = \\sum_{i=1}^n\ni^2\nE \\Big(Z_{(i)} \\big(1 - E \\big(Z_{(i)} \\big) \\big) \\Big).\n$$\ne\n\n$$\nVar (T^+) = \\sum_{i=1}^n i^2 E \\Big( Z_{(i)} \\big( 1 - E \\big( Z_{(i)} \\big) \\big) \\Big).\n$$\n\nDe maneira que, considerando $H_0$ verdadeira, temos\n\n$$\nE_{H_0} (T^+) = \\frac{1}{2}\n\\sum_{i=1}^n i = \\frac{n(n + 1)}{4}\n$$\n\ne\n\n$$\nvar_{H_0} (T^+) = \\frac{1}{4}\n\\sum_{i=1}^n i^2 = \\frac{n(n + 1)(2n+1)}{24}\n$$\n\nObserve que $T^+ = 0$ se todas as diferenças tiverem sinais negativos e $T^+ = \\frac{n(n + 1)}{2}$ se todas as diferenças tiverem sinais positivos. Aqui, uma diferença significa uma diferença entre as observações e o valor postulado da mediana.\n\nA estatística $T^+$ é completamente determinada pelas funções indicadoras $Z_{(i)}$, assim o espaço amostral pode ser considerado\ncomo um conjunto de $(z_1, z_2, \\cdots, z_n)$, onde cada $z_i$ é 0 ou 1, de cardinalidade $2^n$.\n\nSob $H_0, m = m_0$ e cada arranjo é igualmente provável. Portanto\n\n$$\nP_{H_0} (T^+ = t) =\n\\frac\n{\\begin{cases}\n\\text{no. de maneiras de atribuir sinais + ou - aos} \\\\\n\\text{inteiros 1,2,..., n para que a soma seja t}\n\\end{cases}}\n{2^n}\n·\n$$\n\nObservemos que toda atribuição tem uma atribuição conjugada com sinais de mais e menos trocados, de modo que, este conjugado $T^+$ é dado por \n\n$$\n\\sum_{i=1}^n i \\big( 1 − Z_{(i)} \\big) = \n\\frac{n(n + 1)}{2} −\n\\sum_{i=1}^n iZ_{(i)}.\n$$\n\nAssim, sob $H_0$, a distribuição de $T^+$ é simétrica em relação à média $\\frac{n(n + 1)}{4}$.\n\n**Exemplo**:\n\nPara os dados -0.465, 0.120, -0.238, -0.869, -1.016, 0.417, 0.056, 0.561 queremos verificar se $H_0 : m = 1.0$ versus $H_1 : m > 1.0$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = c(-0.465,0.120,-0.238,-0.869,-1.016,0.417,0.056,0.561)\nlibrary(ggplot2)\ndados = data.frame(x=c(rep(\" \",8)), dados = x)\nqplot( x=x, y=dados, data=dados , geom=c(\"boxplot\",\"jitter\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\nhist(x, xlab = \"x\", col = \"green\", border = \"red\",\n     xlim = c(-1.5,1.5), ylim=c(0,0.8), breaks = 5,\n     freq = F, ylab=\"dados\", main = \"\")\nlines(density(x)); box(); grid()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n\n```{.r .cell-code}\nwilcox.test(x, mu = -1, alternative = \"greater\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWilcoxon signed rank exact test\n\ndata:  x\nV = 35, p-value = 0.007813\nalternative hypothesis: true location is greater than -1\n```\n\n\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}