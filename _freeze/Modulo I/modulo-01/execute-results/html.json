{
  "hash": "f4fbacbcb4b2ef3c04a9ed5dc2700929",
  "result": {
    "engine": "knitr",
    "markdown": "---\nTitle: Módulo I - Estimação não paramétrica\ncode-fold: true\n---\n\n\n# Módulo I {#sec-modulo01}\n\n:::{.callout-warning}\nÚltima atualização: 03 de agosto de 2018.\n:::\n\nSejam $x_{1},  x_{2}, \\cdots  x_{n}$, variáveis aleatórias independentes identicamente distribuídas com distribuição comum $L(X)$ e seja $P$  a classe de todas as possíveis distribuições de $X$ que consiste de todas as distribuições absolutamente contínuas ou discretas.\n \n>\n**Definição**. A estatística $T(X)$ é suficiente para a família de distribuições $P$ se a distribuição condicional de $X|T=t$ é a mesma, seja qual for a função de distribuição $F∈P$.\n \n**Exemplo**. Sejam $x_{1},  x_{2}, \\cdots  x_{n}$ variáveis aleatórias independentes igualmente distribuídas com distribuição absolutamente contínua e seja $T = (X_{(1)}, \\cdots, X_{(n)})$ a estatística de ordem. Então:\n$$f(x|T = t) = 1n!,$$\ne vemos que $T$ é uma estatística suficiente para a família das distribuições absolutamente contínuas.\n\nDefinição. A família de distribuições P\n é completa se somente a função zero for o estimador não viesado de 0, isto é,\nEF(h(X))=0,\npara todo F∈P\n implica que h(X)=0\n. Isto para todo x\n exceto para um conjunto nulo em relação a cada F∈P\n.\n\nExemplo. Seja X∼Uniforme(0,θ)\n, onde θ∈(0,∞)\n. Mostraremos que esta família de distribuições é completa. Precisamos mostrar que\nEθ(g(X))=∫θ0g(x)1θdx=0,∀θ>0,\nse, e somente se, g(x)=0\n, para todo x\n. Em geral, esse resultado segue a teoria da integração. Se g\n for contíua, diferenciamos ambos os lados de\n∫θ0g(x)dx=0,\npara obter g(θ)=0\n, para todo θ>0\n. Agora, seja X1,X2,⋯,Xn\n uma amostra aleatória da distribuição Uniforme(0,θ)\n. Também, seja X(n)=max(X1,X2,⋯,Xn)\n. Então, a função de densidade de X(n)\n é dada por\nfX(n)(x|θ)=nθ−nxn−1,0<x<θ,\nzero caso contrário. Vemos por um argumento semelhante que X(n)\n é completa, o que é o mesmo do que dizer que {fX(n)(x|θ);θ>0}\n é uma família de densidades completa. Claramente X(n)\n é suficiente.\n\nDefinição. Uma estatística T(X)\n é dita ser completa em relação a uma classe de distribuições P\n se a classe de distribuições induzidas de T\n for completa.\n\nClaro que todos os exemplos já encontrados de estatísticas completas ou famílias completas de distribuições para o caso paramétrico podem ser aplicados nesta situação.\n\n\nTeorema. A estatística de ordem (X(1),⋯,X(n))\n é uma estatística suficiente e completa desde que a amostra X1,X2,⋯,Xn\n seja composta de variáveis aleatórias independentes identicamente distribuídas do tipo discreta ou contínua.\nDemonstração Ver Fraser (1965).▉\n\nDefinição. Diz-se que uma função real g(F)\n é estimável se tiver um estimador não viciado, isto é, se existe uma estatística T(X)\n tal que\nEF(T(X))=g(F),\npara todo F∈P\n.\n\nExemplo. Se P\n é a classe de todas as distribuições para as quais o segundo momento existe, X\n é um estimador não viciado de μ(F)\n, a média da população. Similarmente\nμ2(F)=VarF(X),\né também estimável e um estimador não viciado é\nS2=1n−1∑i=1n(Xi−X¯¯¯¯)2⋅\nDa mesma forma, X¯¯¯¯−Y¯¯¯¯\n é um estimador não viciado de E(X)−E(Y)\n,\n1n{número de X>c}\né um estimador não viciado de PF(X>c)\n e assim por diante.\n\nDefinição. O grau m\n, m≥1\n, de um parâmetro estimável g(F)\n é o menor tamanho de amostra para o qual o parâmetro é estimável, ou seja, é o menor n\n para o qual existe um estimador não viciado T(X1,⋯,Xn)\n com\nEF(T(X))=g(F),\npara todo F∈P\n.\n\nExemplo. O parâmetro\ng(F)=PF(X>c),\nonde c\n é uma constante conhecida, têm grau 1. Também μ(F)\n é estimável com grau 1, para isto assumimos que existe ao menos um F∈P\n tal que μ(F)≠0\n. Acontece que μ2(F)\n é estimável com grau 2, desde que μ2(F)\n não seja estimável de forma não viciada por somente uma observação. Ao menos duas observações são necessárias. De maneira similar, μ2(F)\n têm grau 2.\n\nDefinição. Um estimador não viciado de algum parâmetro baseado no tamanho mínimo de amostra, ou seja, com amostra iagual ao grau m\n é chamado de kernel.\n\nExemplo. Seja X1,⋯,Xn\n uma amostra aleatória com distribuição F\n. Então Xi\n é o kernel de μ(F)\n; XiXj\n, i≠j\n, é o kernel de μ2(F)\n e cada\nT(Xi,Xj)=X2i−XiXj,i=1,⋯,n,i≠j,\né kernel de μ2(F)\n.\n\n\nTeorema. Existe um kernel simétrico para cada parâmetro estimável.\nDemonstração Seja T(X1,⋯,Xm)\n um kernel para g(F)\n. Também é\nTs(X1,⋯,Xm)=1m!∑PT(Xi1,Xi2⋯,Xim)\num kernel para g(F)\n, onde a soma P\n acontece sobre todas as m!\n permutacoes de {1,2,⋯,m}\n.▉\n\nExemplo. Seja X1,⋯,Xn\n uma amostra aleatória com distribuição F\n. Um kernel simétrico para μ2(F)\n é\nTs(Xi,Xj)=12(T(Xi,Xj)+T(Xj,Xi))=12(Xi−Xj)2,i=1,⋯,n,i≠j⋅\n\nDefinição. Seja g(F)\n um parâmetro estimável de grau m\n e X1,X2,⋯,Xn\n uma amostra aleatória de F\n de tamanho n\n, n≥m\n. Correspondendo a qualquer kernel T(X1,⋯,Xn)\n de g(F)\n, definimos a U-estatística para a amostra como\nU(X1,X2,⋯,Xn)=1(nm)∑CTs(Xi1,Xi2,⋯,Xin),\nonde o índice da soma C\n percorre todas as (nm)\n permutações de m\n inteiros (i1,i2,⋯,im)\n escolhidos de {1,2,⋯,n}\n e Ts\n é um kernel simétrico, como definido na demonstração do Teorema anterior.\n\nClaramente, a U-estatística definida é simétrica nos X\n e\nEF(U(X))=g(F),\npara todo F\n.\n\nExemplo. Seja X1,⋯,Xn\n uma amostra aleatória com distribuição F\n. Para estimarmos μ(F)\n a U-estatística é dada por\nU(X1,X2,⋯,Xn)=1n∑i=1nXi⋅\nPara estimarmos μ2(F)\n, um kernel simétrico é\nTs(Xi1,Xi2)=12(Xi1−Xi2)2,\npara i1=1,2,⋯,n\n, i1≠i2\n. A correspondente U-estatística é\nU(X)=1(n2)∑i1<i212(Xi1−Xi2)2=1n−1∑i=1n(Xi−X¯¯¯¯)2=S2⋅\n\nDe maneira similar, para estimarmos μ2(F)\n, o kernel simétrico é Ts(Xi1,Xi2)=Xi1Xi2\n e a correspondente U-estatística é\nU(X)=1(n2)∑i<jXiXj=1n(n−1)∑i≠jXiXj⋅\nPara estimarmos μ3(F)\n um kernel simétrico seria Ts(Xi1,Xi2,Xi3)=Xi1Xi2Xi3\n, sendo que a U-estatística é\nU(X)=1(n3)∑i<j<kXiXjXk=1n(n−1)(n−2)∑i≠j≠kXiXjXk⋅\nO seguinte resultado mostra a importância da U-estatística.\n\n\nTeorema. Seja P\n a classe de todas as distribuições absolutamente contínuas ou discretas. Qualquer função estimável g(F)\n, F∈P\n, tem um estimador único que é não viciado, simétrico nas observações e uniformemente de variância mínima entre todos os estimadores não viciados.\nDemonstração Seja X1,⋯,Xn\n uma amostra aleatória de F\n, F∈P\n e seja T(X1,⋯,Xn)\n um estimador não viciado de g(F)\n. Considere o conjunto de todas as n!\n permutações de {1,2,⋯,n}\n e indexá-los adequadamente. Seja {i1,i2,⋯,in}\n o i-ésimo deste conjunto e seja\nTi=Ti(X1,X2,⋯,Xn)=T(Xi1,Xi2⋯,Xin),i=1,2,⋯,n!⋅\nSeja\nT¯¯¯¯=1n!∑i=1n!Ti⋅\nClaro que E(T)=g(F)\n e\nVar(T¯¯¯¯)=≤E(1n!∑n!i=1Ti)2−(g(F))2E(1(n!)2∑n!i=1T2i)−(g(F))2=E(T2)−(g(F))2=Var(T)⋅\nA igualdade se mantém se, e somente se,\nTi(X1,X2,⋯,Xn)=αn!,i=1,2,⋯,n!,\npara todos os pontos no espaço amostral, exceto talvez para um conjunto nulo, em que α\n é uma constante. Segue-se que T(X)\n é simétrico nos argumentos X1,X2,⋯,Xn\n com probabilidade 1 e T¯¯¯¯\n é idêntico a T\n. A exclusividade é deixada como exercício.▉\n\n\nTeorema. Seja T(X1,⋯,Xn)\n um estimador não viciado para g(F)\n, F∈P\n. A correspondente U-estatística é essencialmente o único estimador não viciado uniformemente de mínima variância.\nDemonstração Consequência do teorema anterior.▉\n\nDe acordo com os teoremas acima precisamos apenas considerar estimadores que sejam simétricos nas observações e tudo o que devemos fazer é torná-las não viciados. Este procedimento leva a um estimador não viciado com a menor variância na classe de todos os estimadores não viciados do parâmetro. Por exemplo, como consequência destes teoremas, X¯¯¯¯\n e S2\n são os únicos estimadores não viciados uniformemente de variância mínima de μ(F)\n e μ2(F)\n, respectivamente.\n\nExemplo. Seja P\n a classe de todas as distribuições absolutamente contínuas e X1,X2,⋯,Xn\n uma amostra aleatória de tamanho n\n. Para estimarmos\ng(F)=PF(X1>c),\nonde c\n é uma constante fixa, definimos\nYi={1,0,Xi>c,Xi≤ci=1,2,⋯,n⋅\nConsidere agora\nT(Y1,Y2,⋯,Yn)=∑i=1nαiYi,\ncomo um estimador de g(F)\n. Para encontrar o estimador não viciado de mínima variância de g\n simetrizamos T\n nos Y1,Y2,⋯,Yn\n. Isso acontece se αi=α\n, i=1,2,⋯,n\n e T(Y)=α∑ni=1Yi\n. Para T\n ser não viciado, temos que\nEF(T)=α∑i=1nEF(Yi)=αng(F),\nde maneira que α=1n\n. Portanto, 1n∑ni=1Yi\n é o estimador não viciado de mínima variância; também\nVarF(T)=g(F)(1−g(F))n≤14n⋅\nAlém disso, Yi\n têm distribuição Bernoulli\n, de modo a\n1n(T−g(F))(g(F)(1−g(F)))12⟶DZ,n→∞,\nonde Z∼N(0,1)\n. Este resultado pode ser usado para encontrar limites de confiança em g(F)\n.\n\nSeja P\n a classe de todas as distribuições absolutamente contínuas na reta real. Sejam F,G∈P\n e definamos a função distância Δ(F,G)\n como segue:\nΔ(F,G)=∫∞−∞(F(x)−G(x))2F′(x)+G′(x)2dx⋅\nEsta função satisfaz as seguintes propriedades:\n\nΔ(F,G)=0\n se, e somente se, F=G\n.\nΔ(F,G)=Δ(G,F)\n.\nΔ(F,G)>0\n.\nPor outro lado, vamos supor que F(x)≠G(x)\n para algum x1\n, onde F(x1)−G(x1)=d>0\n. Dado que F\n e G\n são distribuições absolutamente contínuas, existe um x0<x1\n, tal que\nF(x0)−G(x0)=d2eF(x)−G(x)≥d2,\npara x0≤x≤x1\n. Dado que tanto F\n quanto G\n são ambas não decrescentes, pelo menos um dos F\n e G\n deve aumentar pelo menos d/2\n quando x\n varia de x0\n a x1\n. Então\nΔ(F,G)≥∫x1x0(F(x)−G(x))2F′(x)+G′(x)2dx≥(d2)2d/22>0⋅\nExemplo. Encontremos um estimador não viciado de mínima variância para Δ(F,G)\n. Sejam X1,X2,⋯,Xm\n uma amostra aleatória de F\n e Y1,Y2,⋯,Yn\n uma amostra aleatória de G\n, independentes. Consideramos que F,G∈P\n. Primeiro mostramos que\ng(F,G)=P({max(X1,X2)<min(Y1,Y2)}⋃{max(Y1,Y2)<min(X1,X2)})=13+2Δ(F,G)⋅\nTemos que\ng(F,G)=P({max(X1,X2)<min(Y1,Y2)})+P({max(Y1,Y2)<min(X1,X2)})\ne\nP(max(X1,X2)≤x)=F2(x),P(min(Y1,Y2)≥y)=[1−G(y)]2⋅\nEntão\ng(F,G)======∫∞−∞[1−G(y)]22F(y)F′(y)dy+∫∞−∞[1−F(x)]22G(x)G′(x)dx∫∞−∞[1+G2(y)−2G(y)]2F(y)F′(y)dy+∫∞−∞[1+F2(x)−2F(x)]2G(x)G′(x)dx2+∫∞−∞2[G2(x)F(x)F′(x)+F2(x)G(x)G′(x)−2F(x)G(x)(F′(x)+G′(x))]dx3−2∫∞−∞((F(x)+G(x))2−(F(x)−G(x))2)(F′(x)+G′(x)2)dx3−8∫∞−∞(F(x)+G(x)2)2(F′(x)+G′(x)2)dx+2Δ(F,G)3−83+2Δ(F,G)=g(F,G)⋅\nPara utilizarmos os teoremas acima, vamos definir\nφ(X1,X2,Y1,Y2)={1,0,se max(X1,X2)<min(Y1,Y2) ou se max(Y1,Y2)<min(X1,X2)caso contrário\nEntão φ(X1,X2,Y1,Y2)\n é um estimador não viciado de g(F,G)\n e de fato é um kernel de g(F,G)\n. A U-estatística correspondente, portanto, deve ser o estimador não viciado de mínima variância. Nós temos\nU(X,Y)=1(m2)(n2)∑i1<i2∑k1<k2φ(Xi1,Xi2,Yk1,Yk2),\nde maneira que U\n é o estimador não viciado de mínima variância de g(F,G)\n, assim como o estimador não viciado de mínima variância de Δ(F,G)\n é\nΔˆ(F,G)=12U(X,Y)−16⋅\n\nExemplo. Seja P\n a classe de todas as funções de distribuição absolutamente contínuas na reta real e X1,X2,⋯,Xm\n e Y1,Y2,⋯,Yn\n duas amostras aleatórias independentes de F\n e G\n, respectivamente, com F,G∈P\n. Queremos estimar\nρ(F,G)=P(X<Y)⋅\nCom esse objetivo, vamos definir\nZij={1,0,Xi<YjXi≥Yj\npara cada par Xi,Yj\n, i=1,2,⋯,m\n e j=1,2,⋯,n\n. Então ∑i=1mZij\n é o número de vezes que X<Yj\n e ∑j=1nZij\n é o número de vezes que Xi<Y\n. Mann and Whitney (1947) sugeriram utilizar o estimador U/mn\n, onde\nU=∑i=1m∑j=1nZij\ne\nE(U)=mnE(Zij)=mnP(X<Y)⋅\nEntão\nρˆ(F,G)=Umn,\né não viciado para ρ\n. Além disso, ρˆ\n é simétrico em X\n e Y\n, de modo que tem uma variância mínima. Para calcular a variância mínima, temos\nE(U2)=∑i∑j∑h∑kE(ZijZhk),\nonde\nZijZhk={1,0,se Xi<Yj e Xh<Ykcaso contrário,\nde modo a\nE(ZijZhk)=P(Xi<Yj,Xh<Yk)=⎧⎩⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪∫F(x)G′(x)dx,∫(1−G(x))2F′(x)dx,∫F2(x)G′(x)dx,(∫F(x)G′(x)dx)2, caso i=h,j=k caso i=h,j≠k caso i≠h,j=k caso i≠h,j≠k⋅\nHá mn\n termos com i=h,j=k\n; m(m−1)n\n termos com i≠h,j=k\n; mn(n−1)\n termos com i=h,j≠k\n e m(m−1)n(n−1)\n termos com i≠h,j≠k\n. Segue que\nE(U2)=mn∫F(x)G′(x)dx+mn(n−1)∫(1−G(x))2F′(x)dx+m(m−1)n∫F2(x)G′(x)dx+m(m−1)n(n−1)(∫F(x)G′(x)dx)2,\nque leva à variância de U\n. Em particular, se F=G\n, então\nVar(U)=mn(m+n+1)12⋅\n\nI.1 Estimação de densidades\n\nDe certa forma, problemas de estimação não-paramétrica são extensões de problemas de estimação paramétrica, mas a natureza do primeiro é bem diferente do último. Considere, por exemplo, a situação de observações independentes identicamente distribuídas, digamos X1,X2,⋯,Xn\n. Em um problema paramétrico, assumimos que a distribuição de Xi\n é F(⋅;θ)\n, a qual é totalmente especificada até o vetor de parâmetros θ\n; então o problema é essencialmente a estimaçã de θ\n. Em um problema não-paramétrico, a distribuição é totalmente desconhecida com, talvez, algumas restriçõs em propriedades gerais e, portanto, é denotada por F\n.\n\nAqui consideramos estimadores de F\n em termos de função de densidade f\n. A função de densidade tem a vantagem de fornecer uma representação visualmente mais informativa da distribuição subjacente. Por exemplo, o histograma geralmente dá uma ideia aproximada da forma da distribuição. Este último ficou como o único estimador de densidade não paramétrico até 1950. Por essa razão, nossa discussão começará com os histogramas.\n\nEmbora o histograma seja usado extensivamente, não é tão frequente que seja necessária uma definição matemática. Uma maneira de defini-lo é através da função de densidade empírica.\n\nDefinição. Seja f\n a derivada de F\n; por isso pode-se expressar como\nf(x)=limh→0F(x+h)−F(x−h)2h⋅\nEntão, dizemos que fˆ\n, definido por\nfˆ(x)=Fˆ(x+h)−Fˆ(x−h)2h,\né o histograma, sendo que Fˆ\n é a função de distribuição empírica.\n\nO parâmetro h\n é chamado de largura de banda. Podemos escrever fˆ\n, definido acima como,\nfˆ(x)=12nh∑i=1n11(x−h;x+h)(Xi)⋅\nPodemos excrever a função de densidade como f(x)=limh→01h(F(x+h)−F(x−h))\n, mas não se pode definir daqui o histograma porque, então, esse limite é zero ou infinito e assim em algum momento é preciso parar, em outras palavras, não se pode chegar muito perto de zero.\n\n\nTeorema. Seja f\n a função de densidade da função de distribuição F\n. Então, com probabilidade 1,\nfˆ(x)∼Binomial(n,p),\ncom p=F(x+h)−F(x−h)\n. Assim, o comportamento assintótico do histograma pode ser derivado da distribuição binomial como\nE(fˆ(x))=F(x+h)−F(x−h)2h\ne\nVar(fˆ(x))=p(1−p)4nh2⋅\nDemonstração. Exercício.▉\n\nDeste teorema segue que fˆ(x)\n é um estimador consistente pontual de f(x)\n quando h→0\n e nh→∞\n. A seguir, o processo de limite é entendido como h=hn\n, de maneira que hn→0\n e nhn→∞\n. Estas condições podem ser interpretadas como se fosse necessário hn\n ir a zero, mas não muito rápido. Isso é exatamente o que temos especulado, exceto que agora temos a taxa exata de convergência, que pode ser escrita como hn=o(n)\n.\n\nExemplo. Utilaremos dados simulados da distribuíão N(0,1)\n, com isso mostramos o histograma destes 50 dados utilizando duas formas diferentes de encontrarmos uma expressão para hn\n, a chamada largura de banda.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1340)\nx = rnorm(50)\npar(mar=c(4,2,1,1))\nhist(x, main = NULL, freq = FALSE, breaks = \"Sturges\")\nbox()\n```\n\n::: {.cell-output-display}\n![](modulo-01_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mar=c(4,2,1,1))\nhist(x, main = NULL, freq = FALSE, breaks = \"Scott\")\nbox()\n```\n\n::: {.cell-output-display}\n![](modulo-01_files/figure-html/unnamed-chunk-1-2.png){width=672}\n:::\n:::\n\n\nNeste exemplo utilizamos duas formas de escolher a largura de banda hn\n dentre três diferentes possibilidades programadas na função hist. Por padrão escolhe-se breaks = \"Sturges\", porposto por Sturges (1929), o qual sugere que\nhn=max(X1,⋯,Xn)−min(X1,⋯,Xn)1+3.322ln(n)⋅\nA segunda situação indica que o qual significa que se os dados provêm da distribuição Normal temos que hn=3.49sn−1/3\n sendo s\n o desvio padrão estimado. Esta proposta deve-se à Scott (1979).\n\nEmbora o histograma é um estimador consistente quando hn→0\n e nhn→∞\n, verifica-se que se pode fazer melhor. A melhoria também é motivada por uma preocupação prática: o histograma não é uma função suave, uma propriedade que se pode esperar que qualquer função de densidade real tenha.\n\nDefinição. O estimador kernel da função de densidade é dado por\nfˆ(x)=1nhn∑i=1nK(x−Xihn),\nonde K(⋅)\n é uma função conhecida como kernel.\n\nÉ tipicamente assumido que K\n seja não-negativa, simétrica em torno de zero e satisfaz ∫K(u)du=1\n. Claro que o histograma é um caso especial do estimador do kernel se K\n for escolhido como a função de densidade da distribuição Uniforme(−1,1)\n. O último não é uma função suave e é por isso que o histograma não é suave; mas escolhendo K\n como uma função suave, tem-se um estimador de f\n que seja suave.\n\nPor exemplo, escolhendo a função de densidade N(0,1)\n, temos por resultado o conhecido como kernel Gaussiano e assim também utilizando a densidade de densidade Beta\n simétrica, dada por\nK(u)=Γ(ν+3/2)Γ(1/2)Γ(ν+1)(1−u2)ν,−1<u<1,\ne K(u)=0\n caso contrário. Os casos especiais ν=0,1,2,3\n correspondem às funções kernel uniforme, Epanechnikov, biweight e triweight, respectivamente.\n\n::: {.panel-tabset}\n## Diferentes kernel em R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkernels = eval(formals(density.default)$kernel)\nplot (density(0, bw = 1), xlab = \"\", main = \"Diferentes kernel em R\")\nfor(i in 2:length(kernels)) lines(density(0, bw = 1, kernel =  kernels[i]), col = i)\nlegend(1.5,.4, legend = kernels, col = seq(kernels), lty = 1, cex = .8, y.intersp = 1)\n```\n\n::: {.cell-output-display}\n![](modulo-01_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\nh.f = sapply(kernels, function(k) density(kernel = k, give.Rkern = TRUE))\nh.f = (h.f[\"gaussian\"] / h.f)^ .2\nh.f\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    gaussian epanechnikov  rectangular   triangular     biweight       cosine \n   1.0000000    1.0100567    0.9953989    1.0071923    1.0088217    1.0079575 \n   optcosine \n   1.0099458 \n```\n\n\n:::\n:::\n\n\n## Larguras de banda equivalentes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbw = bw.SJ(x) ## escolha automática\nplot(density(x, bw = bw), main = \"Larguras de banda equivalentes\")\nfor(i in 2:length(kernels)) lines(density(x, bw = bw, adjust = h.f[i], \n                                                           kernel = kernels[i]), col = i)\nlegend(55, 0.035, legend = kernels, col = seq(kernels), lty = 1)\n```\n\n::: {.cell-output-display}\n![](modulo-01_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n:::\n\nUm problema prático importante na estimação de densidades via kernel é como escolher a largura de banda hn\n. Note que dadas condições como hn→0\n e nhn→∞\n, ainda existem muitas opções para hn\n. Então, de certo modo, a ordem de convergência ou divergência não resolve o problema. Uma solução para esse problema é conhecida como compensação de viés-variância. Antes de entrarmos nos detalhes, vamos primeiro apressentar um resultado em relação ao viés assintótico do estimador kernel. Aqui, o viés é definido como\nviés(fˆ(x))=E(fˆ(x))−f(x),\npara um dado x\n.\n\n\nTeorema. Suponhamos que f\n seja contínua e limitada. Então o viés do estimador kernel de densidade converge a zero quando hn→0\n, para todo x\n.\nDemonstração. Observemos que\nE(fˆ(x))==1n∑ni=11hn∫K(x−yhn)f(y)dy∫K(u)f(x−hnu)du=f(x)+∫K(u)(f(x−hnu)−f(x))du⋅\nUtilizando então o teorema da convergência dominada completa-se a demonstração.▉\n\n\nTeorema. Suponhamos que f\n seja contínua três vezes diferenciável, com terceira derivada limitada na vizinhança de x\n e K\n satisfazendo\n∫K2(u)du<∞e∫|u|3K(u)du<∞⋅\nSe hn→0\n quando n→∞\n, temos que\nviés(fˆ(x))=h2n2f′′(x)∫u2K(u)du+o(h2n)⋅\nSe, além disso nhn→∞\n quando n→∞\n, então temos\nVar(fˆ(x))=f(x)nhn∫K2(u)du+o((nhn)−1)⋅\nDemonstração. A demonstração é baseada na expansão de Taylor,\nf(x−hnu)=f(x)−hnuf′(x)+h2nu22f′′(x)−h3nu36f′′′(ϵ),\nsendo ϵ\n fica entre x−hnu\n e x\n. Os detalhes são deixados como um exercício.▉\n\nUma medida de precisão do estimador é o erro quadrático médio (EQM), dado por\nEQM(fˆ(x))=E(fˆ(x)−f(x))2⋅\nÉ fácil mostrar que o EQM\n combina o viés e a variância de tal maneira que\nEQM(fˆ(x))=viés(fˆ(x))2+Var(fˆ(x))⋅\nVemos que, sob as condiçotilde;es hn→0\n e nhn→∞\n e se ignorarmos os termos de baixa ordem, temos\nEQM(fˆ(x))≈h4n4(f′′(x))2τ4+f(x)nhnγ2,\nonde τ2=∫u2K(u)du\n e γ2=∫K2(u)du\n. O termo à direita da expressão acima é minimizada quando\nhn=(γ2f(x)τ4(f′′(x))2)15n−15⋅\nNote ainda que a expressão acima não é a solução ideal, isso porque f\n é desconhecida na prática. No entanto, dá-nos pelo menos alguma ideia sobre a taxa ideal de convergência a zero, sendo esta hn=O(n−15)\n.\n\nQuando f\n é desconhecida, uma abordagem natural seria substituí-lo por um estimador e, assim, obter uma largura de banda ideal estimada. Uma complicação é que a largura de banda ideal depende de x\n mas, idealmente, gostaríamos de usar uma largura de banda que funcionasse para diferentes x\n dentro de um certo intervalo, se não todos os x\n. Para obter uma largura de banda ideal que não depende de x\n, integramos os dois lados da expressão de EQM\n em relação a x\n. Isto nos leva a\n∫EQM(fˆ(x))dx≈τ4h4n4∫(f′′(x))2dx+γ2nhn∫f(x)dx=τ4θ2h4n4+γ2nhn,\ncom θ2=∫(f′′(x))2dx\n. Pelo mesmo argumento, o lado direito acima é minimizado quando\nhn=(γ2τ4θ2)15n−15⋅\nDesta vez, o hn\n ideal não depende de x\n. Além disso, a integral do EQM\n ou o IEQM\n mínimo é dado por\nIEQM=∫EQM(fˆ(x))dx=54(τγ2)45θ25n−45⋅\nUma implicação é a seguinte. Note que o IEQM\n depende do kernel K\n através de cK=(τγ2)45\n. Mostrou-se que para os kernels comumente usados, tais como aqueles listados, o desempenho dos estimadores de kernel correspondentes é quase o mesmo em termos dos valores de cK\n. Voltando ao problema sobre a estimação da largura de banda ideal, vemos que tudo o que precisamos é encontrar um estimador consistente de θ2\n. Se f\n é a função de densidade da distribuição normal com desvio padrão σ\n, então pode ser mostrado que θ2=3/8π−−√σ5\n. Naturalmente, se alguém souber que f\n é normal, então a estimação da densidade não-paramétrica não seria necessária, porque um método paramétrico provavelmente seria melhor. Em geral, pode-se expandir f\n em torno da densidade gaussiana usando a expansão de Edgeworth.\n\nUtilizando a abordagem acima, Hjort and Jones (1996) obteveram o seguinte estimador ótimo para a largura de banda\nhˆn=hˆ0(1+3548γˆ4+3532γˆ23+3851024γˆ24)−15,\nonde hˆ0\n é a estimativa ideal da largura de banda assumindo que f\n é normal, isto é, com θ2\n substituído por 3/8π−−√σ5\n ou mais explicitamente\nhˆ0=1.06(σˆn15),\nchamamos hˆ0\n a largura de banda da linha de base e σˆ2\n é a variância amostral dada por\nσˆ2=1n−1∑i=1n(Xi−X¯¯¯¯)2⋅\nAlém disso, γˆ3\n e γˆ4\n são os estimadores dos coefcientes de assimetria de amostra e curtose, dado por\nγˆ3=1(n−1)σˆ3∑i=1n(Xi−X¯¯¯¯)3\ne\nγˆ4=1(n−1)σˆ4∑i=1n(Xi−X¯¯¯¯)4−3,\nrespectivamente. Houve outras abordagens para a seleção da largura de banda ótima, incluindo o método de validação cruzada. Ambos procedimentos foram programados na função density.\n\nExemplo. Utilaremos os dados simulados da distribuíão N(0,1)\n no exemplo anterior para com isso mostrarmos o histograma e o estimador Kernel da função de densidade.\n\n> set.seed(1340)\n> x = rnorm(50)\n> par(mar=c(4,2,1,1))\n> hist(x, main = NULL, freq = FALSE, breaks = \"Sturges\")\n> box()\n> lines(density(x, bw = \"nrd0\"), col = \"red\")\n> par(mar=c(4,2,1,1))\n> hist(x, main = NULL, freq = FALSE, breaks = \"Scott\")\n> box()\n> lines(density(x, bw = \"bcv\"), col = \"red\")\n\n\t\n\nI.2 Exercícios\nSeja T(X1,⋯,Xn)\n uma estatística simétrica nas observaçóes. Mostre que T\n pode ser escrita como função das estatísticas de ordem. Por outro lado, se T(X1,⋯,Xn)\n pode ser escrita como função das estatísticas de ordem, T\n é simétrica nas observações.\n\nSejam X1,X2,⋯,Xm\n e Y1,Y2,⋯,Yn\n amostras independentes de duas distribuições absolutamente contínuas. Encontre o estimador não viciado de mínima variância de:\n(a) E(XY)\n\n(b) Var(X+Y)\n\nSeja (X1,Y1),(X2,Y2),⋯,(Xn,Yn)\n uma amostra aleatória com distribuição absolutamente contínua bivariada. Encontre o estimador não viciado de mínima variância de:\n(a) E(XY)\n\n(b) Var(X+Y)\n\nCosidere (R,B,Pθ)\n um espaço de probabilidade e P={Pθ:θ∈Θ}\n. Seja A\n um elemento da σ\n-álgebra de Borel e considere d(θ)=Pθ(A)\n.\n(a) A função d\n é estimável? Se sim, qual é o grau?\n(b) Encontre o estimador não viciado de mínima variância de d\n, baseado em uma amostra de tamanho n\n e assumindo que P\n seja a classe de todas as distribuições contínuas.\n\n",
    "supporting": [
      "modulo-01_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}