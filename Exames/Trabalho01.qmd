---
Title: Trabalho 01
code-fold: true
---

# Trabalho 01 {.unnumbered}

**CE13 - Estatística Não Paramétrica**
<br>Departamento de Estatística - UFPR
<br>Professor: Fernando Lucambio
<br>Aluno: GRR 20233881 - Raymundo Eduardo Pilz
<br>3 de abril de 2024.

## Exercício 1. 

**Seja $X \sim Bernoulli \ \text{(1\2)}$ e considere todas as possíveis amostras aleatórias de tamanho $n = 3$. Calcule $\overline{X}_n$ e $S_n^2$ para cada uma das oito amostras. Encontre a função de probabilidade de $\overline{X}_n$ e $S_n^2$**.

* Amostras aleatórias possíveis de tamanho $n = 3$:

1. (0, 0, 0)
2. (0, 0, 1)
3. (0, 1, 0)
4. (0, 1, 1)
5. (1, 0, 0)
6. (1, 0, 1)
7. (1, 1, 0)
8. (1, 1, 1)

* Cálculo de $\overline{X}_n$ e $S_n^2$ para cada amostra:

1. (0, 0, 0)
   - $\overline{X}_n = \frac{(0 + 0 + 0)}{3} = 0$
   - $S_n^2 = \frac{(0-0)^2 + (0-0)^2 + (0-0)^2}{3} = 0$

2. (0, 0, 1)
   - $\overline{X}_n = \frac{(0 + 0 + 1)}{3} = \frac{1}{3}$
   - $S_n^2 = \frac{\big(0-\frac{1}{3}\big)^2 + \big(0-\frac{1}{3}\big)^2 + \big(1-\frac{1}{3}\big)^2}{3} = \frac{1}{3}$

3. (0, 1, 0)
   - $\overline{X}_n = \frac{(0 + 1 + 0)}{3} = \frac{1}{3}$
   - $S_n^2 = \frac{\big(0-\frac{1}{3}\big)^2 + \big(1-\frac{1}{3}\big)^2 + \big(0-\frac{1}{3}\big)^2}{3} = \frac{1}{3}$

4. (0, 1, 1)
   - $\overline{X}_n = \frac{(0 + 1 + 1)}{3} = \frac{2}{3}$
   - $S_n^2 = \frac{\big(0-\frac{2}{3}\big)^2 + \big(1-\frac{2}{3}\big)^2 + \big(1-\frac{2}{3}\big)^2}{3} = \frac{1}{3}$

5. (1, 0, 0)
   - $\overline{X}_n = \frac{(1 + 0 + 0)}{3} = \frac{1}{3}$
   - $S_n^2 = \frac{\big(1-\frac{1}{3}\big)^2 + \big(0-\frac{1}{3}\big)^2 + \big(0-\frac{1}{3}\big)^2}{3} = \frac{1}{3}$

6. (1, 0, 1)
   - $\overline{X}_n = \frac{(1 + 0 + 1)}{3} = \frac{2}{3}$
   - $S_n^2 = \frac{\big(1-\frac{2}{3}\big)^2 + \big(0-\frac{2}{3}\big)^2 + \big(1-\frac{2}{3}\big)^2}{3} = \frac{1}{3}$

7. (1, 1, 0)
   - $\overline{X}_n = \frac{(1 + 1 + 0)}{3} = \frac{2}{3}$
   - $S_n^2 = \frac{\big(1-\frac{2}{3}\big)^2 + \big(1-\frac{2}{3}\big)^2 + \big(0-\frac{2}{3}\big)^2}{3} = \frac{1}{3}$

8. (1, 1, 1)
   - $\overline{X}_n = \frac{(1 + 1 + 1)}{3} = 1$
   - $S_n^2 = \frac{(1-1)^2 + (1-1)^2 + (1-1)^2}{3} = 0$

* Cálculo de da função de Probilidade $(\mathcal{P})$ das amostras:

a) Para $\overline{X}_n$:

1. $\mathcal{P} \Big(\overline{X}_n = 0\Big) = \frac{1}{8}$ 

2. $\mathcal{P} \Big(\overline{X}_n = \frac{1}{3}\Big) = \frac{1}{8}$ 

3. $\mathcal{P} \Big(\overline{X}_n = \frac{2}{3}\Big) = \frac{3}{8}$ 

4. $\mathcal{P} \Big(\overline{X}_n = 1\Big) = \frac{1}{8}$ 

b) Para $S_n^2$: 

1. $\mathcal{P} \Big(S_n^2 = \frac{1}{3}\Big) = 1$ 

---

## Exercício 2. 

**Um dado é lançado. Seja $X$ o valor da face superior que aparece e $X_1, X_2$ duas observações independentes de $X$. Encontre a função de probabilidade de $\overline{X}_n$**.

Dado:

- $X$: Valor da face superior do dado lançado.
- $X_1$ e $X_2$: Duas observações independentes de $X$.

Sabemos que:

- O dado tem 6 faces numeradas de 1 a 6, cada uma com probabilidade igual de $\frac{1}{6}$ de aparecer.
- Vamos calcular $\overline{X}_n$, que é a média das observações.

$\overline{X}_n = \frac{X_1 + X_2}{2}$

Como $X_1$ e $X_2$ são independentes e têm a mesma distribuição de probabilidade que $X$, a média $\overline{X}_n$ terá a mesma distribuição que $X$.

Portanto, a função de probabilidade de $\overline{X}_n$ é a mesma que a função de probabilidade de $X$.

A função de probabilidade de $X$ é:

$$
\mathcal{P}(X = x) = 
\begin{cases}
\frac{1}{6} & \text{se } x = 1, 2, 3, 4, 5, 6 \\
0 & \text{caso contrário}
\end{cases}
$$

Portanto, a função de probabilidade de $\overline{X}_n$ é:

$$
\mathcal{P}(\overline{X}_n = x) = 
\begin{cases}
\frac{1}{6} & \text{se } x = 1, 2, 3, 4, 5, 6 \\
0 & \text{caso contrário}
\end{cases}
$$

Isso significa que a média das duas observações do dado tem a mesma distribuição de probabilidade que o valor de uma única observação do dado.

---

## Exercício 3. 

**Provar que, qualquer seja a amostra aleatória $X_1.X_2, \cdots , X_n$ sempre cumpre-se que $X_{(1)} \leq X \leq X_{(n)}$**.

* **Base da indução:** Para $n = 1$, a amostra consiste apenas em um elemento, então $X_{(1)} = X = X_{(n)}$. A desigualdade é normalmente verdadeira.

* **Hipótese de indução:** Suponha que a desigualdade seja verdadeira para uma amostra aleatória de tamanho $n$, isto é, $X_{(1)} \leq X \leq X_{(n)}$.

* **Passo da indução:** Vamos considerar uma amostra aleatória de tamanho $(n+1)$. Seja $X_{(1)}^{(n)}$ o menor valor na amostra e $X_{(n+1)}^{(n)}$ o maior valor na amostra.

Existem duas possibilidades:

1. O novo elemento é menor ou igual ao menor valor atual na amostra, ou seja, $X_{(1)}^{(n+1)} = X_{(1)}^{(n)}$.
2. O novo elemento é maior ou igual ao maior valor atual na amostra, ou seja, $X_{(n+1)}^{(n+1)} = X_{(n+1)}^{(n)}$.

Em ambos os casos, a desigualdade $X_{(1)} \leq X \leq X_{(n)}$ continua válida. Se o novo elemento for menor ou igual ao menor valor atual, então $X \geq X_{(1)}^{(n+1)} = X_{(1)}^{(n)}$; se for maior ou igual ao maior valor atual, então $X \leq X_{(n+1)}^{(n+1)} = X_{(n+1)}^{(n)}$.

Portanto, por indução, podemos concluir que para qualquer amostra aleatória $X_1, X_2, \cdots, X_n$, sempre cumpre-se que $X_{(1)} \leq X \leq X_{(n)}$.

---

## Exercício 4. 

**Seja $X_1, \cdots , X_n$ uma amostra aleatória $Poisson(\theta)$. Encontre $Var(S_n^2)$ e compare-a com Var($\overline{X}_n$)**.

Para uma amostra aleatória de uma distribuição Poisson com parâmetro $\theta$, a média e a variância da distribuição Poisson são ambas iguais a $\theta$. Portanto, para cada observação $X_i$, temos $E[X_i] = \theta$ e $Var(X_i) = \theta$.

A média amostral $\overline{X}_n$ é dada por:

$\overline{X}_n = \frac{1}{n}\sum_{i=1}^{n} X_i$

E a variância amostral $S_n^2$ é dada por:

$S_n^2 = \frac{1}{n-1}\sum_{i=1}^{n} (X_i - \overline{X}_n)^2$

Como os $X_i$ são independentes e cada um tem média $\theta$ e variância $\theta$, temos que:

$E\Big[\overline{X}_n\Big] = \frac{1}{n} \times n \times \theta = \theta$

$Var(\overline{X}_n) = \frac{1}{n^2} \times n \times \theta = \frac{\theta}{n}$

Para encontrar $Var(S_n^2)$, primeiro precisamos encontrar $E[S_n^2]$, que é a média da variância amostral $S_n^2$. Isso requer o cálculo dos momentos de segunda ordem da distribuição Poisson.

Para uma variável aleatória Poisson com parâmetro $\theta$, temos que:

$E[X_i^2] = Var(X_i) + (E[X_i])^2 = \theta + \theta^2$

Agora podemos calcular $E[S_n^2]$:

$E[S_n^2] = \frac{1}{n-1}\sum_{i=1}^{n} E[(X_i - \overline{X}_n)^2]$

$= \frac{1}{n-1}\sum_{i=1}^{n} (E[X_i^2] - 2E[X_i]\overline{X}_n + \overline{X}_n^2)$

$= \frac{1}{n-1}\sum_{i=1}^{n} (\theta + \theta^2 - 2\theta\overline{X}_n + \overline{X}_n^2)$

$= \frac{1}{n-1}\sum_{i=1}^{n} (\theta + \theta^2 - 2\theta\overline{X}_n + \frac{\theta^2}{n^2})$

$= \frac{1}{n-1}\sum_{i=1}^{n} \theta + \frac{\theta^2}{n} - 2\theta\overline{X}_n + \frac{\theta^2}{n^2}$

$= \frac{n \times \theta + n \times \frac{\theta^2}{n} - 2n \times \theta\overline{X}_n + \theta^2}{n-1}$

$= \frac{n \times \theta + \theta^2 - 2n \times \theta^2 + \theta^2}{n-1}$

$= \frac{n \times \theta - n \times \theta^2}{n-1}$

$= \frac{n \times (\theta - \theta^2)}{n-1}$

Portanto, 

$Var(S_n^2) = E[S_n^2] - (E[S_n^2])^2$

$= \frac{n \times (\theta - \theta^2)}{n-1} - \left(\frac{n \times (\theta - \theta^2)}{n-1}\right)^2$

Comparando $Var(S_n^2)$ com $Var(\overline{X}_n)$, podemos ver que a variância de $S_n^2$ é geralmente maior do que a variância de $\overline{X}_n$. Isso ocorre porque a variância de $S_n^2$ depende do tamanho da amostra $n$, enquanto a variância de $\overline{X}_n$ diminui à medida que o tamanho da amostra aumenta.

---

## Exercício 5. 

**Sejam $X_1, X_2, \cdots , X_m$ e $Y_1, Y_2, \cdots , Y_n$ amostras independentes de duas distribuições absolutamente contínuas. Encontre o estimador não viciado de mínima variância de**: 

**(a)$E(XY)$** 

O estimador de momentos para $E(XY)$ é simplesmente o produto dos estimadores de momentos de $X$ e $Y$, já que eles são independentes:

$\hat{E}(XY) = \hat{E}(X) \times \hat{E}(Y)$

Onde:
- $\hat{E}(X)$ é o estimador não viciado de mínima variância para a média de $X$.
- $\hat{E}(Y)$ é o estimador não viciado de mínima variância para a média de $Y$.

**(b)$Var(X + Y)$**

Para encontrar o estimador não viciado de mínima variância para $Var(X + Y)$, vamos primeiro lembrar que, se $X$ e $Y$ são independentes, então:

$Var(X + Y) = Var(X) + Var(Y)$

Então, precisamos encontrar os estimadores não viciados de mínima variância para $Var(X)$ e $Var(Y)$.

O estimador não viciado de mínima variância para a variância é a média dos quadrados das observações menos o quadrado da média. Assim:

$\hat{Var}(X) = \frac{1}{m-1} \sum_{i=1}^{m} (X_i - \overline{X})^2$
$\hat{Var}(Y) = \frac{1}{n-1} \sum_{i=1}^{n} (Y_i - \overline{Y})^2$

Onde:
- $\overline{X}$ é a média da amostra de $X$.
- $\overline{Y}$ é a média da amostra de $Y$.

Portanto, o estimador não viciado de mínima variância para $Var(X + Y)$ é:

$\hat{Var}(X + Y) = \hat{Var}(X) + \hat{Var}(Y)$


---

## Exercício 6. 

**Seja $f$ a função de densidade da função de distribuição $F$ e**
$$
\hat{f}(x) = \frac{1}{2nh} \sum_{i=1}^n 1_{x-h,x+h)} (X_i),
$$
**um estimador da função de densidade, sendo $h$ um número real conhecido como largura de banda. Então, com probabilidade 1,**
$$
2nh \hat{f}b(x) \sim Binomial(n, p),
$$
**sendo $p = F(x+h)−F(x−h)$. Encontrar**:

**(a)$E\big(\hat{f}(x)\big)$**

Como $\hat{f}(x)$ é uma média de variáveis indicadoras, sua esperança pode ser encontrada usando a linearidade da esperança:

$$E(\hat{f}(x)) = \frac{1}{2nh} \sum_{i=1}^n E(1_{x-h,x+h}(X_i))$$

A função indicadora $1_{x-h,x+h}(X_i)$ é 1 se $X_i$ cai no intervalo \((x-h, x+h)\), e 0 caso contrário. Portanto:

$$E(1_{x-h,x+h}(X_i)) = P(X_i \in (x-h, x+h))$$

Isso é igual à probabilidade de $X_i$ cair no intervalo \((x-h, x+h)\), que podemos expressar como a diferença entre as probabilidades acumuladas $F(x+h)$ e $F(x-h)$.

Então, $p = F(x+h) - F(x-h)$.

Portanto,

$$E(\hat{f}(x)) = \frac{1}{2nh} \sum_{i=1}^n p = \frac{n \times p}{2nh} = \frac{p}{2h}$$

**(b)$Var \big(\hat{f}(x) \big)$**

Para encontrar a variância de $\hat{f}(x)$, usamos a propriedade de variância de uma soma de variáveis independentes:

$$Var(\hat{f}(x)) = \frac{1}{(2nh)^2} \sum_{i=1}^n Var(1_{x-h,x+h}(X_i))$$

Como as variáveis indicadoras são binárias (tomando valores 0 ou 1), suas variâncias são:

$$Var(1_{x-h,x+h}(X_i)) = p(1-p)$$

Já que $1_{x-h,x+h}(X_i)$ é uma variável indicadora e $p$ é a probabilidade de sucesso.

Portanto,

$$Var(\hat{f}(x)) = \frac{n \times p(1-p)}{(2nh)^2} = \frac{p(1-p)}{(2nh)}$$
